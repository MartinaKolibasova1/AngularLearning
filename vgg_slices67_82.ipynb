{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "vgg_slices67-82.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMG3GwpzMmjhl73VDNgXF4H",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MartinaKolibasova1/AngularLearning/blob/master/vgg_slices67_82.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_YXxUEgR01m",
        "outputId": "cabc5b30-69d5-498c-b36b-e79d90c15984"
      },
      "source": [
        "pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==1.6.0+cu101 in /usr/local/lib/python3.7/dist-packages (1.6.0+cu101)\n",
            "Requirement already satisfied: torchvision==0.7.0+cu101 in /usr/local/lib/python3.7/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (1.19.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0+cu101) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.7.0+cu101) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2M912_jaR2ft",
        "outputId": "f9a1d1e4-e444-43f4-86a8-de4f7dd9511a"
      },
      "source": [
        "!pip3 install SimpleITK"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: SimpleITK in /usr/local/lib/python3.7/dist-packages (2.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SkrOWbgsR3vV",
        "outputId": "dc52eea6-18e9-4336-fbe4-e766f9fd32d9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbMnwzc_R5WO"
      },
      "source": [
        "!cp \"drive/My Drive/DP/image_nifti_2d_dataset.py\" ."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUjtTp23R6hh"
      },
      "source": [
        "import SimpleITK as sitk\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import time\n",
        "import torchvision.models as models\n",
        "from image_nifti_2d_dataset import ImageNifti2dDataset as ImageNifti2dDataset\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from statistics import mean \n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "import copy"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Twf2axEpQm-R"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# open a file with the pickled data\n",
        "file = open('drive/My Drive/DP/Dataloaders/dataloaders_64_ventricles_hippo_mri_masks_slices_67_82_coronal_standard_norm.pickle', 'rb')\n",
        "\n",
        "# dump information to that file\n",
        "dataloaders = pickle.load(file)\n",
        "\n",
        "# close the file\n",
        "\n",
        "file.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hv2J_rUs96Ow",
        "outputId": "0c7428f5-6597-4bdf-878b-62e36385de9d"
      },
      "source": [
        "dataloaders"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sizes': {'test': 1455, 'train': 5295, 'val': 780},\n",
              " 'test': <torch.utils.data.dataloader.DataLoader at 0x7f98ea901c10>,\n",
              " 'train': <torch.utils.data.dataloader.DataLoader at 0x7f995b964290>,\n",
              " 'val': <torch.utils.data.dataloader.DataLoader at 0x7f98ea99de10>}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i__vyBJbSBnK",
        "outputId": "c316dbf5-c32d-4824-beb1-ed04d9ef2e4a"
      },
      "source": [
        "from torch import optim, cuda\n",
        "\n",
        "gpu_count = cuda.device_count()\n",
        "print(f'{gpu_count} gpus detected.')\n",
        "if gpu_count > 1:\n",
        "    multi_gpu = True\n",
        "else:\n",
        "    multi_gpu = False"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1 gpus detected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Cu14n2ixW63"
      },
      "source": [
        "def visualize_data(data):\n",
        "    fig = plt.figure(figsize=(20, 20))\n",
        "\n",
        "    fig.add_subplot(131)\n",
        "    plt.title('before', fontsize=20)\n",
        "    plt.set_cmap('gray')\n",
        "    plt.imshow(data)\n",
        "    plt.show()"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RD2nvCp8Shpq"
      },
      "source": [
        "def get_cn_ad(dataloaders):\n",
        "  ad = 0\n",
        "  cn = 0\n",
        "  for inputs, mask, labels in dataloaders['train']:\n",
        "    ad += labels.sum()\n",
        "    cn += len(labels) - labels.sum()\n",
        "  \n",
        "  return cn, ad"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7iIrv9tJIf2D"
      },
      "source": [
        "dataset_sizes = dataloaders['sizes']\n",
        "\n",
        "def train_model(model, criterion, optimizer, dataloaders, scheduler = None, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    #best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    train_loss , train_accuracy, train_precision , train_recall = [], [], [], []\n",
        "    val_loss , val_accuracy, val_precision , val_recall = [], [], [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            print(f'-----------------{phase}-----------------')\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "\n",
        "            running_fp = 0\n",
        "            running_fn = 0\n",
        "            running_tp = 0\n",
        "            running_tn = 0\n",
        "            # Iterate over data.\n",
        "            for inputs, masks, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "                \n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs.float())\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    \n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                cm = confusion_matrix(labels.data.to('cpu'), preds.to('cpu'), labels=[1,0])\n",
        "\n",
        "                TP = cm[0][0]\n",
        "                TN = cm[1][1]\n",
        "                FN = cm[1][0]\n",
        "                FP = cm[0][1]\n",
        "\n",
        "                running_fp += FP\n",
        "                running_fn += FN\n",
        "                running_tp += TP\n",
        "                running_tn += TN\n",
        "          \n",
        "                del labels\n",
        "                del inputs\n",
        "\n",
        "            if phase == 'train' and scheduler is not None:\n",
        "              scheduler.step()\n",
        "\n",
        "            print(f'Running Corrects: {running_corrects} Size: {dataset_sizes[phase]}')\n",
        "            print(f'Running Loss: {running_loss} Size: {dataset_sizes[phase]}')\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            epoch_accuracy = 100. * running_corrects/dataset_sizes[phase]\n",
        "            epoch_recall = 100. * running_tp/(running_tp + running_fn)\n",
        "            epoch_precision = 100. * running_tp/(running_tp + running_fp)\n",
        "            print(f'{phase} Loss: {epoch_loss:.4f}, {phase} Acc: {phase} {epoch_accuracy:.2f}, {phase} Recall: {epoch_recall:.2f}, {phase} Precision: {epoch_precision:.2f}')\n",
        "\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            if phase == 'train':\n",
        "              train_loss.append(epoch_loss)\n",
        "              train_accuracy.append(epoch_accuracy)\n",
        "              train_recall.append(epoch_recall)\n",
        "              train_precision.append(epoch_precision)\n",
        "            else: \n",
        "              val_loss.append(epoch_loss)\n",
        "              val_accuracy.append(epoch_accuracy)\n",
        "              val_recall.append(epoch_recall)\n",
        "              val_precision.append(epoch_precision)   \n",
        "        print()\n",
        "\n",
        "    get_metrics(train_accuracy, train_precision, train_recall, val_accuracy, val_precision, val_recall)\n",
        "    visualize_plotly(train_accuracy, val_accuracy, num_epochs)\n",
        "    visualize_plotly(train_loss, val_loss, num_epochs)\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnXrRnd7gOHC"
      },
      "source": [
        " def x(model, device, test_loader):\n",
        "    model.eval()\n",
        "    actuals = []\n",
        "    probabilities = []\n",
        "    probabilities\n",
        "    with torch.no_grad():\n",
        "        for data, mask, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data.float())\n",
        "            outputs_softmax = torch.nn.functional.log_softmax(output, dim = 1)\n",
        "            _, preds = torch.max(output.data, 1)\n",
        "            actuals.extend(target.data.cpu())\n",
        "            probabilities.extend(_.data.cpu()) \n",
        "\n",
        "    return actuals, probabilities"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKB26WMTgOIz"
      },
      "source": [
        "def test_class_probabilities(model, device, test_loader, which_class):\n",
        "    model.eval()\n",
        "    actuals = []\n",
        "    probabilities = []\n",
        "    probabilities\n",
        "    with torch.no_grad():\n",
        "        for data, mask, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data.float())\n",
        "\n",
        "            prediction = output.argmax(dim=1, keepdim=True)\n",
        "            # print(prediction)\n",
        "            actuals.extend(target.view_as(prediction) == which_class)\n",
        "            probabilities.extend(np.exp(output[:, which_class].cpu()))\n",
        "    return [i.item() for i in actuals], [i.item() for i in probabilities]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFQtQzcwHD4N"
      },
      "source": [
        "def get_metrics(train_accuracy, train_precision, train_recall, val_accuracy, val_precision, val_recall):\n",
        "  print(\"Mean train accuracy: \" + str(sum(train_accuracy)/len(train_accuracy)))\n",
        "  print(\"Mean train recall: \" + str(sum(train_precision)/len(train_precision)))\n",
        "  print(\"Mean train precision: \" + str(sum(train_recall)/len(train_recall)))\n",
        "\n",
        "  print(\"Mean val accuracy: \" + str(sum(val_accuracy)/len(val_accuracy)))\n",
        "  print(\"Mean val recall: \" + str(sum(val_precision)/len(val_precision)))\n",
        "  print(\"Mean val precision: \" + str(sum(val_recall)/len(val_recall)))\n",
        "\n",
        "  print(\"MAX train accuracy: \" + str(max(train_accuracy)))\n",
        "  print(\"MAX train recall: \" + str(max(train_precision)))\n",
        "  print(\"MAX train precision: \" + str(max(train_recall)))\n",
        "\n",
        "  print(\"MAX val accuracy: \" + str(max(val_accuracy)))\n",
        "  print(\"MAX val recall: \" + str(max(val_precision)))\n",
        "  print(\"MAX val precision: \" + str(max(val_recall)))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYFZXbvvHJJG"
      },
      "source": [
        "def visualize(train_accuracy, val_accuracy):\n",
        "  plt.figure(figsize=(10, 7))\n",
        "  plt.plot(train_accuracy, color='green', label='train accuracy')\n",
        "  plt.plot(val_accuracy, color='blue', label='validataion accuracy')\n",
        "  plt.legend()\n",
        "  plt.savefig('accuracy.png')\n",
        "  plt.show()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hJJTjxZHG_U"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "def visualize_plotly(train_accuracy, val_accuracy, epochs):\n",
        "    train = np.full(\n",
        "        shape=len(train_accuracy),\n",
        "        fill_value='train',\n",
        "        dtype=np.str)\n",
        "\n",
        "    val = np.full(\n",
        "        shape=len(val_accuracy),\n",
        "        fill_value='val',\n",
        "        dtype=np.str)\n",
        "\n",
        "    epoch = np.concatenate([np.arange(1, epochs + 1), np.arange(1, epochs + 1)])\n",
        "\n",
        "    labels = np.concatenate([train, val])\n",
        "\n",
        "    acc_all = np.concatenate([train_accuracy, val_accuracy])\n",
        "\n",
        "    df = pd.DataFrame({'accuracy': acc_all, 'label': labels, 'epoch': epoch})\n",
        "\n",
        "    fig = px.line(df, x=\"epoch\", y=\"accuracy\", color='label')\n",
        "    fig.show()"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHkHsKbbHzaI"
      },
      "source": [
        "cn, ad = get_cn_ad(dataloaders)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOv11BvhH0uP",
        "outputId": "aac3e0ff-accf-411e-ac23-05f6a7820820"
      },
      "source": [
        "print(f'AD: {ad}, CN: {cn}')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AD: 3225, CN: 2070\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cD1jzSFtIOWG",
        "outputId": "5f010bb3-fb40-4ce8-e6f8-c0a1bc07591e"
      },
      "source": [
        "np.true_divide(torch.tensor([cn.item(), ad.item()]), ad + cn.item()).float()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.3909, 0.6091])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJIRLM27IRc2"
      },
      "source": [
        "weight = np.true_divide(torch.tensor([cn.item(), ad.item()]), ad + cn.item()).float()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSjFcvUjSRKl"
      },
      "source": [
        "model_ft = models.vgg13(pretrained=True)\n",
        "\n",
        "for param in model_ft.features.parameters():\n",
        "  param.requires_grad_(False)\n",
        "\n",
        "model_ft.classifier[-1] = nn.Linear(in_features=4096, out_features=2)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight.to(device))\n",
        "\n",
        "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_ft = optim.AdamW(model_ft.parameters(), lr=0.0001)\n",
        "\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "93YvNo11SUmS",
        "outputId": "643239e1-283d-42d9-f896-a3c593554a3b"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, dataloaders, num_epochs=30)\n",
        "# Save model\n",
        "torch.save(model_ft.state_dict(), 'vgg13.pt')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3552 Size: 5295\n",
            "Running Loss: 2847.4785244464874 Size: 5295\n",
            "train Loss: 0.5378, train Acc: train 67.08, train Recall: 66.28, train Precision: 93.55\n",
            "train Loss: 0.5378 Acc: 0.6708\n",
            "-----------------val-----------------\n",
            "Running Corrects: 561 Size: 780\n",
            "Running Loss: 403.34740257263184 Size: 780\n",
            "val Loss: 0.5171, val Acc: val 71.92, val Recall: 79.50, val Precision: 82.98\n",
            "val Loss: 0.5171 Acc: 0.7192\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4321 Size: 5295\n",
            "Running Loss: 1910.020146816969 Size: 5295\n",
            "train Loss: 0.3607, train Acc: train 81.61, train Recall: 80.04, train Precision: 92.99\n",
            "train Loss: 0.3607 Acc: 0.8161\n",
            "-----------------val-----------------\n",
            "Running Corrects: 605 Size: 780\n",
            "Running Loss: 323.06153321266174 Size: 780\n",
            "val Loss: 0.4142, val Acc: val 77.56, val Recall: 81.60, val Precision: 89.47\n",
            "val Loss: 0.4142 Acc: 0.7756\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4726 Size: 5295\n",
            "Running Loss: 1180.9705236554146 Size: 5295\n",
            "train Loss: 0.2230, train Acc: train 89.25, train Recall: 88.38, train Precision: 94.82\n",
            "train Loss: 0.2230 Acc: 0.8925\n",
            "-----------------val-----------------\n",
            "Running Corrects: 584 Size: 780\n",
            "Running Loss: 383.2822871208191 Size: 780\n",
            "val Loss: 0.4914, val Acc: val 74.87, val Recall: 81.80, val Precision: 84.39\n",
            "val Loss: 0.4914 Acc: 0.7487\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5032 Size: 5295\n",
            "Running Loss: 650.8566025793552 Size: 5295\n",
            "train Loss: 0.1229, train Acc: train 95.03, train Recall: 94.42, train Precision: 97.61\n",
            "train Loss: 0.1229 Acc: 0.9503\n",
            "-----------------val-----------------\n",
            "Running Corrects: 545 Size: 780\n",
            "Running Loss: 609.0048499107361 Size: 780\n",
            "val Loss: 0.7808, val Acc: val 69.87, val Recall: 77.78, val Precision: 82.28\n",
            "val Loss: 0.7808 Acc: 0.6987\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5133 Size: 5295\n",
            "Running Loss: 407.06996538490057 Size: 5295\n",
            "train Loss: 0.0769, train Acc: train 96.94, train Recall: 96.62, train Precision: 98.42\n",
            "train Loss: 0.0769 Acc: 0.9694\n",
            "-----------------val-----------------\n",
            "Running Corrects: 538 Size: 780\n",
            "Running Loss: 698.1975586414337 Size: 780\n",
            "val Loss: 0.8951, val Acc: val 68.97, val Recall: 86.61, val Precision: 68.07\n",
            "val Loss: 0.8951 Acc: 0.6897\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5216 Size: 5295\n",
            "Running Loss: 198.64558305591345 Size: 5295\n",
            "train Loss: 0.0375, train Acc: train 98.51, train Recall: 98.43, train Precision: 99.13\n",
            "train Loss: 0.0375 Acc: 0.9851\n",
            "-----------------val-----------------\n",
            "Running Corrects: 559 Size: 780\n",
            "Running Loss: 728.598588347435 Size: 780\n",
            "val Loss: 0.9341, val Acc: val 71.67, val Recall: 80.24, val Precision: 81.23\n",
            "val Loss: 0.9341 Acc: 0.7167\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5160 Size: 5295\n",
            "Running Loss: 300.80873942375183 Size: 5295\n",
            "train Loss: 0.0568, train Acc: train 97.45, train Recall: 97.36, train Precision: 98.48\n",
            "train Loss: 0.0568 Acc: 0.9745\n",
            "-----------------val-----------------\n",
            "Running Corrects: 546 Size: 780\n",
            "Running Loss: 841.5756123065948 Size: 780\n",
            "val Loss: 1.0789, val Acc: val 70.00, val Recall: 80.66, val Precision: 77.54\n",
            "val Loss: 1.0789 Acc: 0.7000\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5215 Size: 5295\n",
            "Running Loss: 190.16362071409822 Size: 5295\n",
            "train Loss: 0.0359, train Acc: train 98.49, train Recall: 98.46, train Precision: 99.07\n",
            "train Loss: 0.0359 Acc: 0.9849\n",
            "-----------------val-----------------\n",
            "Running Corrects: 529 Size: 780\n",
            "Running Loss: 999.2299098968506 Size: 780\n",
            "val Loss: 1.2811, val Acc: val 67.82, val Recall: 81.21, val Precision: 72.81\n",
            "val Loss: 1.2811 Acc: 0.6782\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5258 Size: 5295\n",
            "Running Loss: 104.67069333558902 Size: 5295\n",
            "train Loss: 0.0198, train Acc: train 99.30, train Recall: 99.26, train Precision: 99.60\n",
            "train Loss: 0.0198 Acc: 0.9930\n",
            "-----------------val-----------------\n",
            "Running Corrects: 543 Size: 780\n",
            "Running Loss: 1044.2542328834534 Size: 780\n",
            "val Loss: 1.3388, val Acc: val 69.62, val Recall: 82.08, val Precision: 74.74\n",
            "val Loss: 1.3388 Acc: 0.6962\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5245 Size: 5295\n",
            "Running Loss: 125.0337760578841 Size: 5295\n",
            "train Loss: 0.0236, train Acc: train 99.06, train Recall: 98.95, train Precision: 99.50\n",
            "train Loss: 0.0236 Acc: 0.9906\n",
            "-----------------val-----------------\n",
            "Running Corrects: 523 Size: 780\n",
            "Running Loss: 1296.3984994888306 Size: 780\n",
            "val Loss: 1.6620, val Acc: val 67.05, val Recall: 85.65, val Precision: 65.96\n",
            "val Loss: 1.6620 Acc: 0.6705\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5255 Size: 5295\n",
            "Running Loss: 107.38498739758506 Size: 5295\n",
            "train Loss: 0.0203, train Acc: train 99.24, train Recall: 99.14, train Precision: 99.63\n",
            "train Loss: 0.0203 Acc: 0.9924\n",
            "-----------------val-----------------\n",
            "Running Corrects: 537 Size: 780\n",
            "Running Loss: 1122.4667177200317 Size: 780\n",
            "val Loss: 1.4391, val Acc: val 68.85, val Recall: 80.56, val Precision: 75.61\n",
            "val Loss: 1.4391 Acc: 0.6885\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5261 Size: 5295\n",
            "Running Loss: 81.1961697419174 Size: 5295\n",
            "train Loss: 0.0153, train Acc: train 99.36, train Recall: 99.26, train Precision: 99.69\n",
            "train Loss: 0.0153 Acc: 0.9936\n",
            "-----------------val-----------------\n",
            "Running Corrects: 529 Size: 780\n",
            "Running Loss: 1312.707703113556 Size: 780\n",
            "val Loss: 1.6830, val Acc: val 67.82, val Recall: 84.15, val Precision: 68.95\n",
            "val Loss: 1.6830 Acc: 0.6782\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5259 Size: 5295\n",
            "Running Loss: 76.52810598909855 Size: 5295\n",
            "train Loss: 0.0145, train Acc: train 99.32, train Recall: 99.38, train Precision: 99.50\n",
            "train Loss: 0.0145 Acc: 0.9932\n",
            "-----------------val-----------------\n",
            "Running Corrects: 500 Size: 780\n",
            "Running Loss: 1787.7924842834473 Size: 780\n",
            "val Loss: 2.2920, val Acc: val 64.10, val Recall: 83.41, val Precision: 63.51\n",
            "val Loss: 2.2920 Acc: 0.6410\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5238 Size: 5295\n",
            "Running Loss: 147.2171212285757 Size: 5295\n",
            "train Loss: 0.0278, train Acc: train 98.92, train Recall: 98.80, train Precision: 99.44\n",
            "train Loss: 0.0278 Acc: 0.9892\n",
            "-----------------val-----------------\n",
            "Running Corrects: 501 Size: 780\n",
            "Running Loss: 1514.617091178894 Size: 780\n",
            "val Loss: 1.9418, val Acc: val 64.23, val Recall: 81.98, val Precision: 65.44\n",
            "val Loss: 1.9418 Acc: 0.6423\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5255 Size: 5295\n",
            "Running Loss: 107.28757576621138 Size: 5295\n",
            "train Loss: 0.0203, train Acc: train 99.24, train Recall: 99.11, train Precision: 99.66\n",
            "train Loss: 0.0203 Acc: 0.9924\n",
            "-----------------val-----------------\n",
            "Running Corrects: 485 Size: 780\n",
            "Running Loss: 1950.0091524124146 Size: 780\n",
            "val Loss: 2.5000, val Acc: val 62.18, val Recall: 80.49, val Precision: 63.68\n",
            "val Loss: 2.5000 Acc: 0.6218\n",
            "\n",
            "Epoch 15/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5260 Size: 5295\n",
            "Running Loss: 78.6087300973013 Size: 5295\n",
            "train Loss: 0.0148, train Acc: train 99.34, train Recall: 99.38, train Precision: 99.53\n",
            "train Loss: 0.0148 Acc: 0.9934\n",
            "-----------------val-----------------\n",
            "Running Corrects: 471 Size: 780\n",
            "Running Loss: 2314.277766227722 Size: 780\n",
            "val Loss: 2.9670, val Acc: val 60.38, val Recall: 85.75, val Precision: 54.91\n",
            "val Loss: 2.9670 Acc: 0.6038\n",
            "\n",
            "Epoch 16/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5250 Size: 5295\n",
            "Running Loss: 127.31660402892157 Size: 5295\n",
            "train Loss: 0.0240, train Acc: train 99.15, train Recall: 99.13, train Precision: 99.47\n",
            "train Loss: 0.0240 Acc: 0.9915\n",
            "-----------------val-----------------\n",
            "Running Corrects: 493 Size: 780\n",
            "Running Loss: 1967.402424812317 Size: 780\n",
            "val Loss: 2.5223, val Acc: val 63.21, val Recall: 82.53, val Precision: 62.98\n",
            "val Loss: 2.5223 Acc: 0.6321\n",
            "\n",
            "Epoch 17/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5250 Size: 5295\n",
            "Running Loss: 104.77047356311232 Size: 5295\n",
            "train Loss: 0.0198, train Acc: train 99.15, train Recall: 99.13, train Precision: 99.47\n",
            "train Loss: 0.0198 Acc: 0.9915\n",
            "-----------------val-----------------\n",
            "Running Corrects: 541 Size: 780\n",
            "Running Loss: 1380.2536635398865 Size: 780\n",
            "val Loss: 1.7696, val Acc: val 69.36, val Recall: 78.39, val Precision: 80.18\n",
            "val Loss: 1.7696 Acc: 0.6936\n",
            "\n",
            "Epoch 18/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5272 Size: 5295\n",
            "Running Loss: 64.95488467533141 Size: 5295\n",
            "train Loss: 0.0123, train Acc: train 99.57, train Recall: 99.57, train Precision: 99.72\n",
            "train Loss: 0.0123 Acc: 0.9957\n",
            "-----------------val-----------------\n",
            "Running Corrects: 499 Size: 780\n",
            "Running Loss: 1877.0715646743774 Size: 780\n",
            "val Loss: 2.4065, val Acc: val 63.97, val Recall: 81.76, val Precision: 65.26\n",
            "val Loss: 2.4065 Acc: 0.6397\n",
            "\n",
            "Epoch 19/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5280 Size: 5295\n",
            "Running Loss: 41.028731664526276 Size: 5295\n",
            "train Loss: 0.0077, train Acc: train 99.72, train Recall: 99.66, train Precision: 99.88\n",
            "train Loss: 0.0077 Acc: 0.9972\n",
            "-----------------val-----------------\n",
            "Running Corrects: 504 Size: 780\n",
            "Running Loss: 1833.0671606063843 Size: 780\n",
            "val Loss: 2.3501, val Acc: val 64.62, val Recall: 79.40, val Precision: 69.65\n",
            "val Loss: 2.3501 Acc: 0.6462\n",
            "\n",
            "Epoch 20/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5268 Size: 5295\n",
            "Running Loss: 80.30489421612583 Size: 5295\n",
            "train Loss: 0.0152, train Acc: train 99.49, train Recall: 99.44, train Precision: 99.72\n",
            "train Loss: 0.0152 Acc: 0.9949\n",
            "-----------------val-----------------\n",
            "Running Corrects: 521 Size: 780\n",
            "Running Loss: 1669.9147896766663 Size: 780\n",
            "val Loss: 2.1409, val Acc: val 66.79, val Recall: 85.91, val Precision: 65.26\n",
            "val Loss: 2.1409 Acc: 0.6679\n",
            "\n",
            "Epoch 21/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5209 Size: 5295\n",
            "Running Loss: 217.54094980843365 Size: 5295\n",
            "train Loss: 0.0411, train Acc: train 98.38, train Recall: 98.43, train Precision: 98.91\n",
            "train Loss: 0.0411 Acc: 0.9838\n",
            "-----------------val-----------------\n",
            "Running Corrects: 476 Size: 780\n",
            "Running Loss: 1584.6391463279724 Size: 780\n",
            "val Loss: 2.0316, val Acc: val 61.03, val Recall: 80.93, val Precision: 61.05\n",
            "val Loss: 2.0316 Acc: 0.6103\n",
            "\n",
            "Epoch 22/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5257 Size: 5295\n",
            "Running Loss: 86.84672765014693 Size: 5295\n",
            "train Loss: 0.0164, train Acc: train 99.28, train Recall: 99.14, train Precision: 99.69\n",
            "train Loss: 0.0164 Acc: 0.9928\n",
            "-----------------val-----------------\n",
            "Running Corrects: 536 Size: 780\n",
            "Running Loss: 1341.3201160430908 Size: 780\n",
            "val Loss: 1.7196, val Acc: val 68.72, val Recall: 82.86, val Precision: 72.11\n",
            "val Loss: 1.7196 Acc: 0.6872\n",
            "\n",
            "Epoch 23/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5272 Size: 5295\n",
            "Running Loss: 64.69991806754842 Size: 5295\n",
            "train Loss: 0.0122, train Acc: train 99.57, train Recall: 99.57, train Precision: 99.72\n",
            "train Loss: 0.0122 Acc: 0.9957\n",
            "-----------------val-----------------\n",
            "Running Corrects: 537 Size: 780\n",
            "Running Loss: 1439.915587902069 Size: 780\n",
            "val Loss: 1.8460, val Acc: val 68.85, val Recall: 82.90, val Precision: 72.28\n",
            "val Loss: 1.8460 Acc: 0.6885\n",
            "\n",
            "Epoch 24/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5276 Size: 5295\n",
            "Running Loss: 48.34541004919447 Size: 5295\n",
            "train Loss: 0.0091, train Acc: train 99.64, train Recall: 99.66, train Precision: 99.75\n",
            "train Loss: 0.0091 Acc: 0.9964\n",
            "-----------------val-----------------\n",
            "Running Corrects: 555 Size: 780\n",
            "Running Loss: 1315.8774609565735 Size: 780\n",
            "val Loss: 1.6870, val Acc: val 71.15, val Recall: 78.70, val Precision: 82.98\n",
            "val Loss: 1.6870 Acc: 0.7115\n",
            "\n",
            "Epoch 25/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5282 Size: 5295\n",
            "Running Loss: 37.13922369107604 Size: 5295\n",
            "train Loss: 0.0070, train Acc: train 99.75, train Recall: 99.69, train Precision: 99.91\n",
            "train Loss: 0.0070 Acc: 0.9975\n",
            "-----------------val-----------------\n",
            "Running Corrects: 493 Size: 780\n",
            "Running Loss: 2178.6708612442017 Size: 780\n",
            "val Loss: 2.7932, val Acc: val 63.21, val Recall: 79.54, val Precision: 66.84\n",
            "val Loss: 2.7932 Acc: 0.6321\n",
            "\n",
            "Epoch 26/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5246 Size: 5295\n",
            "Running Loss: 145.71375178708695 Size: 5295\n",
            "train Loss: 0.0275, train Acc: train 99.07, train Recall: 99.13, train Precision: 99.35\n",
            "train Loss: 0.0275 Acc: 0.9907\n",
            "-----------------val-----------------\n",
            "Running Corrects: 568 Size: 780\n",
            "Running Loss: 1264.2758203744888 Size: 780\n",
            "val Loss: 1.6209, val Acc: val 72.82, val Recall: 78.32, val Precision: 86.84\n",
            "val Loss: 1.6209 Acc: 0.7282\n",
            "\n",
            "Epoch 27/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5212 Size: 5295\n",
            "Running Loss: 244.16430010087788 Size: 5295\n",
            "train Loss: 0.0461, train Acc: train 98.43, train Recall: 98.49, train Precision: 98.95\n",
            "train Loss: 0.0461 Acc: 0.9843\n",
            "-----------------val-----------------\n",
            "Running Corrects: 520 Size: 780\n",
            "Running Loss: 1505.621594429016 Size: 780\n",
            "val Loss: 1.9303, val Acc: val 66.67, val Recall: 79.92, val Precision: 72.63\n",
            "val Loss: 1.9303 Acc: 0.6667\n",
            "\n",
            "Epoch 28/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5282 Size: 5295\n",
            "Running Loss: 33.192306503528016 Size: 5295\n",
            "train Loss: 0.0063, train Acc: train 99.75, train Recall: 99.69, train Precision: 99.91\n",
            "train Loss: 0.0063 Acc: 0.9975\n",
            "-----------------val-----------------\n",
            "Running Corrects: 577 Size: 780\n",
            "Running Loss: 1191.2815442085266 Size: 780\n",
            "val Loss: 1.5273, val Acc: val 73.97, val Recall: 83.30, val Precision: 80.53\n",
            "val Loss: 1.5273 Acc: 0.7397\n",
            "\n",
            "Epoch 29/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 5288 Size: 5295\n",
            "Running Loss: 17.33752687711967 Size: 5295\n",
            "train Loss: 0.0033, train Acc: train 99.87, train Recall: 99.85, train Precision: 99.94\n",
            "train Loss: 0.0033 Acc: 0.9987\n",
            "-----------------val-----------------\n",
            "Running Corrects: 523 Size: 780\n",
            "Running Loss: 1738.849772453308 Size: 780\n",
            "val Loss: 2.2293, val Acc: val 67.05, val Recall: 85.33, val Precision: 66.32\n",
            "val Loss: 2.2293 Acc: 0.6705\n",
            "\n",
            "Mean train accuracy: tensor(96.9657, device='cuda:0')\n",
            "Mean train recall: 98.83410852713179\n",
            "Mean train precision: 96.7946117620618\n",
            "Mean val accuracy: tensor(67.9444, device='cuda:0')\n",
            "Mean val recall: 72.54970760233918\n",
            "Mean val precision: 81.7755660311914\n",
            "MAX train accuracy: tensor(99.8678, device='cuda:0')\n",
            "MAX train recall: 99.93798449612403\n",
            "MAX train precision: 99.8451053283767\n",
            "MAX val accuracy: tensor(77.5641, device='cuda:0')\n",
            "MAX val recall: 89.47368421052632\n",
            "MAX val precision: 86.60714285714286\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"589745bf-de53-4fb1-b868-c14fcbc2f962\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"589745bf-de53-4fb1-b868-c14fcbc2f962\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '589745bf-de53-4fb1-b868-c14fcbc2f962',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=t<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=t\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=t\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"xaxis\": \"x\", \"y\": [67.0821533203125, 81.60529327392578, 89.25401306152344, 95.03305053710938, 96.94051361083984, 98.50802612304688, 97.45043182373047, 98.48914337158203, 99.30123138427734, 99.05571746826172, 99.24457550048828, 99.3578872680664, 99.32011413574219, 98.92351531982422, 99.24457550048828, 99.33900451660156, 99.150146484375, 99.150146484375, 99.56562805175781, 99.71672058105469, 99.4900894165039, 98.3758316040039, 99.2823486328125, 99.56562805175781, 99.64117431640625, 99.75448608398438, 99.07460021972656, 98.43248748779297, 99.75448608398438, 99.86780548095703], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=v<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=v\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"xaxis\": \"x\", \"y\": [71.92308044433594, 77.56410217285156, 74.87179565429688, 69.87179565429688, 68.97435760498047, 71.66667175292969, 70.0, 67.82051849365234, 69.61538696289062, 67.05128479003906, 68.84615325927734, 67.82051849365234, 64.10256958007812, 64.23077392578125, 62.17948913574219, 60.38461685180664, 63.20513153076172, 69.35897827148438, 63.974361419677734, 64.61538696289062, 66.79487609863281, 61.02564239501953, 68.71794891357422, 68.84615325927734, 71.15384674072266, 63.20513153076172, 72.82051849365234, 66.66667175292969, 73.97435760498047, 67.05128479003906], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('589745bf-de53-4fb1-b868-c14fcbc2f962');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"9f4213d4-cf84-4224-bcde-0c290e71dcda\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"9f4213d4-cf84-4224-bcde-0c290e71dcda\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '9f4213d4-cf84-4224-bcde-0c290e71dcda',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=t<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=t\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=t\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"xaxis\": \"x\", \"y\": [0.5377674267132176, 0.36072146304380903, 0.2230350375175476, 0.12291909397154963, 0.07687818043152041, 0.037515690850975156, 0.056809960231114606, 0.035913809388875964, 0.019767836324001704, 0.023613555440582454, 0.02028045087773089, 0.015334498534828593, 0.014452900092369887, 0.027803044613517604, 0.020262053969067304, 0.014845841378149444, 0.02404468442472551, 0.019786680559605727, 0.012267211458986103, 0.007748580106614972, 0.015166174545066257, 0.041084220926994076, 0.016401648281425294, 0.01221905912512718, 0.009130389055560807, 0.00701401769425421, 0.02751912215053578, 0.04611223797939148, 0.0062686131262564716, 0.003274320467822412], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=v<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=v\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"xaxis\": \"x\", \"y\": [0.5171120545802972, 0.41418145283674584, 0.4913875475907937, 0.7807754486035078, 0.8951250751813252, 0.9341007542915833, 1.0789430927007626, 1.2810639870472444, 1.3387874780557094, 1.6620493583190135, 1.4390598945128612, 1.6829585937353282, 2.29204164651724, 1.9418167835626847, 2.50001173386207, 2.9670227772150284, 2.522310801041432, 1.7695559788972903, 2.4065020059927917, 2.350086103341518, 2.140916397021367, 2.0315886491384263, 1.719641174414219, 1.8460456255154731, 1.6870223858417608, 2.7931677708258995, 1.6208664363775498, 1.9302840954218154, 1.5272840310365725, 2.229294580068344], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('9f4213d4-cf84-4224-bcde-0c290e71dcda');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete in 20m 34s\n",
            "Best val Acc: 0.775641\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQXsOhMdUqAa"
      },
      "source": [
        "model_ft = models.vgg13(pretrained=True)\n",
        "\n",
        "for param in model_ft.features.parameters():\n",
        "  param.requires_grad_(False)\n",
        "\n",
        "feats_list = list(model_ft.features)\n",
        "new_feats_list = []\n",
        "for feat in feats_list:\n",
        "    new_feats_list.append(feat)\n",
        "    if isinstance(feat, nn.Conv2d):\n",
        "        new_feats_list.append(nn.Dropout(p=0.5, inplace=True))\n",
        "\n",
        "# modify convolution layers\n",
        "model_ft.features = nn.Sequential(*new_feats_list)\n",
        "\n",
        "model_ft.classifier[-1] = nn.Linear(in_features=4096, out_features=2)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight.to(device))\n",
        "\n",
        "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_ft = optim.AdamW(model_ft.parameters(), lr=0.0001)\n",
        "\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w0Go4cA9VyOy",
        "outputId": "2ccf38d2-1d75-46a6-b4b6-f03141c08862"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, dataloaders, num_epochs=30)\n",
        "# Save model\n",
        "torch.save(model_ft.state_dict(), 'vgg13_AdamW.pt')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3096 Size: 5295\n",
            "Running Loss: 4325.500290811062 Size: 5295\n",
            "train Loss: 0.8169, train Acc: train 58.47, train Recall: 61.99, train Precision: 82.26\n",
            "train Loss: 0.8169 Acc: 0.5847\n",
            "-----------------val-----------------\n",
            "Running Corrects: 590 Size: 780\n",
            "Running Loss: 508.65438055992126 Size: 780\n",
            "val Loss: 0.6521, val Acc: val 75.64, val Recall: 77.54, val Precision: 93.86\n",
            "val Loss: 0.6521 Acc: 0.7564\n",
            "\n",
            "Epoch 1/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3259 Size: 5295\n",
            "Running Loss: 3334.87103164196 Size: 5295\n",
            "train Loss: 0.6298, train Acc: train 61.55, train Recall: 62.46, train Precision: 92.43\n",
            "train Loss: 0.6298 Acc: 0.6155\n",
            "-----------------val-----------------\n",
            "Running Corrects: 582 Size: 780\n",
            "Running Loss: 500.0758512020111 Size: 780\n",
            "val Loss: 0.6411, val Acc: val 74.62, val Recall: 74.73, val Precision: 98.60\n",
            "val Loss: 0.6411 Acc: 0.7462\n",
            "\n",
            "Epoch 2/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3271 Size: 5295\n",
            "Running Loss: 3257.041457951069 Size: 5295\n",
            "train Loss: 0.6151, train Acc: train 61.78, train Recall: 62.73, train Precision: 91.78\n",
            "train Loss: 0.6151 Acc: 0.6178\n",
            "-----------------val-----------------\n",
            "Running Corrects: 548 Size: 780\n",
            "Running Loss: 497.0518476963043 Size: 780\n",
            "val Loss: 0.6372, val Acc: val 70.26, val Recall: 72.35, val Precision: 95.96\n",
            "val Loss: 0.6372 Acc: 0.7026\n",
            "\n",
            "Epoch 3/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3329 Size: 5295\n",
            "Running Loss: 3191.1790530085564 Size: 5295\n",
            "train Loss: 0.6027, train Acc: train 62.87, train Recall: 63.35, train Precision: 92.62\n",
            "train Loss: 0.6027 Acc: 0.6287\n",
            "-----------------val-----------------\n",
            "Running Corrects: 559 Size: 780\n",
            "Running Loss: 501.11587262153625 Size: 780\n",
            "val Loss: 0.6425, val Acc: val 71.67, val Recall: 72.81, val Precision: 97.72\n",
            "val Loss: 0.6425 Acc: 0.7167\n",
            "\n",
            "Epoch 4/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3298 Size: 5295\n",
            "Running Loss: 3135.1018903255463 Size: 5295\n",
            "train Loss: 0.5921, train Acc: train 62.29, train Recall: 62.82, train Precision: 93.30\n",
            "train Loss: 0.5921 Acc: 0.6229\n",
            "-----------------val-----------------\n",
            "Running Corrects: 543 Size: 780\n",
            "Running Loss: 502.2657699584961 Size: 780\n",
            "val Loss: 0.6439, val Acc: val 69.62, val Recall: 72.23, val Precision: 94.91\n",
            "val Loss: 0.6439 Acc: 0.6962\n",
            "\n",
            "Epoch 5/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3382 Size: 5295\n",
            "Running Loss: 3072.8591094613075 Size: 5295\n",
            "train Loss: 0.5803, train Acc: train 63.87, train Recall: 63.89, train Precision: 93.55\n",
            "train Loss: 0.5803 Acc: 0.6387\n",
            "-----------------val-----------------\n",
            "Running Corrects: 557 Size: 780\n",
            "Running Loss: 507.1188201904297 Size: 780\n",
            "val Loss: 0.6502, val Acc: val 71.41, val Recall: 74.00, val Precision: 93.86\n",
            "val Loss: 0.6502 Acc: 0.7141\n",
            "\n",
            "Epoch 6/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3411 Size: 5295\n",
            "Running Loss: 3014.1293553113937 Size: 5295\n",
            "train Loss: 0.5692, train Acc: train 64.42, train Recall: 64.48, train Precision: 92.56\n",
            "train Loss: 0.5692 Acc: 0.6442\n",
            "-----------------val-----------------\n",
            "Running Corrects: 552 Size: 780\n",
            "Running Loss: 503.184072971344 Size: 780\n",
            "val Loss: 0.6451, val Acc: val 70.77, val Recall: 72.80, val Precision: 95.79\n",
            "val Loss: 0.6451 Acc: 0.7077\n",
            "\n",
            "Epoch 7/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3453 Size: 5295\n",
            "Running Loss: 2948.492542207241 Size: 5295\n",
            "train Loss: 0.5568, train Acc: train 65.21, train Recall: 65.20, train Precision: 91.97\n",
            "train Loss: 0.5568 Acc: 0.6521\n",
            "-----------------val-----------------\n",
            "Running Corrects: 557 Size: 780\n",
            "Running Loss: 495.5152213573456 Size: 780\n",
            "val Loss: 0.6353, val Acc: val 71.41, val Recall: 72.62, val Precision: 97.72\n",
            "val Loss: 0.6353 Acc: 0.7141\n",
            "\n",
            "Epoch 8/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3475 Size: 5295\n",
            "Running Loss: 2946.2899571061134 Size: 5295\n",
            "train Loss: 0.5564, train Acc: train 65.63, train Recall: 65.38, train Precision: 92.62\n",
            "train Loss: 0.5564 Acc: 0.6563\n",
            "-----------------val-----------------\n",
            "Running Corrects: 550 Size: 780\n",
            "Running Loss: 502.55887722969055 Size: 780\n",
            "val Loss: 0.6443, val Acc: val 70.51, val Recall: 72.67, val Precision: 95.61\n",
            "val Loss: 0.6443 Acc: 0.7051\n",
            "\n",
            "Epoch 9/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3530 Size: 5295\n",
            "Running Loss: 2910.4981979727745 Size: 5295\n",
            "train Loss: 0.5497, train Acc: train 66.67, train Recall: 66.23, train Precision: 92.37\n",
            "train Loss: 0.5497 Acc: 0.6667\n",
            "-----------------val-----------------\n",
            "Running Corrects: 593 Size: 780\n",
            "Running Loss: 515.0440261363983 Size: 780\n",
            "val Loss: 0.6603, val Acc: val 76.03, val Recall: 82.08, val Precision: 85.96\n",
            "val Loss: 0.6603 Acc: 0.7603\n",
            "\n",
            "Epoch 10/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3525 Size: 5295\n",
            "Running Loss: 2890.351477921009 Size: 5295\n",
            "train Loss: 0.5459, train Acc: train 66.57, train Recall: 66.49, train Precision: 90.98\n",
            "train Loss: 0.5459 Acc: 0.6657\n",
            "-----------------val-----------------\n",
            "Running Corrects: 550 Size: 780\n",
            "Running Loss: 509.8324685096741 Size: 780\n",
            "val Loss: 0.6536, val Acc: val 70.51, val Recall: 74.08, val Precision: 91.75\n",
            "val Loss: 0.6536 Acc: 0.7051\n",
            "\n",
            "Epoch 11/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3561 Size: 5295\n",
            "Running Loss: 2869.39118540287 Size: 5295\n",
            "train Loss: 0.5419, train Acc: train 67.25, train Recall: 67.04, train Precision: 90.95\n",
            "train Loss: 0.5419 Acc: 0.6725\n",
            "-----------------val-----------------\n",
            "Running Corrects: 561 Size: 780\n",
            "Running Loss: 506.45992255210876 Size: 780\n",
            "val Loss: 0.6493, val Acc: val 71.92, val Recall: 74.68, val Precision: 93.16\n",
            "val Loss: 0.6493 Acc: 0.7192\n",
            "\n",
            "Epoch 12/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3560 Size: 5295\n",
            "Running Loss: 2825.321687877178 Size: 5295\n",
            "train Loss: 0.5336, train Acc: train 67.23, train Recall: 66.83, train Precision: 91.72\n",
            "train Loss: 0.5336 Acc: 0.6723\n",
            "-----------------val-----------------\n",
            "Running Corrects: 574 Size: 780\n",
            "Running Loss: 518.3337349891663 Size: 780\n",
            "val Loss: 0.6645, val Acc: val 73.59, val Recall: 78.98, val Precision: 87.02\n",
            "val Loss: 0.6645 Acc: 0.7359\n",
            "\n",
            "Epoch 13/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3646 Size: 5295\n",
            "Running Loss: 2783.7133708000183 Size: 5295\n",
            "train Loss: 0.5257, train Acc: train 68.86, train Recall: 68.11, train Precision: 91.88\n",
            "train Loss: 0.5257 Acc: 0.6886\n",
            "-----------------val-----------------\n",
            "Running Corrects: 573 Size: 780\n",
            "Running Loss: 516.4682450294495 Size: 780\n",
            "val Loss: 0.6621, val Acc: val 73.46, val Recall: 77.54, val Precision: 89.65\n",
            "val Loss: 0.6621 Acc: 0.7346\n",
            "\n",
            "Epoch 14/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3614 Size: 5295\n",
            "Running Loss: 2768.4181495010853 Size: 5295\n",
            "train Loss: 0.5228, train Acc: train 68.25, train Recall: 67.61, train Precision: 91.91\n",
            "train Loss: 0.5228 Acc: 0.6825\n",
            "-----------------val-----------------\n",
            "Running Corrects: 558 Size: 780\n",
            "Running Loss: 515.2645983695984 Size: 780\n",
            "val Loss: 0.6606, val Acc: val 71.54, val Recall: 75.82, val Precision: 89.65\n",
            "val Loss: 0.6606 Acc: 0.7154\n",
            "\n",
            "Epoch 15/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3672 Size: 5295\n",
            "Running Loss: 2779.199678659439 Size: 5295\n",
            "train Loss: 0.5249, train Acc: train 69.35, train Recall: 68.63, train Precision: 91.50\n",
            "train Loss: 0.5249 Acc: 0.6935\n",
            "-----------------val-----------------\n",
            "Running Corrects: 553 Size: 780\n",
            "Running Loss: 505.342835187912 Size: 780\n",
            "val Loss: 0.6479, val Acc: val 70.90, val Recall: 74.40, val Precision: 91.75\n",
            "val Loss: 0.6479 Acc: 0.7090\n",
            "\n",
            "Epoch 16/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3677 Size: 5295\n",
            "Running Loss: 2736.201462805271 Size: 5295\n",
            "train Loss: 0.5168, train Acc: train 69.44, train Recall: 68.75, train Precision: 91.35\n",
            "train Loss: 0.5168 Acc: 0.6944\n",
            "-----------------val-----------------\n",
            "Running Corrects: 563 Size: 780\n",
            "Running Loss: 503.45686888694763 Size: 780\n",
            "val Loss: 0.6455, val Acc: val 72.18, val Recall: 75.77, val Precision: 91.05\n",
            "val Loss: 0.6455 Acc: 0.7218\n",
            "\n",
            "Epoch 17/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3756 Size: 5295\n",
            "Running Loss: 2702.258994668722 Size: 5295\n",
            "train Loss: 0.5103, train Acc: train 70.93, train Recall: 69.92, train Precision: 91.75\n",
            "train Loss: 0.5103 Acc: 0.7093\n",
            "-----------------val-----------------\n",
            "Running Corrects: 547 Size: 780\n",
            "Running Loss: 494.47994923591614 Size: 780\n",
            "val Loss: 0.6339, val Acc: val 70.13, val Recall: 73.05, val Precision: 93.68\n",
            "val Loss: 0.6339 Acc: 0.7013\n",
            "\n",
            "Epoch 18/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3704 Size: 5295\n",
            "Running Loss: 2700.6290954351425 Size: 5295\n",
            "train Loss: 0.5100, train Acc: train 69.95, train Recall: 69.37, train Precision: 90.73\n",
            "train Loss: 0.5100 Acc: 0.6995\n",
            "-----------------val-----------------\n",
            "Running Corrects: 554 Size: 780\n",
            "Running Loss: 506.04352498054504 Size: 780\n",
            "val Loss: 0.6488, val Acc: val 71.03, val Recall: 74.64, val Precision: 91.40\n",
            "val Loss: 0.6488 Acc: 0.7103\n",
            "\n",
            "Epoch 19/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3756 Size: 5295\n",
            "Running Loss: 2653.955521285534 Size: 5295\n",
            "train Loss: 0.5012, train Acc: train 70.93, train Recall: 70.06, train Precision: 91.29\n",
            "train Loss: 0.5012 Acc: 0.7093\n",
            "-----------------val-----------------\n",
            "Running Corrects: 540 Size: 780\n",
            "Running Loss: 496.19991755485535 Size: 780\n",
            "val Loss: 0.6362, val Acc: val 69.23, val Recall: 72.79, val Precision: 92.46\n",
            "val Loss: 0.6362 Acc: 0.6923\n",
            "\n",
            "Epoch 20/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3713 Size: 5295\n",
            "Running Loss: 2647.555957555771 Size: 5295\n",
            "train Loss: 0.5000, train Acc: train 70.12, train Recall: 69.41, train Precision: 91.10\n",
            "train Loss: 0.5000 Acc: 0.7012\n",
            "-----------------val-----------------\n",
            "Running Corrects: 585 Size: 780\n",
            "Running Loss: 501.8710253238678 Size: 780\n",
            "val Loss: 0.6434, val Acc: val 75.00, val Recall: 77.21, val Precision: 93.33\n",
            "val Loss: 0.6434 Acc: 0.7500\n",
            "\n",
            "Epoch 21/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3679 Size: 5295\n",
            "Running Loss: 2705.776291459799 Size: 5295\n",
            "train Loss: 0.5110, train Acc: train 69.48, train Recall: 68.85, train Precision: 91.10\n",
            "train Loss: 0.5110 Acc: 0.6948\n",
            "-----------------val-----------------\n",
            "Running Corrects: 570 Size: 780\n",
            "Running Loss: 506.9477026462555 Size: 780\n",
            "val Loss: 0.6499, val Acc: val 73.08, val Recall: 76.95, val Precision: 90.18\n",
            "val Loss: 0.6499 Acc: 0.7308\n",
            "\n",
            "Epoch 22/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3757 Size: 5295\n",
            "Running Loss: 2660.473963677883 Size: 5295\n",
            "train Loss: 0.5025, train Acc: train 70.95, train Recall: 70.07, train Precision: 91.32\n",
            "train Loss: 0.5025 Acc: 0.7095\n",
            "-----------------val-----------------\n",
            "Running Corrects: 571 Size: 780\n",
            "Running Loss: 515.7364468574524 Size: 780\n",
            "val Loss: 0.6612, val Acc: val 73.21, val Recall: 78.25, val Precision: 87.72\n",
            "val Loss: 0.6612 Acc: 0.7321\n",
            "\n",
            "Epoch 23/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3822 Size: 5295\n",
            "Running Loss: 2555.547274529934 Size: 5295\n",
            "train Loss: 0.4826, train Acc: train 72.18, train Recall: 70.91, train Precision: 92.12\n",
            "train Loss: 0.4826 Acc: 0.7218\n",
            "-----------------val-----------------\n",
            "Running Corrects: 552 Size: 780\n",
            "Running Loss: 503.08806586265564 Size: 780\n",
            "val Loss: 0.6450, val Acc: val 70.77, val Recall: 75.60, val Precision: 88.60\n",
            "val Loss: 0.6450 Acc: 0.7077\n",
            "\n",
            "Epoch 24/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3774 Size: 5295\n",
            "Running Loss: 2606.2188163399696 Size: 5295\n",
            "train Loss: 0.4922, train Acc: train 71.27, train Recall: 70.26, train Precision: 91.63\n",
            "train Loss: 0.4922 Acc: 0.7127\n",
            "-----------------val-----------------\n",
            "Running Corrects: 576 Size: 780\n",
            "Running Loss: 507.3441367149353 Size: 780\n",
            "val Loss: 0.6504, val Acc: val 73.85, val Recall: 78.68, val Precision: 88.07\n",
            "val Loss: 0.6504 Acc: 0.7385\n",
            "\n",
            "Epoch 25/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3802 Size: 5295\n",
            "Running Loss: 2569.467959821224 Size: 5295\n",
            "train Loss: 0.4853, train Acc: train 71.80, train Recall: 70.64, train Precision: 91.91\n",
            "train Loss: 0.4853 Acc: 0.7180\n",
            "-----------------val-----------------\n",
            "Running Corrects: 566 Size: 780\n",
            "Running Loss: 507.5548515319824 Size: 780\n",
            "val Loss: 0.6507, val Acc: val 72.56, val Recall: 78.25, val Precision: 86.49\n",
            "val Loss: 0.6507 Acc: 0.7256\n",
            "\n",
            "Epoch 26/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3810 Size: 5295\n",
            "Running Loss: 2544.3660945892334 Size: 5295\n",
            "train Loss: 0.4805, train Acc: train 71.95, train Recall: 71.06, train Precision: 91.04\n",
            "train Loss: 0.4805 Acc: 0.7195\n",
            "-----------------val-----------------\n",
            "Running Corrects: 578 Size: 780\n",
            "Running Loss: 504.4390916824341 Size: 780\n",
            "val Loss: 0.6467, val Acc: val 74.10, val Recall: 78.48, val Precision: 88.95\n",
            "val Loss: 0.6467 Acc: 0.7410\n",
            "\n",
            "Epoch 27/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3878 Size: 5295\n",
            "Running Loss: 2536.4844550192356 Size: 5295\n",
            "train Loss: 0.4790, train Acc: train 73.24, train Recall: 71.74, train Precision: 92.50\n",
            "train Loss: 0.4790 Acc: 0.7324\n",
            "-----------------val-----------------\n",
            "Running Corrects: 574 Size: 780\n",
            "Running Loss: 511.28712463378906 Size: 780\n",
            "val Loss: 0.6555, val Acc: val 73.59, val Recall: 79.84, val Precision: 85.44\n",
            "val Loss: 0.6555 Acc: 0.7359\n",
            "\n",
            "Epoch 28/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3793 Size: 5295\n",
            "Running Loss: 2565.5320930182934 Size: 5295\n",
            "train Loss: 0.4845, train Acc: train 71.63, train Recall: 70.38, train Precision: 92.25\n",
            "train Loss: 0.4845 Acc: 0.7163\n",
            "-----------------val-----------------\n",
            "Running Corrects: 564 Size: 780\n",
            "Running Loss: 501.76656913757324 Size: 780\n",
            "val Loss: 0.6433, val Acc: val 72.31, val Recall: 77.74, val Precision: 87.02\n",
            "val Loss: 0.6433 Acc: 0.7231\n",
            "\n",
            "Epoch 29/29\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3847 Size: 5295\n",
            "Running Loss: 2494.1094784140587 Size: 5295\n",
            "train Loss: 0.4710, train Acc: train 72.65, train Recall: 71.51, train Precision: 91.60\n",
            "train Loss: 0.4710 Acc: 0.7265\n",
            "-----------------val-----------------\n",
            "Running Corrects: 572 Size: 780\n",
            "Running Loss: 508.7812194824219 Size: 780\n",
            "val Loss: 0.6523, val Acc: val 73.33, val Recall: 79.38, val Precision: 85.79\n",
            "val Loss: 0.6523 Acc: 0.7333\n",
            "\n",
            "Mean train accuracy: tensor(67.8942, device='cuda:0')\n",
            "Mean train recall: 91.5359173126615\n",
            "Mean train precision: 67.53831283169696\n",
            "Mean val accuracy: tensor(72.2735, device='cuda:0')\n",
            "Mean val recall: 91.43859649122807\n",
            "Mean val precision: 75.8653428705121\n",
            "MAX train accuracy: tensor(73.2389, device='cuda:0')\n",
            "MAX train recall: 93.55038759689923\n",
            "MAX train precision: 71.74122174122174\n",
            "MAX val accuracy: tensor(76.0256, device='cuda:0')\n",
            "MAX val recall: 98.59649122807018\n",
            "MAX val precision: 82.07705192629815\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"fb38db7c-d0b5-47d5-8a82-005b8f9fade9\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"fb38db7c-d0b5-47d5-8a82-005b8f9fade9\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'fb38db7c-d0b5-47d5-8a82-005b8f9fade9',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=t<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=t\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=t\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"xaxis\": \"x\", \"y\": [58.47025680541992, 61.54863357543945, 61.77526092529297, 62.870635986328125, 62.28517532348633, 63.871578216552734, 64.41926574707031, 65.21246337890625, 65.6279525756836, 66.66667175292969, 66.5722427368164, 67.25212860107422, 67.23323822021484, 68.85741424560547, 68.2530746459961, 69.34844207763672, 69.44287109375, 70.93484497070312, 69.95278930664062, 70.93484497070312, 70.12275695800781, 69.48064422607422, 70.9537353515625, 72.18130493164062, 71.27478790283203, 71.8035888671875, 71.95467376708984, 73.23890686035156, 71.63362121582031, 72.65345001220703], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=v<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=v\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"xaxis\": \"x\", \"y\": [75.64102935791016, 74.61538696289062, 70.25640869140625, 71.66667175292969, 69.61538696289062, 71.4102554321289, 70.76923370361328, 71.4102554321289, 70.51282501220703, 76.02564239501953, 70.51282501220703, 71.92308044433594, 73.5897445678711, 73.46154022216797, 71.53846740722656, 70.8974380493164, 72.17948913574219, 70.12820434570312, 71.02564239501953, 69.23077392578125, 75.0, 73.0769271850586, 73.20513153076172, 70.76923370361328, 73.84615325927734, 72.56410217285156, 74.10256958007812, 73.5897445678711, 72.30769348144531, 73.33333587646484], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fb38db7c-d0b5-47d5-8a82-005b8f9fade9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"ce7a8a17-1eaf-4989-af5e-e110a10eb121\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"ce7a8a17-1eaf-4989-af5e-e110a10eb121\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        'ce7a8a17-1eaf-4989-af5e-e110a10eb121',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=t<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=t\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=t\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"xaxis\": \"x\", \"y\": [0.8169027933543082, 0.629815114568831, 0.6151164226536485, 0.602677819265072, 0.5920872314118123, 0.5803322208614368, 0.569240671446911, 0.5568446727492429, 0.5564286982258949, 0.5496691592016572, 0.5458643017792274, 0.5419057951657923, 0.5335829438861527, 0.525724904778096, 0.5228362888576176, 0.524872460558912, 0.5167519287639795, 0.5103416420526388, 0.5100338235004991, 0.5012191730473152, 0.5000105679992013, 0.5110059096241357, 0.5024502292120648, 0.48263404618129063, 0.49220374246269494, 0.4852630707877666, 0.4805223974672773, 0.4790338914106205, 0.4845197531668165, 0.47103106296771646], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=v<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=v\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30], \"xaxis\": \"x\", \"y\": [0.6521210007178477, 0.6411228861564245, 0.6372459585850055, 0.6424562469506875, 0.6439304743057642, 0.6501523335774739, 0.6451077858606974, 0.6352759248171097, 0.6443062528585777, 0.6603128540210235, 0.6536313698841976, 0.6493075930155241, 0.6645304294732901, 0.6621387756787813, 0.6605956389353825, 0.6478754297280923, 0.6454575242140355, 0.6339486528665591, 0.6487737499750578, 0.6361537404549428, 0.6434243914408562, 0.649932952110584, 0.6612005728941698, 0.6449846998239175, 0.6504412009165838, 0.6507113481179262, 0.6467167842082489, 0.6554963136330629, 0.643290473253299, 0.6522836147210537], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ce7a8a17-1eaf-4989-af5e-e110a10eb121');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete in 23m 42s\n",
            "Best val Acc: 0.760256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05wJjL4HfMVM"
      },
      "source": [
        "model_ft = models.vgg13(pretrained=True)\n",
        "\n",
        "for param in model_ft.features.parameters():\n",
        "  param.requires_grad_(False)\n",
        "\n",
        "feats_list = list(model_ft.features)\n",
        "new_feats_list = []\n",
        "for feat in feats_list:\n",
        "    new_feats_list.append(feat)\n",
        "    if isinstance(feat, nn.Conv2d):\n",
        "        new_feats_list.append(nn.Dropout(p=0.5, inplace=True))\n",
        "\n",
        "# modify convolution layers\n",
        "model_ft.features = nn.Sequential(*new_feats_list)\n",
        "\n",
        "model_ft.classifier[-1] = nn.Linear(in_features=4096, out_features=2)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight.to(device))\n",
        "\n",
        "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_ft = optim.AdamW(model_ft.parameters(), lr=0.0001)\n",
        "\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YvKbAul-fQWI",
        "outputId": "c5fad68d-e1b4-4762-f897-de557c063507"
      },
      "source": [
        "model_ft = train_model(model_ft, criterion, optimizer_ft, dataloaders, num_epochs=100)\n",
        "# Save model\n",
        "torch.save(model_ft.state_dict(), 'vgg13_AdamW_100_0.2.pt')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3060 Size: 5295\n",
            "Running Loss: 4433.1400846242905 Size: 5295\n",
            "train Loss: 0.8372, train Acc: train 57.79, train Recall: 61.63, train Precision: 81.36\n",
            "train Loss: 0.8372 Acc: 0.5779\n",
            "-----------------val-----------------\n",
            "Running Corrects: 568 Size: 780\n",
            "Running Loss: 495.8306658267975 Size: 780\n",
            "val Loss: 0.6357, val Acc: val 72.82, val Recall: 73.01, val Precision: 99.65\n",
            "val Loss: 0.6357 Acc: 0.7282\n",
            "\n",
            "Epoch 1/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3218 Size: 5295\n",
            "Running Loss: 3320.4464549422264 Size: 5295\n",
            "train Loss: 0.6271, train Acc: train 60.77, train Recall: 62.08, train Precision: 91.47\n",
            "train Loss: 0.6271 Acc: 0.6077\n",
            "-----------------val-----------------\n",
            "Running Corrects: 570 Size: 780\n",
            "Running Loss: 484.8841338157654 Size: 780\n",
            "val Loss: 0.6216, val Acc: val 73.08, val Recall: 73.08, val Precision: 100.00\n",
            "val Loss: 0.6216 Acc: 0.7308\n",
            "\n",
            "Epoch 2/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3230 Size: 5295\n",
            "Running Loss: 3277.5369383096695 Size: 5295\n",
            "train Loss: 0.6190, train Acc: train 61.00, train Recall: 62.00, train Precision: 92.93\n",
            "train Loss: 0.6190 Acc: 0.6100\n",
            "-----------------val-----------------\n",
            "Running Corrects: 569 Size: 780\n",
            "Running Loss: 495.9075086116791 Size: 780\n",
            "val Loss: 0.6358, val Acc: val 72.95, val Recall: 73.04, val Precision: 99.82\n",
            "val Loss: 0.6358 Acc: 0.7295\n",
            "\n",
            "Epoch 3/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3263 Size: 5295\n",
            "Running Loss: 3172.466487824917 Size: 5295\n",
            "train Loss: 0.5991, train Acc: train 61.62, train Recall: 62.45, train Precision: 92.78\n",
            "train Loss: 0.5991 Acc: 0.6162\n",
            "-----------------val-----------------\n",
            "Running Corrects: 569 Size: 780\n",
            "Running Loss: 506.50629138946533 Size: 780\n",
            "val Loss: 0.6494, val Acc: val 72.95, val Recall: 73.10, val Precision: 99.65\n",
            "val Loss: 0.6494 Acc: 0.7295\n",
            "\n",
            "Epoch 4/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3387 Size: 5295\n",
            "Running Loss: 3103.465240418911 Size: 5295\n",
            "train Loss: 0.5861, train Acc: train 63.97, train Recall: 63.88, train Precision: 93.95\n",
            "train Loss: 0.5861 Acc: 0.6397\n",
            "-----------------val-----------------\n",
            "Running Corrects: 570 Size: 780\n",
            "Running Loss: 479.60882782936096 Size: 780\n",
            "val Loss: 0.6149, val Acc: val 73.08, val Recall: 73.08, val Precision: 100.00\n",
            "val Loss: 0.6149 Acc: 0.7308\n",
            "\n",
            "Epoch 5/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3394 Size: 5295\n",
            "Running Loss: 3030.593521654606 Size: 5295\n",
            "train Loss: 0.5724, train Acc: train 64.10, train Recall: 64.08, train Precision: 93.43\n",
            "train Loss: 0.5724 Acc: 0.6410\n",
            "-----------------val-----------------\n",
            "Running Corrects: 557 Size: 780\n",
            "Running Loss: 510.98240423202515 Size: 780\n",
            "val Loss: 0.6551, val Acc: val 71.41, val Recall: 74.75, val Precision: 91.93\n",
            "val Loss: 0.6551 Acc: 0.7141\n",
            "\n",
            "Epoch 6/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3381 Size: 5295\n",
            "Running Loss: 3019.6289840340614 Size: 5295\n",
            "train Loss: 0.5703, train Acc: train 63.85, train Recall: 64.29, train Precision: 91.44\n",
            "train Loss: 0.5703 Acc: 0.6385\n",
            "-----------------val-----------------\n",
            "Running Corrects: 565 Size: 780\n",
            "Running Loss: 500.9391005039215 Size: 780\n",
            "val Loss: 0.6422, val Acc: val 72.44, val Recall: 72.90, val Precision: 99.12\n",
            "val Loss: 0.6422 Acc: 0.7244\n",
            "\n",
            "Epoch 7/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3514 Size: 5295\n",
            "Running Loss: 2978.3875393271446 Size: 5295\n",
            "train Loss: 0.5625, train Acc: train 66.36, train Recall: 66.08, train Precision: 92.00\n",
            "train Loss: 0.5625 Acc: 0.6636\n",
            "-----------------val-----------------\n",
            "Running Corrects: 558 Size: 780\n",
            "Running Loss: 502.0305869579315 Size: 780\n",
            "val Loss: 0.6436, val Acc: val 71.54, val Recall: 72.83, val Precision: 97.37\n",
            "val Loss: 0.6436 Acc: 0.7154\n",
            "\n",
            "Epoch 8/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3527 Size: 5295\n",
            "Running Loss: 2931.0155451893806 Size: 5295\n",
            "train Loss: 0.5535, train Acc: train 66.61, train Recall: 66.14, train Precision: 92.56\n",
            "train Loss: 0.5535 Acc: 0.6661\n",
            "-----------------val-----------------\n",
            "Running Corrects: 570 Size: 780\n",
            "Running Loss: 502.7753469944 Size: 780\n",
            "val Loss: 0.6446, val Acc: val 73.08, val Recall: 74.26, val Precision: 96.67\n",
            "val Loss: 0.6446 Acc: 0.7308\n",
            "\n",
            "Epoch 9/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3546 Size: 5295\n",
            "Running Loss: 2934.2034174203873 Size: 5295\n",
            "train Loss: 0.5541, train Acc: train 66.97, train Recall: 66.59, train Precision: 91.84\n",
            "train Loss: 0.5541 Acc: 0.6697\n",
            "-----------------val-----------------\n",
            "Running Corrects: 568 Size: 780\n",
            "Running Loss: 507.480788230896 Size: 780\n",
            "val Loss: 0.6506, val Acc: val 72.82, val Recall: 73.61, val Precision: 97.89\n",
            "val Loss: 0.6506 Acc: 0.7282\n",
            "\n",
            "Epoch 10/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3563 Size: 5295\n",
            "Running Loss: 2846.791260778904 Size: 5295\n",
            "train Loss: 0.5376, train Acc: train 67.29, train Recall: 66.44, train Precision: 93.55\n",
            "train Loss: 0.5376 Acc: 0.6729\n",
            "-----------------val-----------------\n",
            "Running Corrects: 586 Size: 780\n",
            "Running Loss: 502.4340364933014 Size: 780\n",
            "val Loss: 0.6441, val Acc: val 75.13, val Recall: 76.11, val Precision: 96.14\n",
            "val Loss: 0.6441 Acc: 0.7513\n",
            "\n",
            "Epoch 11/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3569 Size: 5295\n",
            "Running Loss: 2853.330445587635 Size: 5295\n",
            "train Loss: 0.5389, train Acc: train 67.40, train Recall: 66.94, train Precision: 91.84\n",
            "train Loss: 0.5389 Acc: 0.6740\n",
            "-----------------val-----------------\n",
            "Running Corrects: 570 Size: 780\n",
            "Running Loss: 491.0508658885956 Size: 780\n",
            "val Loss: 0.6296, val Acc: val 73.08, val Recall: 73.75, val Precision: 98.07\n",
            "val Loss: 0.6296 Acc: 0.7308\n",
            "\n",
            "Epoch 12/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3597 Size: 5295\n",
            "Running Loss: 2810.510164320469 Size: 5295\n",
            "train Loss: 0.5308, train Acc: train 67.93, train Recall: 67.41, train Precision: 91.66\n",
            "train Loss: 0.5308 Acc: 0.6793\n",
            "-----------------val-----------------\n",
            "Running Corrects: 581 Size: 780\n",
            "Running Loss: 501.56574296951294 Size: 780\n",
            "val Loss: 0.6430, val Acc: val 74.49, val Recall: 76.54, val Precision: 93.86\n",
            "val Loss: 0.6430 Acc: 0.7449\n",
            "\n",
            "Epoch 13/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3624 Size: 5295\n",
            "Running Loss: 2801.174679815769 Size: 5295\n",
            "train Loss: 0.5290, train Acc: train 68.44, train Recall: 67.99, train Precision: 91.07\n",
            "train Loss: 0.5290 Acc: 0.6844\n",
            "-----------------val-----------------\n",
            "Running Corrects: 593 Size: 780\n",
            "Running Loss: 506.5672423839569 Size: 780\n",
            "val Loss: 0.6494, val Acc: val 76.03, val Recall: 77.01, val Precision: 95.79\n",
            "val Loss: 0.6494 Acc: 0.7603\n",
            "\n",
            "Epoch 14/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3594 Size: 5295\n",
            "Running Loss: 2791.4208739995956 Size: 5295\n",
            "train Loss: 0.5272, train Acc: train 67.88, train Recall: 67.34, train Precision: 91.75\n",
            "train Loss: 0.5272 Acc: 0.6788\n",
            "-----------------val-----------------\n",
            "Running Corrects: 589 Size: 780\n",
            "Running Loss: 500.65315198898315 Size: 780\n",
            "val Loss: 0.6419, val Acc: val 75.51, val Recall: 77.42, val Precision: 93.86\n",
            "val Loss: 0.6419 Acc: 0.7551\n",
            "\n",
            "Epoch 15/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3696 Size: 5295\n",
            "Running Loss: 2739.448166549206 Size: 5295\n",
            "train Loss: 0.5174, train Acc: train 69.80, train Recall: 68.99, train Precision: 91.60\n",
            "train Loss: 0.5174 Acc: 0.6980\n",
            "-----------------val-----------------\n",
            "Running Corrects: 598 Size: 780\n",
            "Running Loss: 515.0287871360779 Size: 780\n",
            "val Loss: 0.6603, val Acc: val 76.67, val Recall: 81.39, val Precision: 88.25\n",
            "val Loss: 0.6603 Acc: 0.7667\n",
            "\n",
            "Epoch 16/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3687 Size: 5295\n",
            "Running Loss: 2737.967806696892 Size: 5295\n",
            "train Loss: 0.5171, train Acc: train 69.63, train Recall: 69.20, train Precision: 90.36\n",
            "train Loss: 0.5171 Acc: 0.6963\n",
            "-----------------val-----------------\n",
            "Running Corrects: 564 Size: 780\n",
            "Running Loss: 481.7059681415558 Size: 780\n",
            "val Loss: 0.6176, val Acc: val 72.31, val Recall: 73.60, val Precision: 96.84\n",
            "val Loss: 0.6176 Acc: 0.7231\n",
            "\n",
            "Epoch 17/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3699 Size: 5295\n",
            "Running Loss: 2768.7314633727074 Size: 5295\n",
            "train Loss: 0.5229, train Acc: train 69.86, train Recall: 69.22, train Precision: 90.95\n",
            "train Loss: 0.5229 Acc: 0.6986\n",
            "-----------------val-----------------\n",
            "Running Corrects: 569 Size: 780\n",
            "Running Loss: 504.5252537727356 Size: 780\n",
            "val Loss: 0.6468, val Acc: val 72.95, val Recall: 74.97, val Precision: 94.56\n",
            "val Loss: 0.6468 Acc: 0.7295\n",
            "\n",
            "Epoch 18/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3712 Size: 5295\n",
            "Running Loss: 2718.5276107788086 Size: 5295\n",
            "train Loss: 0.5134, train Acc: train 70.10, train Recall: 69.17, train Precision: 91.84\n",
            "train Loss: 0.5134 Acc: 0.7010\n",
            "-----------------val-----------------\n",
            "Running Corrects: 588 Size: 780\n",
            "Running Loss: 513.2709052562714 Size: 780\n",
            "val Loss: 0.6580, val Acc: val 75.38, val Recall: 80.29, val Precision: 87.89\n",
            "val Loss: 0.6580 Acc: 0.7538\n",
            "\n",
            "Epoch 19/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3705 Size: 5295\n",
            "Running Loss: 2701.075007855892 Size: 5295\n",
            "train Loss: 0.5101, train Acc: train 69.97, train Recall: 69.28, train Precision: 91.10\n",
            "train Loss: 0.5101 Acc: 0.6997\n",
            "-----------------val-----------------\n",
            "Running Corrects: 578 Size: 780\n",
            "Running Loss: 492.7506229877472 Size: 780\n",
            "val Loss: 0.6317, val Acc: val 74.10, val Recall: 75.27, val Precision: 96.14\n",
            "val Loss: 0.6317 Acc: 0.7410\n",
            "\n",
            "Epoch 20/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3707 Size: 5295\n",
            "Running Loss: 2669.7901321947575 Size: 5295\n",
            "train Loss: 0.5042, train Acc: train 70.01, train Recall: 69.29, train Precision: 91.16\n",
            "train Loss: 0.5042 Acc: 0.7001\n",
            "-----------------val-----------------\n",
            "Running Corrects: 599 Size: 780\n",
            "Running Loss: 500.0696098804474 Size: 780\n",
            "val Loss: 0.6411, val Acc: val 76.79, val Recall: 79.34, val Precision: 92.28\n",
            "val Loss: 0.6411 Acc: 0.7679\n",
            "\n",
            "Epoch 21/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3731 Size: 5295\n",
            "Running Loss: 2657.9103323221207 Size: 5295\n",
            "train Loss: 0.5020, train Acc: train 70.46, train Recall: 69.62, train Precision: 91.38\n",
            "train Loss: 0.5020 Acc: 0.7046\n",
            "-----------------val-----------------\n",
            "Running Corrects: 600 Size: 780\n",
            "Running Loss: 515.9926993846893 Size: 780\n",
            "val Loss: 0.6615, val Acc: val 76.92, val Recall: 83.51, val Precision: 85.26\n",
            "val Loss: 0.6615 Acc: 0.7692\n",
            "\n",
            "Epoch 22/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3785 Size: 5295\n",
            "Running Loss: 2582.3351392149925 Size: 5295\n",
            "train Loss: 0.4877, train Acc: train 71.48, train Recall: 70.50, train Precision: 91.44\n",
            "train Loss: 0.4877 Acc: 0.7148\n",
            "-----------------val-----------------\n",
            "Running Corrects: 573 Size: 780\n",
            "Running Loss: 508.10654878616333 Size: 780\n",
            "val Loss: 0.6514, val Acc: val 73.46, val Recall: 76.57, val Precision: 91.75\n",
            "val Loss: 0.6514 Acc: 0.7346\n",
            "\n",
            "Epoch 23/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3809 Size: 5295\n",
            "Running Loss: 2586.725582808256 Size: 5295\n",
            "train Loss: 0.4885, train Acc: train 71.94, train Recall: 70.90, train Precision: 91.47\n",
            "train Loss: 0.4885 Acc: 0.7194\n",
            "-----------------val-----------------\n",
            "Running Corrects: 570 Size: 780\n",
            "Running Loss: 512.4826679229736 Size: 780\n",
            "val Loss: 0.6570, val Acc: val 73.08, val Recall: 77.27, val Precision: 89.47\n",
            "val Loss: 0.6570 Acc: 0.7308\n",
            "\n",
            "Epoch 24/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3735 Size: 5295\n",
            "Running Loss: 2644.1139429807663 Size: 5295\n",
            "train Loss: 0.4994, train Acc: train 70.54, train Recall: 69.44, train Precision: 92.22\n",
            "train Loss: 0.4994 Acc: 0.7054\n",
            "-----------------val-----------------\n",
            "Running Corrects: 582 Size: 780\n",
            "Running Loss: 489.3239200115204 Size: 780\n",
            "val Loss: 0.6273, val Acc: val 74.62, val Recall: 76.42, val Precision: 94.39\n",
            "val Loss: 0.6273 Acc: 0.7462\n",
            "\n",
            "Epoch 25/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3803 Size: 5295\n",
            "Running Loss: 2573.265590429306 Size: 5295\n",
            "train Loss: 0.4860, train Acc: train 71.82, train Recall: 70.73, train Precision: 91.66\n",
            "train Loss: 0.4860 Acc: 0.7182\n",
            "-----------------val-----------------\n",
            "Running Corrects: 586 Size: 780\n",
            "Running Loss: 495.57533502578735 Size: 780\n",
            "val Loss: 0.6354, val Acc: val 75.13, val Recall: 77.89, val Precision: 92.11\n",
            "val Loss: 0.6354 Acc: 0.7513\n",
            "\n",
            "Epoch 26/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3843 Size: 5295\n",
            "Running Loss: 2551.6377549767494 Size: 5295\n",
            "train Loss: 0.4819, train Acc: train 72.58, train Recall: 71.36, train Precision: 91.84\n",
            "train Loss: 0.4819 Acc: 0.7258\n",
            "-----------------val-----------------\n",
            "Running Corrects: 563 Size: 780\n",
            "Running Loss: 508.9245533943176 Size: 780\n",
            "val Loss: 0.6525, val Acc: val 72.18, val Recall: 76.62, val Precision: 89.12\n",
            "val Loss: 0.6525 Acc: 0.7218\n",
            "\n",
            "Epoch 27/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3883 Size: 5295\n",
            "Running Loss: 2491.34989130497 Size: 5295\n",
            "train Loss: 0.4705, train Acc: train 73.33, train Recall: 72.13, train Precision: 91.63\n",
            "train Loss: 0.4705 Acc: 0.7333\n",
            "-----------------val-----------------\n",
            "Running Corrects: 578 Size: 780\n",
            "Running Loss: 490.9552638530731 Size: 780\n",
            "val Loss: 0.6294, val Acc: val 74.10, val Recall: 75.84, val Precision: 94.74\n",
            "val Loss: 0.6294 Acc: 0.7410\n",
            "\n",
            "Epoch 28/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3863 Size: 5295\n",
            "Running Loss: 2495.882724404335 Size: 5295\n",
            "train Loss: 0.4714, train Acc: train 72.96, train Recall: 71.60, train Precision: 92.16\n",
            "train Loss: 0.4714 Acc: 0.7296\n",
            "-----------------val-----------------\n",
            "Running Corrects: 587 Size: 780\n",
            "Running Loss: 501.55677604675293 Size: 780\n",
            "val Loss: 0.6430, val Acc: val 75.26, val Recall: 78.52, val Precision: 91.05\n",
            "val Loss: 0.6430 Acc: 0.7526\n",
            "\n",
            "Epoch 29/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3864 Size: 5295\n",
            "Running Loss: 2488.929912865162 Size: 5295\n",
            "train Loss: 0.4701, train Acc: train 72.97, train Recall: 71.95, train Precision: 91.16\n",
            "train Loss: 0.4701 Acc: 0.7297\n",
            "-----------------val-----------------\n",
            "Running Corrects: 587 Size: 780\n",
            "Running Loss: 505.32185792922974 Size: 780\n",
            "val Loss: 0.6478, val Acc: val 75.26, val Recall: 78.87, val Precision: 90.35\n",
            "val Loss: 0.6478 Acc: 0.7526\n",
            "\n",
            "Epoch 30/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3848 Size: 5295\n",
            "Running Loss: 2502.5208311378956 Size: 5295\n",
            "train Loss: 0.4726, train Acc: train 72.67, train Recall: 71.57, train Precision: 91.47\n",
            "train Loss: 0.4726 Acc: 0.7267\n",
            "-----------------val-----------------\n",
            "Running Corrects: 582 Size: 780\n",
            "Running Loss: 497.4272794723511 Size: 780\n",
            "val Loss: 0.6377, val Acc: val 74.62, val Recall: 77.84, val Precision: 91.23\n",
            "val Loss: 0.6377 Acc: 0.7462\n",
            "\n",
            "Epoch 31/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3899 Size: 5295\n",
            "Running Loss: 2468.664000183344 Size: 5295\n",
            "train Loss: 0.4662, train Acc: train 73.64, train Recall: 72.42, train Precision: 91.60\n",
            "train Loss: 0.4662 Acc: 0.7364\n",
            "-----------------val-----------------\n",
            "Running Corrects: 602 Size: 780\n",
            "Running Loss: 496.0921883583069 Size: 780\n",
            "val Loss: 0.6360, val Acc: val 77.18, val Recall: 80.25, val Precision: 91.23\n",
            "val Loss: 0.6360 Acc: 0.7718\n",
            "\n",
            "Epoch 32/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3934 Size: 5295\n",
            "Running Loss: 2450.1204336881638 Size: 5295\n",
            "train Loss: 0.4627, train Acc: train 74.30, train Recall: 73.18, train Precision: 91.22\n",
            "train Loss: 0.4627 Acc: 0.7430\n",
            "-----------------val-----------------\n",
            "Running Corrects: 599 Size: 780\n",
            "Running Loss: 506.47134351730347 Size: 780\n",
            "val Loss: 0.6493, val Acc: val 76.79, val Recall: 80.34, val Precision: 90.35\n",
            "val Loss: 0.6493 Acc: 0.7679\n",
            "\n",
            "Epoch 33/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3917 Size: 5295\n",
            "Running Loss: 2439.8791850209236 Size: 5295\n",
            "train Loss: 0.4608, train Acc: train 73.98, train Recall: 72.61, train Precision: 91.97\n",
            "train Loss: 0.4608 Acc: 0.7398\n",
            "-----------------val-----------------\n",
            "Running Corrects: 598 Size: 780\n",
            "Running Loss: 513.9265229701996 Size: 780\n",
            "val Loss: 0.6589, val Acc: val 76.67, val Recall: 84.15, val Precision: 83.86\n",
            "val Loss: 0.6589 Acc: 0.7667\n",
            "\n",
            "Epoch 34/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3933 Size: 5295\n",
            "Running Loss: 2422.756567001343 Size: 5295\n",
            "train Loss: 0.4576, train Acc: train 74.28, train Recall: 72.85, train Precision: 92.09\n",
            "train Loss: 0.4576 Acc: 0.7428\n",
            "-----------------val-----------------\n",
            "Running Corrects: 576 Size: 780\n",
            "Running Loss: 493.36132621765137 Size: 780\n",
            "val Loss: 0.6325, val Acc: val 73.85, val Recall: 77.73, val Precision: 90.00\n",
            "val Loss: 0.6325 Acc: 0.7385\n",
            "\n",
            "Epoch 35/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3949 Size: 5295\n",
            "Running Loss: 2391.6524731218815 Size: 5295\n",
            "train Loss: 0.4517, train Acc: train 74.58, train Recall: 73.35, train Precision: 91.50\n",
            "train Loss: 0.4517 Acc: 0.7458\n",
            "-----------------val-----------------\n",
            "Running Corrects: 578 Size: 780\n",
            "Running Loss: 517.1425812244415 Size: 780\n",
            "val Loss: 0.6630, val Acc: val 74.10, val Recall: 86.36, val Precision: 76.67\n",
            "val Loss: 0.6630 Acc: 0.7410\n",
            "\n",
            "Epoch 36/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3912 Size: 5295\n",
            "Running Loss: 2396.620430767536 Size: 5295\n",
            "train Loss: 0.4526, train Acc: train 73.88, train Recall: 72.92, train Precision: 90.85\n",
            "train Loss: 0.4526 Acc: 0.7388\n",
            "-----------------val-----------------\n",
            "Running Corrects: 608 Size: 780\n",
            "Running Loss: 502.1483438014984 Size: 780\n",
            "val Loss: 0.6438, val Acc: val 77.95, val Recall: 83.50, val Precision: 87.02\n",
            "val Loss: 0.6438 Acc: 0.7795\n",
            "\n",
            "Epoch 37/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3962 Size: 5295\n",
            "Running Loss: 2367.8548235297203 Size: 5295\n",
            "train Loss: 0.4472, train Acc: train 74.83, train Recall: 73.39, train Precision: 92.03\n",
            "train Loss: 0.4472 Acc: 0.7483\n",
            "-----------------val-----------------\n",
            "Running Corrects: 568 Size: 780\n",
            "Running Loss: 509.7797820568085 Size: 780\n",
            "val Loss: 0.6536, val Acc: val 72.82, val Recall: 81.51, val Precision: 81.23\n",
            "val Loss: 0.6536 Acc: 0.7282\n",
            "\n",
            "Epoch 38/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4029 Size: 5295\n",
            "Running Loss: 2335.9597529172897 Size: 5295\n",
            "train Loss: 0.4412, train Acc: train 76.09, train Recall: 75.10, train Precision: 90.88\n",
            "train Loss: 0.4412 Acc: 0.7609\n",
            "-----------------val-----------------\n",
            "Running Corrects: 593 Size: 780\n",
            "Running Loss: 506.7706878185272 Size: 780\n",
            "val Loss: 0.6497, val Acc: val 76.03, val Recall: 83.54, val Precision: 83.68\n",
            "val Loss: 0.6497 Acc: 0.7603\n",
            "\n",
            "Epoch 39/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3978 Size: 5295\n",
            "Running Loss: 2350.5558110773563 Size: 5295\n",
            "train Loss: 0.4439, train Acc: train 75.13, train Recall: 73.61, train Precision: 92.22\n",
            "train Loss: 0.4439 Acc: 0.7513\n",
            "-----------------val-----------------\n",
            "Running Corrects: 581 Size: 780\n",
            "Running Loss: 507.6892440319061 Size: 780\n",
            "val Loss: 0.6509, val Acc: val 74.49, val Recall: 82.83, val Precision: 82.11\n",
            "val Loss: 0.6509 Acc: 0.7449\n",
            "\n",
            "Epoch 40/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4001 Size: 5295\n",
            "Running Loss: 2359.6400559544563 Size: 5295\n",
            "train Loss: 0.4456, train Acc: train 75.56, train Recall: 74.10, train Precision: 92.06\n",
            "train Loss: 0.4456 Acc: 0.7556\n",
            "-----------------val-----------------\n",
            "Running Corrects: 598 Size: 780\n",
            "Running Loss: 506.7291898727417 Size: 780\n",
            "val Loss: 0.6497, val Acc: val 76.67, val Recall: 84.04, val Precision: 84.04\n",
            "val Loss: 0.6497 Acc: 0.7667\n",
            "\n",
            "Epoch 41/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3959 Size: 5295\n",
            "Running Loss: 2372.57450774312 Size: 5295\n",
            "train Loss: 0.4481, train Acc: train 74.77, train Recall: 73.78, train Precision: 90.85\n",
            "train Loss: 0.4481 Acc: 0.7477\n",
            "-----------------val-----------------\n",
            "Running Corrects: 575 Size: 780\n",
            "Running Loss: 496.4342291355133 Size: 780\n",
            "val Loss: 0.6365, val Acc: val 73.72, val Recall: 77.95, val Precision: 89.30\n",
            "val Loss: 0.6365 Acc: 0.7372\n",
            "\n",
            "Epoch 42/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3960 Size: 5295\n",
            "Running Loss: 2355.4402027726173 Size: 5295\n",
            "train Loss: 0.4448, train Acc: train 74.79, train Recall: 73.52, train Precision: 91.60\n",
            "train Loss: 0.4448 Acc: 0.7479\n",
            "-----------------val-----------------\n",
            "Running Corrects: 580 Size: 780\n",
            "Running Loss: 509.86166882514954 Size: 780\n",
            "val Loss: 0.6537, val Acc: val 74.36, val Recall: 80.23, val Precision: 86.14\n",
            "val Loss: 0.6537 Acc: 0.7436\n",
            "\n",
            "Epoch 43/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4023 Size: 5295\n",
            "Running Loss: 2292.051982998848 Size: 5295\n",
            "train Loss: 0.4329, train Acc: train 75.98, train Recall: 74.60, train Precision: 91.81\n",
            "train Loss: 0.4329 Acc: 0.7598\n",
            "-----------------val-----------------\n",
            "Running Corrects: 577 Size: 780\n",
            "Running Loss: 488.41701769828796 Size: 780\n",
            "val Loss: 0.6262, val Acc: val 73.97, val Recall: 76.87, val Precision: 92.11\n",
            "val Loss: 0.6262 Acc: 0.7397\n",
            "\n",
            "Epoch 44/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4063 Size: 5295\n",
            "Running Loss: 2253.06651365757 Size: 5295\n",
            "train Loss: 0.4255, train Acc: train 76.73, train Recall: 74.92, train Precision: 92.90\n",
            "train Loss: 0.4255 Acc: 0.7673\n",
            "-----------------val-----------------\n",
            "Running Corrects: 572 Size: 780\n",
            "Running Loss: 508.9217667579651 Size: 780\n",
            "val Loss: 0.6525, val Acc: val 73.33, val Recall: 82.32, val Precision: 80.88\n",
            "val Loss: 0.6525 Acc: 0.7333\n",
            "\n",
            "Epoch 45/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4059 Size: 5295\n",
            "Running Loss: 2287.503713965416 Size: 5295\n",
            "train Loss: 0.4320, train Acc: train 76.66, train Recall: 75.23, train Precision: 91.94\n",
            "train Loss: 0.4320 Acc: 0.7666\n",
            "-----------------val-----------------\n",
            "Running Corrects: 575 Size: 780\n",
            "Running Loss: 492.56125831604004 Size: 780\n",
            "val Loss: 0.6315, val Acc: val 73.72, val Recall: 77.78, val Precision: 89.65\n",
            "val Loss: 0.6315 Acc: 0.7372\n",
            "\n",
            "Epoch 46/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4068 Size: 5295\n",
            "Running Loss: 2251.64668110013 Size: 5295\n",
            "train Loss: 0.4252, train Acc: train 76.83, train Recall: 75.15, train Precision: 92.56\n",
            "train Loss: 0.4252 Acc: 0.7683\n",
            "-----------------val-----------------\n",
            "Running Corrects: 575 Size: 780\n",
            "Running Loss: 491.19360184669495 Size: 780\n",
            "val Loss: 0.6297, val Acc: val 73.72, val Recall: 77.20, val Precision: 90.88\n",
            "val Loss: 0.6297 Acc: 0.7372\n",
            "\n",
            "Epoch 47/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4067 Size: 5295\n",
            "Running Loss: 2220.478871434927 Size: 5295\n",
            "train Loss: 0.4194, train Acc: train 76.81, train Recall: 75.53, train Precision: 91.60\n",
            "train Loss: 0.4194 Acc: 0.7681\n",
            "-----------------val-----------------\n",
            "Running Corrects: 586 Size: 780\n",
            "Running Loss: 489.5486168861389 Size: 780\n",
            "val Loss: 0.6276, val Acc: val 75.13, val Recall: 78.23, val Precision: 91.40\n",
            "val Loss: 0.6276 Acc: 0.7513\n",
            "\n",
            "Epoch 48/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4124 Size: 5295\n",
            "Running Loss: 2195.954210639 Size: 5295\n",
            "train Loss: 0.4147, train Acc: train 77.88, train Recall: 76.44, train Precision: 92.06\n",
            "train Loss: 0.4147 Acc: 0.7788\n",
            "-----------------val-----------------\n",
            "Running Corrects: 562 Size: 780\n",
            "Running Loss: 514.0862579345703 Size: 780\n",
            "val Loss: 0.6591, val Acc: val 72.05, val Recall: 82.71, val Precision: 78.07\n",
            "val Loss: 0.6591 Acc: 0.7205\n",
            "\n",
            "Epoch 49/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4130 Size: 5295\n",
            "Running Loss: 2199.674714356661 Size: 5295\n",
            "train Loss: 0.4154, train Acc: train 78.00, train Recall: 76.17, train Precision: 92.96\n",
            "train Loss: 0.4154 Acc: 0.7800\n",
            "-----------------val-----------------\n",
            "Running Corrects: 576 Size: 780\n",
            "Running Loss: 502.11056447029114 Size: 780\n",
            "val Loss: 0.6437, val Acc: val 73.85, val Recall: 80.00, val Precision: 85.61\n",
            "val Loss: 0.6437 Acc: 0.7385\n",
            "\n",
            "Epoch 50/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4111 Size: 5295\n",
            "Running Loss: 2179.0233135819435 Size: 5295\n",
            "train Loss: 0.4115, train Acc: train 77.64, train Recall: 76.20, train Precision: 92.03\n",
            "train Loss: 0.4115 Acc: 0.7764\n",
            "-----------------val-----------------\n",
            "Running Corrects: 557 Size: 780\n",
            "Running Loss: 509.47532892227173 Size: 780\n",
            "val Loss: 0.6532, val Acc: val 71.41, val Recall: 79.86, val Precision: 81.40\n",
            "val Loss: 0.6532 Acc: 0.7141\n",
            "\n",
            "Epoch 51/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4120 Size: 5295\n",
            "Running Loss: 2168.339050114155 Size: 5295\n",
            "train Loss: 0.4095, train Acc: train 77.81, train Recall: 76.36, train Precision: 92.06\n",
            "train Loss: 0.4095 Acc: 0.7781\n",
            "-----------------val-----------------\n",
            "Running Corrects: 551 Size: 780\n",
            "Running Loss: 499.3133566379547 Size: 780\n",
            "val Loss: 0.6401, val Acc: val 70.64, val Recall: 78.85, val Precision: 81.75\n",
            "val Loss: 0.6401 Acc: 0.7064\n",
            "\n",
            "Epoch 52/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4070 Size: 5295\n",
            "Running Loss: 2192.167506635189 Size: 5295\n",
            "train Loss: 0.4140, train Acc: train 76.86, train Recall: 75.58, train Precision: 91.63\n",
            "train Loss: 0.4140 Acc: 0.7686\n",
            "-----------------val-----------------\n",
            "Running Corrects: 600 Size: 780\n",
            "Running Loss: 485.06482195854187 Size: 780\n",
            "val Loss: 0.6219, val Acc: val 76.92, val Recall: 82.28, val Precision: 87.19\n",
            "val Loss: 0.6219 Acc: 0.7692\n",
            "\n",
            "Epoch 53/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4157 Size: 5295\n",
            "Running Loss: 2140.4601458609104 Size: 5295\n",
            "train Loss: 0.4042, train Acc: train 78.51, train Recall: 76.78, train Precision: 92.78\n",
            "train Loss: 0.4042 Acc: 0.7851\n",
            "-----------------val-----------------\n",
            "Running Corrects: 593 Size: 780\n",
            "Running Loss: 489.72082567214966 Size: 780\n",
            "val Loss: 0.6278, val Acc: val 76.03, val Recall: 81.86, val Precision: 86.32\n",
            "val Loss: 0.6278 Acc: 0.7603\n",
            "\n",
            "Epoch 54/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4148 Size: 5295\n",
            "Running Loss: 2180.334308683872 Size: 5295\n",
            "train Loss: 0.4118, train Acc: train 78.34, train Recall: 76.61, train Precision: 92.74\n",
            "train Loss: 0.4118 Acc: 0.7834\n",
            "-----------------val-----------------\n",
            "Running Corrects: 569 Size: 780\n",
            "Running Loss: 504.2502427101135 Size: 780\n",
            "val Loss: 0.6465, val Acc: val 72.95, val Recall: 84.06, val Precision: 77.72\n",
            "val Loss: 0.6465 Acc: 0.7295\n",
            "\n",
            "Epoch 55/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4125 Size: 5295\n",
            "Running Loss: 2213.9742905795574 Size: 5295\n",
            "train Loss: 0.4181, train Acc: train 77.90, train Recall: 76.43, train Precision: 92.12\n",
            "train Loss: 0.4181 Acc: 0.7790\n",
            "-----------------val-----------------\n",
            "Running Corrects: 576 Size: 780\n",
            "Running Loss: 495.87045454978943 Size: 780\n",
            "val Loss: 0.6357, val Acc: val 73.85, val Recall: 80.50, val Precision: 84.74\n",
            "val Loss: 0.6357 Acc: 0.7385\n",
            "\n",
            "Epoch 56/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4105 Size: 5295\n",
            "Running Loss: 2201.773678421974 Size: 5295\n",
            "train Loss: 0.4158, train Acc: train 77.53, train Recall: 75.99, train Precision: 92.25\n",
            "train Loss: 0.4158 Acc: 0.7753\n",
            "-----------------val-----------------\n",
            "Running Corrects: 546 Size: 780\n",
            "Running Loss: 496.2090549468994 Size: 780\n",
            "val Loss: 0.6362, val Acc: val 70.00, val Recall: 78.00, val Precision: 82.11\n",
            "val Loss: 0.6362 Acc: 0.7000\n",
            "\n",
            "Epoch 57/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4164 Size: 5295\n",
            "Running Loss: 2086.14929202199 Size: 5295\n",
            "train Loss: 0.3940, train Acc: train 78.64, train Recall: 77.18, train Precision: 92.19\n",
            "train Loss: 0.3940 Acc: 0.7864\n",
            "-----------------val-----------------\n",
            "Running Corrects: 574 Size: 780\n",
            "Running Loss: 483.16235995292664 Size: 780\n",
            "val Loss: 0.6194, val Acc: val 73.59, val Recall: 78.80, val Precision: 87.37\n",
            "val Loss: 0.6194 Acc: 0.7359\n",
            "\n",
            "Epoch 58/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4187 Size: 5295\n",
            "Running Loss: 2079.600710362196 Size: 5295\n",
            "train Loss: 0.3927, train Acc: train 79.07, train Recall: 77.37, train Precision: 92.78\n",
            "train Loss: 0.3927 Acc: 0.7907\n",
            "-----------------val-----------------\n",
            "Running Corrects: 571 Size: 780\n",
            "Running Loss: 488.1781086921692 Size: 780\n",
            "val Loss: 0.6259, val Acc: val 73.21, val Recall: 80.75, val Precision: 83.16\n",
            "val Loss: 0.6259 Acc: 0.7321\n",
            "\n",
            "Epoch 59/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4211 Size: 5295\n",
            "Running Loss: 2075.3075446486473 Size: 5295\n",
            "train Loss: 0.3919, train Acc: train 79.53, train Recall: 77.75, train Precision: 92.99\n",
            "train Loss: 0.3919 Acc: 0.7953\n",
            "-----------------val-----------------\n",
            "Running Corrects: 592 Size: 780\n",
            "Running Loss: 492.06726336479187 Size: 780\n",
            "val Loss: 0.6309, val Acc: val 75.90, val Recall: 82.05, val Precision: 85.79\n",
            "val Loss: 0.6309 Acc: 0.7590\n",
            "\n",
            "Epoch 60/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4198 Size: 5295\n",
            "Running Loss: 2064.0243586599827 Size: 5295\n",
            "train Loss: 0.3898, train Acc: train 79.28, train Recall: 77.88, train Precision: 92.16\n",
            "train Loss: 0.3898 Acc: 0.7928\n",
            "-----------------val-----------------\n",
            "Running Corrects: 598 Size: 780\n",
            "Running Loss: 493.3457224369049 Size: 780\n",
            "val Loss: 0.6325, val Acc: val 76.67, val Recall: 84.15, val Precision: 83.86\n",
            "val Loss: 0.6325 Acc: 0.7667\n",
            "\n",
            "Epoch 61/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4190 Size: 5295\n",
            "Running Loss: 2082.0698604285717 Size: 5295\n",
            "train Loss: 0.3932, train Acc: train 79.13, train Recall: 77.55, train Precision: 92.53\n",
            "train Loss: 0.3932 Acc: 0.7913\n",
            "-----------------val-----------------\n",
            "Running Corrects: 573 Size: 780\n",
            "Running Loss: 486.58644366264343 Size: 780\n",
            "val Loss: 0.6238, val Acc: val 73.46, val Recall: 79.32, val Precision: 86.14\n",
            "val Loss: 0.6238 Acc: 0.7346\n",
            "\n",
            "Epoch 62/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4207 Size: 5295\n",
            "Running Loss: 2074.307850688696 Size: 5295\n",
            "train Loss: 0.3917, train Acc: train 79.45, train Recall: 77.75, train Precision: 92.84\n",
            "train Loss: 0.3917 Acc: 0.7945\n",
            "-----------------val-----------------\n",
            "Running Corrects: 592 Size: 780\n",
            "Running Loss: 495.3354961872101 Size: 780\n",
            "val Loss: 0.6350, val Acc: val 75.90, val Recall: 85.37, val Precision: 80.88\n",
            "val Loss: 0.6350 Acc: 0.7590\n",
            "\n",
            "Epoch 63/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4238 Size: 5295\n",
            "Running Loss: 1997.7769240140915 Size: 5295\n",
            "train Loss: 0.3773, train Acc: train 80.04, train Recall: 78.65, train Precision: 92.28\n",
            "train Loss: 0.3773 Acc: 0.8004\n",
            "-----------------val-----------------\n",
            "Running Corrects: 594 Size: 780\n",
            "Running Loss: 477.7013018131256 Size: 780\n",
            "val Loss: 0.6124, val Acc: val 76.15, val Recall: 81.07, val Precision: 87.89\n",
            "val Loss: 0.6124 Acc: 0.7615\n",
            "\n",
            "Epoch 64/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4191 Size: 5295\n",
            "Running Loss: 2068.4574370980263 Size: 5295\n",
            "train Loss: 0.3906, train Acc: train 79.15, train Recall: 77.31, train Precision: 93.09\n",
            "train Loss: 0.3906 Acc: 0.7915\n",
            "-----------------val-----------------\n",
            "Running Corrects: 568 Size: 780\n",
            "Running Loss: 503.3570508956909 Size: 780\n",
            "val Loss: 0.6453, val Acc: val 72.82, val Recall: 83.52, val Precision: 78.25\n",
            "val Loss: 0.6453 Acc: 0.7282\n",
            "\n",
            "Epoch 65/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4223 Size: 5295\n",
            "Running Loss: 2018.5236943364143 Size: 5295\n",
            "train Loss: 0.3812, train Acc: train 79.75, train Recall: 78.17, train Precision: 92.62\n",
            "train Loss: 0.3812 Acc: 0.7975\n",
            "-----------------val-----------------\n",
            "Running Corrects: 588 Size: 780\n",
            "Running Loss: 493.9896500110626 Size: 780\n",
            "val Loss: 0.6333, val Acc: val 75.38, val Recall: 83.16, val Precision: 83.16\n",
            "val Loss: 0.6333 Acc: 0.7538\n",
            "\n",
            "Epoch 66/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4228 Size: 5295\n",
            "Running Loss: 2009.483200877905 Size: 5295\n",
            "train Loss: 0.3795, train Acc: train 79.85, train Recall: 78.47, train Precision: 92.22\n",
            "train Loss: 0.3795 Acc: 0.7985\n",
            "-----------------val-----------------\n",
            "Running Corrects: 598 Size: 780\n",
            "Running Loss: 483.88624382019043 Size: 780\n",
            "val Loss: 0.6204, val Acc: val 76.67, val Recall: 83.45, val Precision: 84.91\n",
            "val Loss: 0.6204 Acc: 0.7667\n",
            "\n",
            "Epoch 67/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4236 Size: 5295\n",
            "Running Loss: 2033.1577410399914 Size: 5295\n",
            "train Loss: 0.3840, train Acc: train 80.00, train Recall: 78.52, train Precision: 92.47\n",
            "train Loss: 0.3840 Acc: 0.8000\n",
            "-----------------val-----------------\n",
            "Running Corrects: 552 Size: 780\n",
            "Running Loss: 510.1509675979614 Size: 780\n",
            "val Loss: 0.6540, val Acc: val 70.77, val Recall: 83.93, val Precision: 74.21\n",
            "val Loss: 0.6540 Acc: 0.7077\n",
            "\n",
            "Epoch 68/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4262 Size: 5295\n",
            "Running Loss: 1966.4354164600372 Size: 5295\n",
            "train Loss: 0.3714, train Acc: train 80.49, train Recall: 78.54, train Precision: 93.52\n",
            "train Loss: 0.3714 Acc: 0.8049\n",
            "-----------------val-----------------\n",
            "Running Corrects: 535 Size: 780\n",
            "Running Loss: 512.9259333610535 Size: 780\n",
            "val Loss: 0.6576, val Acc: val 68.59, val Recall: 84.50, val Precision: 69.82\n",
            "val Loss: 0.6576 Acc: 0.6859\n",
            "\n",
            "Epoch 69/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4255 Size: 5295\n",
            "Running Loss: 1959.5059506595135 Size: 5295\n",
            "train Loss: 0.3701, train Acc: train 80.36, train Recall: 78.80, train Precision: 92.68\n",
            "train Loss: 0.3701 Acc: 0.8036\n",
            "-----------------val-----------------\n",
            "Running Corrects: 573 Size: 780\n",
            "Running Loss: 502.32687306404114 Size: 780\n",
            "val Loss: 0.6440, val Acc: val 73.46, val Recall: 84.97, val Precision: 77.37\n",
            "val Loss: 0.6440 Acc: 0.7346\n",
            "\n",
            "Epoch 70/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4301 Size: 5295\n",
            "Running Loss: 1916.0372522473335 Size: 5295\n",
            "train Loss: 0.3619, train Acc: train 81.23, train Recall: 79.85, train Precision: 92.53\n",
            "train Loss: 0.3619 Acc: 0.8123\n",
            "-----------------val-----------------\n",
            "Running Corrects: 579 Size: 780\n",
            "Running Loss: 502.1421446800232 Size: 780\n",
            "val Loss: 0.6438, val Acc: val 74.23, val Recall: 83.12, val Precision: 81.23\n",
            "val Loss: 0.6438 Acc: 0.7423\n",
            "\n",
            "Epoch 71/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4248 Size: 5295\n",
            "Running Loss: 1977.2676553428173 Size: 5295\n",
            "train Loss: 0.3734, train Acc: train 80.23, train Recall: 78.48, train Precision: 93.05\n",
            "train Loss: 0.3734 Acc: 0.8023\n",
            "-----------------val-----------------\n",
            "Running Corrects: 563 Size: 780\n",
            "Running Loss: 503.6184883117676 Size: 780\n",
            "val Loss: 0.6457, val Acc: val 72.18, val Recall: 81.80, val Precision: 79.65\n",
            "val Loss: 0.6457 Acc: 0.7218\n",
            "\n",
            "Epoch 72/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4292 Size: 5295\n",
            "Running Loss: 1955.2434994280338 Size: 5295\n",
            "train Loss: 0.3693, train Acc: train 81.06, train Recall: 79.45, train Precision: 92.93\n",
            "train Loss: 0.3693 Acc: 0.8106\n",
            "-----------------val-----------------\n",
            "Running Corrects: 555 Size: 780\n",
            "Running Loss: 513.015917301178 Size: 780\n",
            "val Loss: 0.6577, val Acc: val 71.15, val Recall: 86.01, val Precision: 72.28\n",
            "val Loss: 0.6577 Acc: 0.7115\n",
            "\n",
            "Epoch 73/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4313 Size: 5295\n",
            "Running Loss: 1932.6197644770145 Size: 5295\n",
            "train Loss: 0.3650, train Acc: train 81.45, train Recall: 79.58, train Precision: 93.55\n",
            "train Loss: 0.3650 Acc: 0.8145\n",
            "-----------------val-----------------\n",
            "Running Corrects: 538 Size: 780\n",
            "Running Loss: 521.5831339359283 Size: 780\n",
            "val Loss: 0.6687, val Acc: val 68.97, val Recall: 85.04, val Precision: 69.82\n",
            "val Loss: 0.6687 Acc: 0.6897\n",
            "\n",
            "Epoch 74/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4317 Size: 5295\n",
            "Running Loss: 1913.036132544279 Size: 5295\n",
            "train Loss: 0.3613, train Acc: train 81.53, train Recall: 79.68, train Precision: 93.52\n",
            "train Loss: 0.3613 Acc: 0.8153\n",
            "-----------------val-----------------\n",
            "Running Corrects: 530 Size: 780\n",
            "Running Loss: 523.4103453159332 Size: 780\n",
            "val Loss: 0.6710, val Acc: val 67.95, val Recall: 85.24, val Precision: 67.89\n",
            "val Loss: 0.6710 Acc: 0.6795\n",
            "\n",
            "Epoch 75/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4288 Size: 5295\n",
            "Running Loss: 1985.0774898827076 Size: 5295\n",
            "train Loss: 0.3749, train Acc: train 80.98, train Recall: 79.15, train Precision: 93.36\n",
            "train Loss: 0.3749 Acc: 0.8098\n",
            "-----------------val-----------------\n",
            "Running Corrects: 513 Size: 780\n",
            "Running Loss: 528.5838966369629 Size: 780\n",
            "val Loss: 0.6777, val Acc: val 65.77, val Recall: 85.15, val Precision: 64.39\n",
            "val Loss: 0.6777 Acc: 0.6577\n",
            "\n",
            "Epoch 76/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4308 Size: 5295\n",
            "Running Loss: 1894.696308940649 Size: 5295\n",
            "train Loss: 0.3578, train Acc: train 81.36, train Recall: 79.87, train Precision: 92.78\n",
            "train Loss: 0.3578 Acc: 0.8136\n",
            "-----------------val-----------------\n",
            "Running Corrects: 584 Size: 780\n",
            "Running Loss: 488.6653516292572 Size: 780\n",
            "val Loss: 0.6265, val Acc: val 74.87, val Recall: 83.04, val Precision: 82.46\n",
            "val Loss: 0.6265 Acc: 0.7487\n",
            "\n",
            "Epoch 77/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4330 Size: 5295\n",
            "Running Loss: 1838.2918650507927 Size: 5295\n",
            "train Loss: 0.3472, train Acc: train 81.78, train Recall: 80.26, train Precision: 92.93\n",
            "train Loss: 0.3472 Acc: 0.8178\n",
            "-----------------val-----------------\n",
            "Running Corrects: 590 Size: 780\n",
            "Running Loss: 496.4859893321991 Size: 780\n",
            "val Loss: 0.6365, val Acc: val 75.64, val Recall: 84.67, val Precision: 81.40\n",
            "val Loss: 0.6365 Acc: 0.7564\n",
            "\n",
            "Epoch 78/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4283 Size: 5295\n",
            "Running Loss: 1931.8400871306658 Size: 5295\n",
            "train Loss: 0.3648, train Acc: train 80.89, train Recall: 79.42, train Precision: 92.62\n",
            "train Loss: 0.3648 Acc: 0.8089\n",
            "-----------------val-----------------\n",
            "Running Corrects: 585 Size: 780\n",
            "Running Loss: 498.97250485420227 Size: 780\n",
            "val Loss: 0.6397, val Acc: val 75.00, val Recall: 85.71, val Precision: 78.95\n",
            "val Loss: 0.6397 Acc: 0.7500\n",
            "\n",
            "Epoch 79/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4279 Size: 5295\n",
            "Running Loss: 1938.5217370688915 Size: 5295\n",
            "train Loss: 0.3661, train Acc: train 80.81, train Recall: 79.21, train Precision: 92.87\n",
            "train Loss: 0.3661 Acc: 0.8081\n",
            "-----------------val-----------------\n",
            "Running Corrects: 583 Size: 780\n",
            "Running Loss: 496.54346799850464 Size: 780\n",
            "val Loss: 0.6366, val Acc: val 74.74, val Recall: 84.35, val Precision: 80.35\n",
            "val Loss: 0.6366 Acc: 0.7474\n",
            "\n",
            "Epoch 80/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4277 Size: 5295\n",
            "Running Loss: 1928.1714301407337 Size: 5295\n",
            "train Loss: 0.3641, train Acc: train 80.77, train Recall: 79.12, train Precision: 92.96\n",
            "train Loss: 0.3641 Acc: 0.8077\n",
            "-----------------val-----------------\n",
            "Running Corrects: 578 Size: 780\n",
            "Running Loss: 498.60212302207947 Size: 780\n",
            "val Loss: 0.6392, val Acc: val 74.10, val Recall: 85.94, val Precision: 77.19\n",
            "val Loss: 0.6392 Acc: 0.7410\n",
            "\n",
            "Epoch 81/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4333 Size: 5295\n",
            "Running Loss: 1844.790223479271 Size: 5295\n",
            "train Loss: 0.3484, train Acc: train 81.83, train Recall: 79.93, train Precision: 93.71\n",
            "train Loss: 0.3484 Acc: 0.8183\n",
            "-----------------val-----------------\n",
            "Running Corrects: 557 Size: 780\n",
            "Running Loss: 511.4722249507904 Size: 780\n",
            "val Loss: 0.6557, val Acc: val 71.41, val Recall: 85.63, val Precision: 73.16\n",
            "val Loss: 0.6557 Acc: 0.7141\n",
            "\n",
            "Epoch 82/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4335 Size: 5295\n",
            "Running Loss: 1875.053560256958 Size: 5295\n",
            "train Loss: 0.3541, train Acc: train 81.87, train Recall: 80.29, train Precision: 93.09\n",
            "train Loss: 0.3541 Acc: 0.8187\n",
            "-----------------val-----------------\n",
            "Running Corrects: 594 Size: 780\n",
            "Running Loss: 488.04323530197144 Size: 780\n",
            "val Loss: 0.6257, val Acc: val 76.15, val Recall: 83.45, val Precision: 84.04\n",
            "val Loss: 0.6257 Acc: 0.7615\n",
            "\n",
            "Epoch 83/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4323 Size: 5295\n",
            "Running Loss: 1872.6241683661938 Size: 5295\n",
            "train Loss: 0.3537, train Acc: train 81.64, train Recall: 79.97, train Precision: 93.21\n",
            "train Loss: 0.3537 Acc: 0.8164\n",
            "-----------------val-----------------\n",
            "Running Corrects: 528 Size: 780\n",
            "Running Loss: 520.8658773899078 Size: 780\n",
            "val Loss: 0.6678, val Acc: val 67.69, val Recall: 85.02, val Precision: 67.72\n",
            "val Loss: 0.6678 Acc: 0.6769\n",
            "\n",
            "Epoch 84/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4320 Size: 5295\n",
            "Running Loss: 1861.0506033301353 Size: 5295\n",
            "train Loss: 0.3515, train Acc: train 81.59, train Recall: 80.00, train Precision: 93.02\n",
            "train Loss: 0.3515 Acc: 0.8159\n",
            "-----------------val-----------------\n",
            "Running Corrects: 580 Size: 780\n",
            "Running Loss: 500.10903453826904 Size: 780\n",
            "val Loss: 0.6412, val Acc: val 74.36, val Recall: 85.17, val Precision: 78.60\n",
            "val Loss: 0.6412 Acc: 0.7436\n",
            "\n",
            "Epoch 85/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4337 Size: 5295\n",
            "Running Loss: 1841.3595122992992 Size: 5295\n",
            "train Loss: 0.3478, train Acc: train 81.91, train Recall: 80.27, train Precision: 93.21\n",
            "train Loss: 0.3478 Acc: 0.8191\n",
            "-----------------val-----------------\n",
            "Running Corrects: 554 Size: 780\n",
            "Running Loss: 511.39307403564453 Size: 780\n",
            "val Loss: 0.6556, val Acc: val 71.03, val Recall: 85.68, val Precision: 72.46\n",
            "val Loss: 0.6556 Acc: 0.7103\n",
            "\n",
            "Epoch 86/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4334 Size: 5295\n",
            "Running Loss: 1835.1344247460365 Size: 5295\n",
            "train Loss: 0.3466, train Acc: train 81.85, train Recall: 80.41, train Precision: 92.81\n",
            "train Loss: 0.3466 Acc: 0.8185\n",
            "-----------------val-----------------\n",
            "Running Corrects: 543 Size: 780\n",
            "Running Loss: 509.90911769866943 Size: 780\n",
            "val Loss: 0.6537, val Acc: val 69.62, val Recall: 83.91, val Precision: 72.28\n",
            "val Loss: 0.6537 Acc: 0.6962\n",
            "\n",
            "Epoch 87/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4376 Size: 5295\n",
            "Running Loss: 1790.1039261817932 Size: 5295\n",
            "train Loss: 0.3381, train Acc: train 82.64, train Recall: 81.09, train Precision: 93.24\n",
            "train Loss: 0.3381 Acc: 0.8264\n",
            "-----------------val-----------------\n",
            "Running Corrects: 543 Size: 780\n",
            "Running Loss: 516.4240293502808 Size: 780\n",
            "val Loss: 0.6621, val Acc: val 69.62, val Recall: 85.05, val Precision: 70.88\n",
            "val Loss: 0.6621 Acc: 0.6962\n",
            "\n",
            "Epoch 88/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4383 Size: 5295\n",
            "Running Loss: 1858.641450613737 Size: 5295\n",
            "train Loss: 0.3510, train Acc: train 82.78, train Recall: 80.98, train Precision: 93.74\n",
            "train Loss: 0.3510 Acc: 0.8278\n",
            "-----------------val-----------------\n",
            "Running Corrects: 562 Size: 780\n",
            "Running Loss: 513.1863749027252 Size: 780\n",
            "val Loss: 0.6579, val Acc: val 72.05, val Recall: 83.46, val Precision: 77.02\n",
            "val Loss: 0.6579 Acc: 0.7205\n",
            "\n",
            "Epoch 89/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4347 Size: 5295\n",
            "Running Loss: 1851.2468302249908 Size: 5295\n",
            "train Loss: 0.3496, train Acc: train 82.10, train Recall: 80.48, train Precision: 93.21\n",
            "train Loss: 0.3496 Acc: 0.8210\n",
            "-----------------val-----------------\n",
            "Running Corrects: 583 Size: 780\n",
            "Running Loss: 495.7003824710846 Size: 780\n",
            "val Loss: 0.6355, val Acc: val 74.74, val Recall: 83.48, val Precision: 81.58\n",
            "val Loss: 0.6355 Acc: 0.7474\n",
            "\n",
            "Epoch 90/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4365 Size: 5295\n",
            "Running Loss: 1781.9604448974133 Size: 5295\n",
            "train Loss: 0.3365, train Acc: train 82.44, train Recall: 80.84, train Precision: 93.27\n",
            "train Loss: 0.3365 Acc: 0.8244\n",
            "-----------------val-----------------\n",
            "Running Corrects: 536 Size: 780\n",
            "Running Loss: 519.6174740791321 Size: 780\n",
            "val Loss: 0.6662, val Acc: val 68.72, val Recall: 84.53, val Precision: 70.00\n",
            "val Loss: 0.6662 Acc: 0.6872\n",
            "\n",
            "Epoch 91/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4370 Size: 5295\n",
            "Running Loss: 1828.6431957185268 Size: 5295\n",
            "train Loss: 0.3454, train Acc: train 82.53, train Recall: 80.91, train Precision: 93.33\n",
            "train Loss: 0.3454 Acc: 0.8253\n",
            "-----------------val-----------------\n",
            "Running Corrects: 567 Size: 780\n",
            "Running Loss: 493.50674057006836 Size: 780\n",
            "val Loss: 0.6327, val Acc: val 72.69, val Recall: 79.60, val Precision: 84.21\n",
            "val Loss: 0.6327 Acc: 0.7269\n",
            "\n",
            "Epoch 92/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4369 Size: 5295\n",
            "Running Loss: 1793.7326243519783 Size: 5295\n",
            "train Loss: 0.3388, train Acc: train 82.51, train Recall: 80.99, train Precision: 93.15\n",
            "train Loss: 0.3388 Acc: 0.8251\n",
            "-----------------val-----------------\n",
            "Running Corrects: 564 Size: 780\n",
            "Running Loss: 507.2877149581909 Size: 780\n",
            "val Loss: 0.6504, val Acc: val 72.31, val Recall: 83.78, val Precision: 77.02\n",
            "val Loss: 0.6504 Acc: 0.7231\n",
            "\n",
            "Epoch 93/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4431 Size: 5295\n",
            "Running Loss: 1741.09469383955 Size: 5295\n",
            "train Loss: 0.3288, train Acc: train 83.68, train Recall: 82.00, train Precision: 93.80\n",
            "train Loss: 0.3288 Acc: 0.8368\n",
            "-----------------val-----------------\n",
            "Running Corrects: 565 Size: 780\n",
            "Running Loss: 501.27302408218384 Size: 780\n",
            "val Loss: 0.6427, val Acc: val 72.44, val Recall: 83.68, val Precision: 77.37\n",
            "val Loss: 0.6427 Acc: 0.7244\n",
            "\n",
            "Epoch 94/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4386 Size: 5295\n",
            "Running Loss: 1774.8028038144112 Size: 5295\n",
            "train Loss: 0.3352, train Acc: train 82.83, train Recall: 81.30, train Precision: 93.27\n",
            "train Loss: 0.3352 Acc: 0.8283\n",
            "-----------------val-----------------\n",
            "Running Corrects: 560 Size: 780\n",
            "Running Loss: 506.40533661842346 Size: 780\n",
            "val Loss: 0.6492, val Acc: val 71.79, val Recall: 86.01, val Precision: 73.33\n",
            "val Loss: 0.6492 Acc: 0.7179\n",
            "\n",
            "Epoch 95/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4396 Size: 5295\n",
            "Running Loss: 1743.1599272191525 Size: 5295\n",
            "train Loss: 0.3292, train Acc: train 83.02, train Recall: 81.42, train Precision: 93.46\n",
            "train Loss: 0.3292 Acc: 0.8302\n",
            "-----------------val-----------------\n",
            "Running Corrects: 576 Size: 780\n",
            "Running Loss: 491.43113565444946 Size: 780\n",
            "val Loss: 0.6300, val Acc: val 73.85, val Recall: 84.01, val Precision: 79.30\n",
            "val Loss: 0.6300 Acc: 0.7385\n",
            "\n",
            "Epoch 96/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4391 Size: 5295\n",
            "Running Loss: 1780.0472467243671 Size: 5295\n",
            "train Loss: 0.3362, train Acc: train 82.93, train Recall: 81.37, train Precision: 93.33\n",
            "train Loss: 0.3362 Acc: 0.8293\n",
            "-----------------val-----------------\n",
            "Running Corrects: 568 Size: 780\n",
            "Running Loss: 490.3301885128021 Size: 780\n",
            "val Loss: 0.6286, val Acc: val 72.82, val Recall: 83.15, val Precision: 78.77\n",
            "val Loss: 0.6286 Acc: 0.7282\n",
            "\n",
            "Epoch 97/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4375 Size: 5295\n",
            "Running Loss: 1767.8233449459076 Size: 5295\n",
            "train Loss: 0.3339, train Acc: train 82.63, train Recall: 80.96, train Precision: 93.46\n",
            "train Loss: 0.3339 Acc: 0.8263\n",
            "-----------------val-----------------\n",
            "Running Corrects: 558 Size: 780\n",
            "Running Loss: 502.67577838897705 Size: 780\n",
            "val Loss: 0.6445, val Acc: val 71.54, val Recall: 83.72, val Precision: 75.79\n",
            "val Loss: 0.6445 Acc: 0.7154\n",
            "\n",
            "Epoch 98/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4413 Size: 5295\n",
            "Running Loss: 1712.5981484949589 Size: 5295\n",
            "train Loss: 0.3234, train Acc: train 83.34, train Recall: 82.23, train Precision: 92.68\n",
            "train Loss: 0.3234 Acc: 0.8334\n",
            "-----------------val-----------------\n",
            "Running Corrects: 593 Size: 780\n",
            "Running Loss: 479.5290558338165 Size: 780\n",
            "val Loss: 0.6148, val Acc: val 76.03, val Recall: 82.40, val Precision: 85.44\n",
            "val Loss: 0.6148 Acc: 0.7603\n",
            "\n",
            "Epoch 99/99\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4385 Size: 5295\n",
            "Running Loss: 1775.8903699219227 Size: 5295\n",
            "train Loss: 0.3354, train Acc: train 82.81, train Recall: 80.94, train Precision: 93.89\n",
            "train Loss: 0.3354 Acc: 0.8281\n",
            "-----------------val-----------------\n",
            "Running Corrects: 563 Size: 780\n",
            "Running Loss: 501.66001081466675 Size: 780\n",
            "val Loss: 0.6432, val Acc: val 72.18, val Recall: 83.11, val Precision: 77.72\n",
            "val Loss: 0.6432 Acc: 0.7218\n",
            "\n",
            "Mean train accuracy: tensor(75.8517, device='cuda:0')\n",
            "Mean train recall: 92.27720930232557\n",
            "Mean train precision: 74.67526160369738\n",
            "Mean val accuracy: tensor(73.5359, device='cuda:0')\n",
            "Mean val recall: 84.95789473684209\n",
            "Mean val precision: 80.61654689947456\n",
            "MAX train accuracy: tensor(83.6827, device='cuda:0')\n",
            "MAX train recall: 93.95348837209302\n",
            "MAX train precision: 82.2283356258597\n",
            "MAX val accuracy: tensor(77.9487, device='cuda:0')\n",
            "MAX val recall: 100.0\n",
            "MAX val precision: 86.36363636363636\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"6a1443d9-7564-42af-9b54-d047d1b3ba60\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"6a1443d9-7564-42af-9b54-d047d1b3ba60\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '6a1443d9-7564-42af-9b54-d047d1b3ba60',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=t<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=t\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=t\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100], \"xaxis\": \"x\", \"y\": [57.79037094116211, 60.77431869506836, 61.000946044921875, 61.624176025390625, 63.966007232666016, 64.09820556640625, 63.852691650390625, 66.36449432373047, 66.6100082397461, 66.96884155273438, 67.28990173339844, 67.40321350097656, 67.93201446533203, 68.44192504882812, 67.87535858154297, 69.80170440673828, 69.63172912597656, 69.85836029052734, 70.10387420654297, 69.97167205810547, 70.00944519042969, 70.46269989013672, 71.48253631591797, 71.935791015625, 70.53824615478516, 71.82247924804688, 72.5779037475586, 73.33333587646484, 72.95561981201172, 72.9745101928711, 72.67233276367188, 73.63550567626953, 74.2965087890625, 73.97544860839844, 74.27762603759766, 74.57979583740234, 73.88101959228516, 74.82530975341797, 76.09065246582031, 75.12747955322266, 75.56185150146484, 74.7686538696289, 74.78753662109375, 75.97734069824219, 76.73277282714844, 76.6572265625, 76.82719421386719, 76.80831146240234, 77.88479614257812, 77.99811553955078, 77.6392822265625, 77.80925750732422, 76.8649673461914, 78.50802612304688, 78.33805847167969, 77.9036865234375, 77.52597045898438, 78.64022827148438, 79.07460021972656, 79.52786254882812, 79.28234100341797, 79.13125610351562, 79.45231628417969, 80.03777313232422, 79.150146484375, 79.75448608398438, 79.84891510009766, 80.0, 80.49103546142578, 80.35883331298828, 81.22757720947266, 80.22663116455078, 81.05760192871094, 81.45420837402344, 81.52974700927734, 80.98206329345703, 81.35977935791016, 81.77526092529297, 80.88763427734375, 80.81208801269531, 80.7743148803711, 81.83191680908203, 81.86968994140625, 81.64305877685547, 81.5864028930664, 81.90746307373047, 81.8508071899414, 82.64400482177734, 82.77620697021484, 82.09632110595703, 82.43626403808594, 82.53069305419922, 82.51180267333984, 83.68272399902344, 82.8328628540039, 83.02172088623047, 82.92729187011719, 82.6251220703125, 83.34278106689453, 82.81398010253906], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=v<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=v\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100], \"xaxis\": \"x\", \"y\": [72.82051849365234, 73.0769271850586, 72.94872283935547, 72.94872283935547, 73.0769271850586, 71.4102554321289, 72.43589782714844, 71.53846740722656, 73.0769271850586, 72.82051849365234, 75.12820434570312, 73.0769271850586, 74.4871826171875, 76.02564239501953, 75.51282501220703, 76.66667175292969, 72.30769348144531, 72.94872283935547, 75.3846206665039, 74.10256958007812, 76.79487609863281, 76.92308044433594, 73.46154022216797, 73.0769271850586, 74.61538696289062, 75.12820434570312, 72.17948913574219, 74.10256958007812, 75.25641632080078, 75.25641632080078, 74.61538696289062, 77.17948913574219, 76.79487609863281, 76.66667175292969, 73.84615325927734, 74.10256958007812, 77.94872283935547, 72.82051849365234, 76.02564239501953, 74.4871826171875, 76.66667175292969, 73.71794891357422, 74.35897827148438, 73.97435760498047, 73.33333587646484, 73.71794891357422, 73.71794891357422, 75.12820434570312, 72.05128479003906, 73.84615325927734, 71.4102554321289, 70.64102935791016, 76.92308044433594, 76.02564239501953, 72.94872283935547, 73.84615325927734, 70.0, 73.5897445678711, 73.20513153076172, 75.8974380493164, 76.66667175292969, 73.46154022216797, 75.8974380493164, 76.15384674072266, 72.82051849365234, 75.3846206665039, 76.66667175292969, 70.76923370361328, 68.5897445678711, 73.46154022216797, 74.23077392578125, 72.17948913574219, 71.15384674072266, 68.97435760498047, 67.94872283935547, 65.76923370361328, 74.87179565429688, 75.64102935791016, 75.0, 74.74359130859375, 74.10256958007812, 71.4102554321289, 76.15384674072266, 67.69230651855469, 74.35897827148438, 71.02564239501953, 69.61538696289062, 69.61538696289062, 72.05128479003906, 74.74359130859375, 68.71794891357422, 72.69230651855469, 72.30769348144531, 72.43589782714844, 71.79487609863281, 73.84615325927734, 72.82051849365234, 71.53846740722656, 76.02564239501953, 72.17948913574219], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6a1443d9-7564-42af-9b54-d047d1b3ba60');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"96d10390-39d6-4eca-83f9-bebb0f1f0dff\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"96d10390-39d6-4eca-83f9-bebb0f1f0dff\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '96d10390-39d6-4eca-83f9-bebb0f1f0dff',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=t<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=t\", \"line\": {\"color\": \"#636efa\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=t\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100], \"xaxis\": \"x\", \"y\": [0.837231366312425, 0.6270909263346981, 0.6189871460452634, 0.5991438126203809, 0.5861124155654223, 0.572350051303986, 0.5702793170980286, 0.5624905645565901, 0.5535440123114977, 0.5541460656129155, 0.5376376318751471, 0.5388726053989867, 0.5307856778697769, 0.529022602420353, 0.5271805238903864, 0.5173650928327111, 0.5170855159012071, 0.5228954605047605, 0.5134140907986419, 0.5101180373665519, 0.5042096566940052, 0.5019660684272182, 0.4876931329962214, 0.4885223008136461, 0.49936051803225046, 0.4859802814786225, 0.4818957044337582, 0.47050989448630215, 0.47136595361743816, 0.4700528636194829, 0.4726196092800558, 0.46622549578533407, 0.4627234057956872, 0.46078927007005166, 0.45755553673302035, 0.4516812980400154, 0.45261953366714563, 0.44718693551080646, 0.44116331499854383, 0.44391988877759325, 0.4456355157609927, 0.4480782828598905, 0.44484234235554626, 0.4328710071763641, 0.4255083123054901, 0.43201203285465833, 0.42524016640229084, 0.41935389451084554, 0.4147222305267233, 0.4154248752326083, 0.4115247051146258, 0.40950690275999146, 0.41400708340607917, 0.40424176503511056, 0.4117722962575774, 0.41812545620010527, 0.4158212801552359, 0.3939847577000925, 0.39274800951127403, 0.39193721334252074, 0.38980630003021394, 0.3932143268042628, 0.39174841372779906, 0.37729498092806263, 0.3906435197541126, 0.3812131622920518, 0.3795057980883673, 0.38397691048913907, 0.37137590490274547, 0.370067223920588, 0.36185783800705074, 0.3734216535113914, 0.3692622284094493, 0.3649895683620424, 0.3612910543048686, 0.37489659865584657, 0.3578274426705664, 0.34717504533537163, 0.3648423205157065, 0.36610419963529583, 0.36414946744867493, 0.34840230849466874, 0.3541177639767626, 0.3536589553099516, 0.3514732017620652, 0.347754393257658, 0.3465787393288077, 0.3380743958794699, 0.3510182154133592, 0.3496216865391862, 0.3365364390741101, 0.3453528226097312, 0.3387597024271914, 0.32881863906318226, 0.33518466549847237, 0.3292086736957795, 0.33617511741725536, 0.33386654295484564, 0.32343685523984117, 0.33539006041962655], \"yaxis\": \"y\"}, {\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"label=v<br>epoch=%{x}<br>accuracy=%{y}\", \"legendgroup\": \"label=v\", \"line\": {\"color\": \"#EF553B\", \"dash\": \"solid\"}, \"mode\": \"lines\", \"name\": \"label=v\", \"showlegend\": true, \"type\": \"scatter\", \"x\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100], \"xaxis\": \"x\", \"y\": [0.6356803408035865, 0.6216463254048274, 0.6357788571944604, 0.6493670402429043, 0.6148831126017448, 0.6551056464513143, 0.6422296160306686, 0.6436289576383737, 0.6445837781979488, 0.6506163951678153, 0.6441462006324377, 0.6295523921648661, 0.6430330038070678, 0.6494451825435344, 0.6418630153704912, 0.6602933168411255, 0.6175717540276356, 0.6468272484265841, 0.6580396221234248, 0.6317315679330092, 0.6411148844621121, 0.6615291017752427, 0.651418652289953, 0.6570290614397097, 0.6273383589891287, 0.6353529936228043, 0.652467376146561, 0.6294298254526578, 0.6430215077522473, 0.6478485358067048, 0.637727281374809, 0.6360156261003934, 0.6493222352785942, 0.658880157654102, 0.6325145207918608, 0.6630033092621045, 0.643779927950639, 0.6535638231497545, 0.6497060100237528, 0.6508836461947515, 0.649652807529156, 0.6364541399173248, 0.6536688061860891, 0.6261756637157538, 0.6524638035358526, 0.6314887927128718, 0.6297353869829423, 0.6276264319053063, 0.6590849460699619, 0.6437314929106297, 0.6531734986182971, 0.6401453290230189, 0.6218779768699255, 0.6278472124001919, 0.6464746701411712, 0.6357313519869096, 0.6361654550601274, 0.6194389230165727, 0.6258693701181657, 0.6308554658522972, 0.6324945159447499, 0.623828773926466, 0.6350455079323206, 0.6124375664270841, 0.6453295524303729, 0.6333200641167469, 0.6203669792566544, 0.6540397020486685, 0.657597350462889, 0.6440088116205656, 0.6437719803590041, 0.6456647286048303, 0.6577127144886897, 0.6686963255588825, 0.6710389042511965, 0.6776716623550806, 0.6264940405503298, 0.636520499143845, 0.6397083395566696, 0.6365941897416726, 0.639233491053948, 0.6557336217317826, 0.625696455515348, 0.6677767658844972, 0.6411654288952168, 0.6556321461995442, 0.6537296380752172, 0.6620820889106164, 0.6579312498752887, 0.6355133108603649, 0.6661762488194001, 0.6327009494488056, 0.650368865331014, 0.642657723182287, 0.6492376110492608, 0.6300399175057044, 0.6286284468112848, 0.6444561261397141, 0.6147808408125853, 0.6431538600188035], \"yaxis\": \"y\"}],\n",
              "                        {\"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"epoch\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"accuracy\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('96d10390-39d6-4eca-83f9-bebb0f1f0dff');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Training complete in 78m 59s\n",
            "Best val Acc: 0.779487\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "_KOZFjn2fQYs",
        "outputId": "b30fe1a3-4308-48b9-8ec0-8be643b3718d"
      },
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "actuals, class_probabilities = x(model_ft, device, dataloaders['train'])\n",
        "\n",
        "print(actuals)\n",
        "print(class_probabilities)\n",
        "fpr, tpr, _ = roc_curve(actuals, class_probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC for class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0)]\n",
            "[tensor(0.0376), tensor(0.0957), tensor(0.0245), tensor(-0.0064), tensor(0.4973), tensor(0.0470), tensor(0.0232), tensor(0.0195), tensor(0.0281), tensor(0.0816), tensor(0.0493), tensor(0.0387), tensor(0.0246), tensor(0.0521), tensor(0.0503), tensor(0.0451), tensor(0.0361), tensor(0.1014), tensor(0.0780), tensor(0.0309), tensor(0.0461), tensor(0.0693), tensor(0.0297), tensor(0.0411), tensor(0.0856), tensor(0.0897), tensor(0.0603), tensor(0.0950), tensor(0.0355), tensor(-0.0017), tensor(0.0363), tensor(0.0587), tensor(0.0035), tensor(0.0353), tensor(0.0259), tensor(0.0524), tensor(0.0107), tensor(0.0030), tensor(0.0233), tensor(0.0004), tensor(0.4821), tensor(0.0668), tensor(0.0143), tensor(0.0296), tensor(-0.0067), tensor(0.0330), tensor(-0.0075), tensor(0.0461), tensor(0.0068), tensor(0.0543), tensor(-0.0061), tensor(0.0135), tensor(0.0425), tensor(0.0379), tensor(0.0044), tensor(0.0438), tensor(0.0351), tensor(-0.0125), tensor(-0.0152), tensor(-0.0211), tensor(0.1706), tensor(0.0289), tensor(0.0092), tensor(0.0385), tensor(0.0148), tensor(0.0638), tensor(0.0491), tensor(-0.0251), tensor(0.0201), tensor(-0.0166), tensor(-0.0278), tensor(0.0701), tensor(0.0311), tensor(-0.0083), tensor(0.0518), tensor(0.0275), tensor(0.0580), tensor(0.0483), tensor(0.0327), tensor(0.0433), tensor(0.0833), tensor(0.1780), tensor(-0.0290), tensor(0.0299), tensor(0.0387), tensor(0.0231), tensor(0.0077), tensor(0.0838), tensor(0.1372), tensor(-0.0174), tensor(0.0440), tensor(0.0174), tensor(0.0372), tensor(0.0207), tensor(0.0321), tensor(0.2493), tensor(0.0630), tensor(0.0049), tensor(0.0821), tensor(0.0099), tensor(0.0260), tensor(-0.0315), tensor(0.0377), tensor(0.0004), tensor(0.1049), tensor(0.4147), tensor(0.0499), tensor(0.0089), tensor(0.2546), tensor(0.0223), tensor(0.1478), tensor(-0.0544), tensor(0.0225), tensor(0.0087), tensor(0.0193), tensor(0.0492), tensor(0.0241), tensor(0.0238), tensor(0.0415), tensor(0.0196), tensor(0.0391), tensor(0.0096), tensor(0.5946), tensor(0.0127), tensor(0.0891), tensor(0.0070), tensor(0.0128), tensor(0.0249), tensor(0.0335), tensor(0.0090), tensor(0.0177), tensor(0.0173), tensor(0.2929), tensor(0.0238), tensor(0.6189), tensor(0.3482), tensor(0.0675), tensor(0.0420), tensor(0.0407), tensor(0.0540), tensor(0.0465), tensor(0.0665), tensor(0.0337), tensor(0.1156), tensor(0.0057), tensor(0.0400), tensor(0.0634), tensor(-0.0074), tensor(0.0497), tensor(0.0540), tensor(0.0306), tensor(0.0195), tensor(0.0828), tensor(0.0825), tensor(0.0048), tensor(0.0475), tensor(0.0873), tensor(0.0534), tensor(0.0262), tensor(0.0138), tensor(0.0010), tensor(0.1689), tensor(-0.0141), tensor(0.0249), tensor(0.0426), tensor(0.0969), tensor(0.0189), tensor(0.0478), tensor(0.0013), tensor(0.0305), tensor(7.3195e-05), tensor(-0.0003), tensor(-0.0038), tensor(0.0548), tensor(0.0383), tensor(0.0094), tensor(0.0407), tensor(0.0026), tensor(0.0222), tensor(0.1006), tensor(0.1264), tensor(0.1181), tensor(-0.0146), tensor(-0.0082), tensor(0.0316), tensor(-0.0248), tensor(0.0195), tensor(0.0482), tensor(0.1734), tensor(0.2094), tensor(0.0444), tensor(0.0391), tensor(0.0400), tensor(0.0769), tensor(0.0364), tensor(0.1084), tensor(0.0555), tensor(0.0346), tensor(-0.0136), tensor(0.1080), tensor(0.0521), tensor(0.0574), tensor(-0.0012), tensor(0.0424), tensor(0.0450), tensor(0.0307), tensor(0.0459), tensor(0.0406), tensor(0.0066), tensor(0.0613), tensor(0.0496), tensor(0.0409), tensor(0.0533), tensor(0.0709), tensor(0.1013), tensor(0.2056), tensor(0.0435), tensor(0.0615), tensor(0.0645), tensor(0.0312), tensor(0.0207), tensor(0.0353), tensor(0.0180), tensor(0.0122), tensor(0.0324), tensor(0.0080), tensor(0.0549), tensor(0.1222), tensor(-0.0285), tensor(0.1045), tensor(-0.0095), tensor(0.0052), tensor(0.0681), tensor(0.0229), tensor(0.0432), tensor(0.0232), tensor(0.0139), tensor(0.0162), tensor(0.0003), tensor(0.0502), tensor(-0.0121), tensor(0.0602), tensor(0.0151), tensor(0.0443), tensor(0.3621), tensor(0.0109), tensor(0.2127), tensor(0.0077), tensor(-0.0242), tensor(0.0515), tensor(0.0754), tensor(0.0386), tensor(0.0136), tensor(0.0061), tensor(-0.0051), tensor(0.0454), tensor(0.0736), tensor(0.0626), tensor(0.0122), tensor(0.0353), tensor(0.0654), tensor(0.0445), tensor(0.0378), tensor(0.0477), tensor(0.0519), tensor(0.0555), tensor(0.0540), tensor(0.0885), tensor(0.1476), tensor(0.0728), tensor(0.0482), tensor(-0.0283), tensor(0.4537), tensor(0.0214), tensor(0.0211), tensor(0.0103), tensor(-0.0153), tensor(0.0233), tensor(0.0331), tensor(0.0194), tensor(0.0234), tensor(0.0583), tensor(-0.0169), tensor(0.0375), tensor(0.0425), tensor(0.0464), tensor(0.0231), tensor(-0.0204), tensor(0.0016), tensor(-0.0023), tensor(0.0442), tensor(0.0870), tensor(0.0161), tensor(0.0743), tensor(-0.0491), tensor(0.0057), tensor(0.0374), tensor(0.1248), tensor(0.0319), tensor(0.0326), tensor(0.0380), tensor(-0.0005), tensor(0.0878), tensor(-0.0080), tensor(0.0019), tensor(0.0780), tensor(0.0751), tensor(0.0231), tensor(0.0688), tensor(0.0681), tensor(0.0591), tensor(0.0414), tensor(0.0601), tensor(0.0131), tensor(0.0460), tensor(0.0460), tensor(0.1096), tensor(0.3789), tensor(-0.0351), tensor(0.0560), tensor(0.0143), tensor(0.0375), tensor(0.0138), tensor(0.0387), tensor(0.0137), tensor(0.0250), tensor(0.0330), tensor(0.0185), tensor(0.0130), tensor(0.0372), tensor(-0.0405), tensor(0.0233), tensor(0.0240), tensor(0.0464), tensor(0.0411), tensor(0.0274), tensor(0.2293), tensor(0.0127), tensor(0.0133), tensor(0.1163), tensor(0.0359), tensor(-0.0116), tensor(0.0468), tensor(0.0885), tensor(0.0503), tensor(0.0718), tensor(0.0713), tensor(0.0162), tensor(0.0332), tensor(0.1126), tensor(-0.0160), tensor(-0.0220), tensor(0.0445), tensor(0.0144), tensor(0.0030), tensor(0.0846), tensor(0.0576), tensor(-0.0025), tensor(0.0308), tensor(0.0431), tensor(0.0080), tensor(0.0678), tensor(0.0750), tensor(-0.0239), tensor(0.0348), tensor(0.0080), tensor(-0.0077), tensor(0.0013), tensor(0.0469), tensor(0.0421), tensor(0.0212), tensor(0.0475), tensor(0.0290), tensor(0.0549), tensor(0.0172), tensor(0.0944), tensor(0.0751), tensor(0.0067), tensor(0.0809), tensor(-0.0170), tensor(0.2350), tensor(0.0043), tensor(0.0810), tensor(0.0179), tensor(0.0724), tensor(0.0795), tensor(-0.0029), tensor(0.0577), tensor(0.0176), tensor(0.1012), tensor(0.5139), tensor(0.0151), tensor(0.0493), tensor(0.0313), tensor(0.0313), tensor(0.0028), tensor(0.0195), tensor(0.0438), tensor(0.0227), tensor(0.0702), tensor(0.0418), tensor(0.0156), tensor(0.0181), tensor(-0.0065), tensor(0.0032), tensor(0.0009), tensor(0.1867), tensor(0.0137), tensor(0.1045), tensor(0.1211), tensor(0.0610), tensor(0.0212), tensor(0.0814), tensor(0.0489), tensor(0.0458), tensor(0.0149), tensor(-0.0675), tensor(0.0225), tensor(0.0234), tensor(0.0869), tensor(0.0321), tensor(0.0516), tensor(0.0604), tensor(0.1227), tensor(-0.0013), tensor(0.0350), tensor(0.0297), tensor(0.0315), tensor(0.0746), tensor(0.0809), tensor(0.0244), tensor(0.0232), tensor(-0.0006), tensor(0.0883), tensor(-0.0134), tensor(0.0444), tensor(0.0446), tensor(0.0265), tensor(0.0969), tensor(-0.0018), tensor(0.0473), tensor(0.1202), tensor(0.0345), tensor(0.0177), tensor(0.0912), tensor(0.0286), tensor(0.0109), tensor(-0.0105), tensor(0.0141), tensor(0.0391), tensor(0.0096), tensor(0.0287), tensor(0.0281), tensor(0.0113), tensor(0.0241), tensor(0.0137), tensor(0.0984), tensor(-0.0269), tensor(0.0614), tensor(0.1915), tensor(0.0418), tensor(0.0170), tensor(0.0161), tensor(0.0640), tensor(0.0258), tensor(0.0555), tensor(0.0308), tensor(0.1140), tensor(0.0653), tensor(0.0632), tensor(0.0370), tensor(0.0153), tensor(0.0689), tensor(0.0058), tensor(0.0050), tensor(0.0215), tensor(0.0368), tensor(0.0290), tensor(0.0231), tensor(0.0132), tensor(0.0084), tensor(0.0286), tensor(-0.0124), tensor(0.0127), tensor(0.0589), tensor(-0.0011), tensor(-0.0020), tensor(0.0441), tensor(-0.0067), tensor(0.0018), tensor(0.0231), tensor(0.0560), tensor(0.0217), tensor(0.0112), tensor(0.0349), tensor(0.0184), tensor(-0.0216), tensor(0.0873), tensor(0.2814), tensor(-0.0019), tensor(0.0300), tensor(0.2001), tensor(0.0694), tensor(0.1237), tensor(0.1427), tensor(0.0481), tensor(0.0091), tensor(0.1115), tensor(0.0759), tensor(0.0496), tensor(0.0024), tensor(0.0595), tensor(0.0366), tensor(0.0016), tensor(0.0120), tensor(-0.0045), tensor(0.0301), tensor(0.0452), tensor(-0.0013), tensor(0.0794), tensor(0.0432), tensor(0.0300), tensor(-0.0048), tensor(0.0425), tensor(-0.0025), tensor(-0.0053), tensor(0.0142), tensor(0.0751), tensor(0.0037), tensor(0.0213), tensor(0.1479), tensor(0.0352), tensor(0.0773), tensor(0.0765), tensor(0.1104), tensor(0.0676), tensor(-0.0029), tensor(0.0108), tensor(0.0204), tensor(-0.0197), tensor(-0.0104), tensor(-0.0095), tensor(0.0907), tensor(0.0457), tensor(0.2611), tensor(0.0222), tensor(0.0630), tensor(0.0162), tensor(0.0575), tensor(0.0327), tensor(0.0433), tensor(0.0504), tensor(0.0049), tensor(0.0681), tensor(0.0092), tensor(-0.0112), tensor(0.0479), tensor(0.0406), tensor(0.0220), tensor(-0.0234), tensor(0.0051), tensor(0.0108), tensor(0.0010), tensor(0.1351), tensor(-0.0110), tensor(0.0280), tensor(0.0638), tensor(-0.0294), tensor(0.0208), tensor(0.0082), tensor(0.0484), tensor(-0.0064), tensor(0.0749), tensor(-0.0468), tensor(0.0216), tensor(0.0247), tensor(0.2270), tensor(0.0103), tensor(-0.0181), tensor(0.0699), tensor(0.0380), tensor(0.0423), tensor(0.0194), tensor(0.0580), tensor(0.0302), tensor(0.0051), tensor(0.0432), tensor(0.0176), tensor(0.0177), tensor(0.0615), tensor(-0.0042), tensor(0.0345), tensor(0.0009), tensor(0.0566), tensor(0.0189), tensor(0.0182), tensor(0.0409), tensor(-0.0070), tensor(0.0259), tensor(-0.0002), tensor(0.0361), tensor(0.0332), tensor(0.0068), tensor(0.0466), tensor(0.0189), tensor(0.0259), tensor(0.0695), tensor(0.0494), tensor(0.0140), tensor(0.0045), tensor(0.0412), tensor(0.0429), tensor(-0.0130), tensor(-0.0340), tensor(0.1193), tensor(0.0386), tensor(0.0090), tensor(0.0754), tensor(0.0551), tensor(0.0427), tensor(0.0911), tensor(0.0640), tensor(0.0425), tensor(0.0553), tensor(0.1118), tensor(0.0291), tensor(0.0263), tensor(0.0040), tensor(0.0620), tensor(0.0837), tensor(0.0401), tensor(0.0676), tensor(-0.0032), tensor(0.0526), tensor(-0.0231), tensor(0.0217), tensor(0.0281), tensor(0.0592), tensor(0.0630), tensor(0.0380), tensor(0.1733), tensor(0.0239), tensor(0.0598), tensor(-0.0172), tensor(0.0358), tensor(0.0774), tensor(0.0354), tensor(0.0179), tensor(0.1099), tensor(0.0383), tensor(0.0110), tensor(0.0460), tensor(0.0417), tensor(0.0497), tensor(-0.0022), tensor(-0.0067), tensor(0.0636), tensor(0.5870), tensor(0.0504), tensor(0.0707), tensor(0.0346), tensor(0.0428), tensor(0.0594), tensor(0.0723), tensor(0.0457), tensor(-0.0003), tensor(0.0586), tensor(0.0404), tensor(0.0105), tensor(0.0469), tensor(0.0132), tensor(-0.0023), tensor(0.0301), tensor(-0.0041), tensor(0.0257), tensor(0.0152), tensor(0.0307), tensor(0.0285), tensor(0.0378), tensor(0.0169), tensor(-0.0260), tensor(0.0528), tensor(0.0492), tensor(0.0459), tensor(0.1163), tensor(0.0482), tensor(0.0379), tensor(0.0809), tensor(0.0205), tensor(0.0222), tensor(-0.0207), tensor(0.0281), tensor(0.0311), tensor(0.0484), tensor(0.0554), tensor(0.0606), tensor(0.0593), tensor(0.2041), tensor(0.0399), tensor(0.0130), tensor(0.0277), tensor(0.1579), tensor(0.0357), tensor(-0.0014), tensor(-0.0322), tensor(0.0489), tensor(-0.0133), tensor(-0.0117), tensor(0.0117), tensor(0.0149), tensor(0.0451), tensor(0.0786), tensor(0.0747), tensor(0.0582), tensor(0.1134), tensor(0.0375), tensor(-0.0311), tensor(0.0778), tensor(-0.0040), tensor(0.1965), tensor(0.1023), tensor(0.0565), tensor(0.0622), tensor(0.0033), tensor(0.0370), tensor(0.0882), tensor(0.0743), tensor(0.0082), tensor(0.0356), tensor(0.0115), tensor(-0.0051), tensor(0.0592), tensor(0.0175), tensor(0.0667), tensor(0.0258), tensor(0.0695), tensor(0.0148), tensor(0.0731), tensor(0.0149), tensor(0.0285), tensor(0.0413), tensor(0.0402), tensor(0.0603), tensor(-0.0120), tensor(0.0581), tensor(0.0820), tensor(0.0384), tensor(0.0987), tensor(0.0521), tensor(0.0622), tensor(0.0133), tensor(0.1467), tensor(0.0037), tensor(0.0938), tensor(0.0424), tensor(0.0169), tensor(0.0784), tensor(0.0276), tensor(0.0564), tensor(0.0297), tensor(0.0639), tensor(0.0362), tensor(0.0343), tensor(-0.0667), tensor(0.0757), tensor(-0.0103), tensor(0.0268), tensor(0.0137), tensor(0.1380), tensor(0.0032), tensor(0.0190), tensor(0.0162), tensor(0.0282), tensor(-0.0053), tensor(-0.0087), tensor(0.0434), tensor(0.0850), tensor(0.0335), tensor(-0.0236), tensor(-0.0014), tensor(0.0469), tensor(0.0225), tensor(0.0916), tensor(0.0252), tensor(0.0545), tensor(0.0024), tensor(0.0080), tensor(0.0026), tensor(0.0386), tensor(-0.0145), tensor(0.0410), tensor(0.0113), tensor(0.0679), tensor(0.0166), tensor(0.2182), tensor(0.0321), tensor(0.0873), tensor(0.0619), tensor(-0.0404), tensor(0.0233), tensor(0.0394), tensor(-0.0118), tensor(0.0125), tensor(0.1181), tensor(-0.0286), tensor(0.0029), tensor(0.0023), tensor(0.0064), tensor(0.0561), tensor(0.0905), tensor(-0.0047), tensor(0.0488), tensor(0.0307), tensor(0.0205), tensor(0.0332), tensor(0.0453), tensor(0.0474), tensor(0.0897), tensor(0.0629), tensor(0.0748), tensor(0.1105), tensor(0.0208), tensor(0.0940), tensor(-0.0246), tensor(0.0664), tensor(-0.0179), tensor(0.0040), tensor(0.0103), tensor(0.0405), tensor(0.0046), tensor(0.0441), tensor(0.0020), tensor(-0.0181), tensor(0.0448), tensor(-0.0073), tensor(0.0583), tensor(0.0410), tensor(-0.0123), tensor(0.0458), tensor(0.0460), tensor(0.0643), tensor(0.0335), tensor(0.4619), tensor(-0.0338), tensor(0.0755), tensor(0.0071), tensor(0.0057), tensor(-0.0148), tensor(0.0309), tensor(0.0357), tensor(0.0386), tensor(0.0278), tensor(-0.0075), tensor(0.0294), tensor(0.0704), tensor(-0.0272), tensor(0.0460), tensor(0.0075), tensor(0.0402), tensor(0.0242), tensor(-0.0234), tensor(0.0465), tensor(-0.0025), tensor(0.0336), tensor(0.0299), tensor(0.1619), tensor(0.0471), tensor(0.0396), tensor(0.0073), tensor(-0.0201), tensor(0.3017), tensor(0.0764), tensor(-0.0060), tensor(0.0013), tensor(0.3192), tensor(0.0287), tensor(0.0733), tensor(0.0706), tensor(0.0577), tensor(-0.0031), tensor(0.0047), tensor(0.1623), tensor(0.0700), tensor(0.0392), tensor(0.0478), tensor(0.0819), tensor(-0.0355), tensor(0.0313), tensor(0.0531), tensor(0.0015), tensor(0.0575), tensor(0.0335), tensor(0.1435), tensor(0.0403), tensor(0.0341), tensor(0.0527), tensor(0.0310), tensor(-0.0035), tensor(-0.0159), tensor(-0.0131), tensor(-0.0224), tensor(0.0742), tensor(-0.0009), tensor(-0.0046), tensor(0.0455), tensor(0.0584), tensor(-0.0023), tensor(0.0670), tensor(0.0296), tensor(-0.0234), tensor(-0.0202), tensor(-0.0307), tensor(-0.0267), tensor(0.0261), tensor(0.0562), tensor(-0.0039), tensor(0.0492), tensor(0.0543), tensor(-0.0074), tensor(0.0217), tensor(0.0814), tensor(0.1154), tensor(0.1244), tensor(0.0843), tensor(-0.0109), tensor(0.0256), tensor(0.0257), tensor(0.0034), tensor(0.0471), tensor(0.0203), tensor(0.1239), tensor(0.0412), tensor(0.0362), tensor(0.0141), tensor(-0.0070), tensor(0.0788), tensor(0.0452), tensor(0.0165), tensor(0.0171), tensor(-0.0226), tensor(0.0841), tensor(0.0758), tensor(0.0351), tensor(-0.0043), tensor(0.0463), tensor(-0.0062), tensor(-0.0365), tensor(0.0368), tensor(0.0409), tensor(-0.0289), tensor(-0.0028), tensor(0.0055), tensor(0.0225), tensor(-0.0178), tensor(0.0602), tensor(0.0702), tensor(0.0509), tensor(0.0586), tensor(0.0295), tensor(0.0926), tensor(0.0088), tensor(0.0086), tensor(0.0388), tensor(-0.0121), tensor(0.0252), tensor(0.0396), tensor(0.0579), tensor(0.0252), tensor(0.0292), tensor(-0.0418), tensor(-0.0404), tensor(0.0056), tensor(0.0403), tensor(0.0740), tensor(0.0039), tensor(0.0278), tensor(0.0345), tensor(0.0850), tensor(0.0008), tensor(0.0581), tensor(0.0361), tensor(0.0221), tensor(0.0330), tensor(0.0764), tensor(-0.0277), tensor(0.1200), tensor(0.0553), tensor(0.0125), tensor(0.0398), tensor(0.0606), tensor(0.0313), tensor(0.1050), tensor(-0.0009), tensor(0.0766), tensor(0.0651), tensor(-0.0398), tensor(0.0078), tensor(0.0299), tensor(-0.0375), tensor(-0.0049), tensor(0.0325), tensor(-0.0322), tensor(0.0235), tensor(-0.0126), tensor(0.0388), tensor(0.0766), tensor(0.0419), tensor(0.0319), tensor(0.0536), tensor(0.0507), tensor(0.0093), tensor(0.0469), tensor(0.0340), tensor(0.0397), tensor(0.0076), tensor(0.0282), tensor(0.0033), tensor(0.0138), tensor(0.0544), tensor(0.0355), tensor(0.0540), tensor(0.0509), tensor(0.0826), tensor(0.0315), tensor(0.0164), tensor(0.0202), tensor(0.0592), tensor(0.0774), tensor(0.0593), tensor(0.0046), tensor(0.0121), tensor(0.0048), tensor(0.0331), tensor(0.0090), tensor(0.0819), tensor(0.0300), tensor(0.0698), tensor(0.0365), tensor(0.0734), tensor(0.1117), tensor(0.0309), tensor(0.0538), tensor(0.0027), tensor(0.0066), tensor(-0.0334), tensor(0.0537), tensor(0.0600), tensor(0.0064), tensor(-0.0022), tensor(0.0740), tensor(-0.0178), tensor(0.0140), tensor(0.0544), tensor(0.0592), tensor(-0.0007), tensor(0.0052), tensor(0.0208), tensor(0.1225), tensor(-0.0288), tensor(0.1236), tensor(0.0802), tensor(0.0467), tensor(-0.0250), tensor(0.0062), tensor(0.0240), tensor(0.1145), tensor(0.0435), tensor(0.0455), tensor(0.0073), tensor(0.0173), tensor(0.1067), tensor(0.0269), tensor(0.1189), tensor(0.0369), tensor(0.0084), tensor(0.0436), tensor(0.0221), tensor(0.0788), tensor(0.0380), tensor(0.0531), tensor(0.0245), tensor(0.0570), tensor(0.0150), tensor(0.0264), tensor(0.0172), tensor(0.0338), tensor(0.0335), tensor(0.0674), tensor(0.0052), tensor(0.0317), tensor(0.0247), tensor(0.0922), tensor(0.0157), tensor(0.0145), tensor(0.0273), tensor(0.0411), tensor(0.0361), tensor(-0.0080), tensor(0.0447), tensor(0.0537), tensor(0.0412), tensor(0.1101), tensor(0.0143), tensor(0.6279), tensor(0.0773), tensor(0.0612), tensor(0.0344), tensor(-0.0099), tensor(-0.0005), tensor(0.0496), tensor(0.0376), tensor(0.0498), tensor(-0.0065), tensor(0.0510), tensor(0.0335), tensor(0.0282), tensor(0.0809), tensor(0.1033), tensor(0.0396), tensor(0.0244), tensor(0.0227), tensor(-0.0074), tensor(0.0053), tensor(0.0676), tensor(0.0189), tensor(0.3035), tensor(0.0135), tensor(0.0664), tensor(-0.0013), tensor(0.0674), tensor(0.0159), tensor(0.0194), tensor(0.0241), tensor(0.0250), tensor(0.0287), tensor(0.0459), tensor(0.0602), tensor(0.0385), tensor(0.0936), tensor(0.0492), tensor(-0.0122), tensor(-0.0058), tensor(0.0332), tensor(0.0023), tensor(0.0277), tensor(0.0545), tensor(0.0472), tensor(0.0383), tensor(0.0192), tensor(0.0888), tensor(0.0254), tensor(0.1206), tensor(0.0363), tensor(0.0533), tensor(0.0386), tensor(-0.0065), tensor(0.0141), tensor(0.0485), tensor(-0.0080), tensor(0.0810), tensor(0.0805), tensor(0.0580), tensor(0.1255), tensor(0.0242), tensor(0.0455), tensor(0.0790), tensor(0.0337), tensor(0.0226), tensor(0.0671), tensor(0.0008), tensor(0.0374), tensor(0.0453), tensor(0.0127), tensor(0.0234), tensor(0.0251), tensor(0.5121), tensor(0.0637), tensor(0.0062), tensor(0.0330), tensor(0.0769), tensor(0.0461), tensor(0.0003), tensor(0.0836), tensor(0.0221), tensor(0.0305), tensor(-0.0293), tensor(-0.0076), tensor(0.0186), tensor(-0.0234), tensor(0.0683), tensor(0.0762), tensor(0.1310), tensor(0.0965), tensor(0.0080), tensor(0.0511), tensor(-0.0326), tensor(0.0759), tensor(-0.0125), tensor(0.3262), tensor(0.0286), tensor(0.0146), tensor(-0.0067), tensor(0.0443), tensor(0.0283), tensor(0.0181), tensor(0.0431), tensor(0.0582), tensor(-0.0556), tensor(0.0642), tensor(-0.0184), tensor(0.0366), tensor(0.0540), tensor(0.0291), tensor(0.0327), tensor(0.1092), tensor(0.0040), tensor(0.0233), tensor(0.0500), tensor(-0.0278), tensor(0.0479), tensor(0.0788), tensor(0.1329), tensor(0.1156), tensor(0.3431), tensor(0.0867), tensor(0.0230), tensor(0.0657), tensor(0.0863), tensor(-0.0165), tensor(-0.0208), tensor(0.0923), tensor(0.0105), tensor(0.0137), tensor(0.0721), tensor(0.0629), tensor(-0.0035), tensor(0.0236), tensor(0.0230), tensor(0.0785), tensor(0.0464), tensor(0.0283), tensor(0.0040), tensor(0.0080), tensor(0.0310), tensor(-0.0029), tensor(0.0838), tensor(0.1271), tensor(0.0079), tensor(0.0497), tensor(0.0320), tensor(0.0188), tensor(0.0740), tensor(0.0035), tensor(0.0363), tensor(-0.0442), tensor(0.0118), tensor(-0.0071), tensor(0.0555), tensor(0.0787), tensor(0.0471), tensor(0.0326), tensor(0.0296), tensor(0.0791), tensor(-0.0396), tensor(0.0720), tensor(-0.0048), tensor(0.0380), tensor(0.0104), tensor(0.0028), tensor(0.0080), tensor(-0.0107), tensor(0.0828), tensor(0.0116), tensor(0.1170), tensor(0.0280), tensor(0.0224), tensor(0.0127), tensor(0.0523), tensor(0.0413), tensor(0.0587), tensor(0.1207), tensor(0.0958), tensor(0.0545), tensor(0.1318), tensor(0.0169), tensor(0.0546), tensor(-0.0211), tensor(0.0972), tensor(-0.0352), tensor(0.0199), tensor(0.0929), tensor(0.0315), tensor(0.0788), tensor(-0.0102), tensor(0.0887), tensor(0.0245), tensor(0.0561), tensor(0.0273), tensor(0.0503), tensor(0.0781), tensor(0.0251), tensor(-0.0072), tensor(0.0102), tensor(0.0822), tensor(0.0018), tensor(0.0838), tensor(0.0097), tensor(0.0463), tensor(0.0235), tensor(0.0237), tensor(0.0560), tensor(0.0372), tensor(0.0174), tensor(0.0669), tensor(-0.0212), tensor(0.0196), tensor(0.0803), tensor(0.0163), tensor(-0.0299), tensor(-0.0037), tensor(0.0636), tensor(0.0193), tensor(0.0345), tensor(0.0717), tensor(-0.0313), tensor(0.0422), tensor(0.0154), tensor(0.2438), tensor(0.0680), tensor(0.0695), tensor(-0.0088), tensor(0.0138), tensor(0.1262), tensor(0.0354), tensor(0.0135), tensor(0.0007), tensor(0.0095), tensor(0.0582), tensor(0.0058), tensor(0.0163), tensor(-0.0041), tensor(0.0991), tensor(0.0543), tensor(-0.0220), tensor(0.0589), tensor(0.0286), tensor(0.0854), tensor(0.0165), tensor(-0.0289), tensor(0.0047), tensor(0.0485), tensor(-0.0035), tensor(0.0451), tensor(-0.0141), tensor(-0.0187), tensor(0.0099), tensor(0.0523), tensor(0.0089), tensor(0.0039), tensor(0.0137), tensor(0.0250), tensor(-0.0131), tensor(-0.0529), tensor(0.0002), tensor(0.0288), tensor(-0.0341), tensor(0.0357), tensor(0.0555), tensor(0.1005), tensor(0.0799), tensor(0.0126), tensor(0.0726), tensor(0.3528), tensor(0.6090), tensor(0.0871), tensor(0.1679), tensor(0.0548), tensor(0.0412), tensor(-0.0189), tensor(-0.0058), tensor(0.0569), tensor(0.0018), tensor(0.0266), tensor(0.0905), tensor(0.0129), tensor(0.0671), tensor(0.0048), tensor(0.0999), tensor(0.0544), tensor(0.2295), tensor(0.0025), tensor(-0.0055), tensor(0.0209), tensor(0.0855), tensor(0.0385), tensor(0.0193), tensor(0.0730), tensor(0.0099), tensor(0.0296), tensor(0.0128), tensor(0.0909), tensor(0.0236), tensor(-0.0179), tensor(-0.0316), tensor(0.0080), tensor(0.0252), tensor(0.0122), tensor(0.0206), tensor(0.0160), tensor(0.0621), tensor(0.0240), tensor(0.0705), tensor(0.0653), tensor(0.3654), tensor(0.0994), tensor(-0.0084), tensor(0.0046), tensor(0.0990), tensor(0.0282), tensor(0.0091), tensor(0.0318), tensor(0.0280), tensor(0.0157), tensor(0.0736), tensor(0.1096), tensor(0.1172), tensor(-0.0147), tensor(0.0122), tensor(0.0970), tensor(0.0573), tensor(-0.0170), tensor(0.0115), tensor(0.0301), tensor(0.1734), tensor(-0.0015), tensor(0.0633), tensor(0.0190), tensor(-0.0312), tensor(0.1009), tensor(0.0725), tensor(0.0135), tensor(0.0418), tensor(0.0499), tensor(0.0293), tensor(0.0355), tensor(-0.0160), tensor(-0.0009), tensor(0.3338), tensor(-0.0252), tensor(0.4124), tensor(0.0123), tensor(0.0013), tensor(-0.0250), tensor(0.0257), tensor(-0.0168), tensor(0.0222), tensor(0.1010), tensor(0.0600), tensor(-0.0112), tensor(0.0347), tensor(0.0326), tensor(0.0141), tensor(0.0101), tensor(0.0299), tensor(0.0388), tensor(0.0312), tensor(0.0788), tensor(0.1020), tensor(0.0591), tensor(0.0191), tensor(0.0132), tensor(0.0768), tensor(0.1101), tensor(0.0272), tensor(0.0300), tensor(0.0237), tensor(0.0901), tensor(0.0076), tensor(0.0066), tensor(0.0823), tensor(-0.0308), tensor(0.0460), tensor(0.0677), tensor(0.0790), tensor(0.0664), tensor(0.0221), tensor(0.0322), tensor(0.0675), tensor(0.0486), tensor(-0.0024), tensor(0.0506), tensor(0.0272), tensor(-0.0120), tensor(0.0116), tensor(0.0416), tensor(0.0143), tensor(-0.0009), tensor(0.0168), tensor(-0.0153), tensor(-0.0147), tensor(0.0009), tensor(0.0975), tensor(0.0380), tensor(0.0214), tensor(0.1315), tensor(0.0302), tensor(0.0955), tensor(0.0855), tensor(0.1323), tensor(0.0734), tensor(0.0046), tensor(0.0509), tensor(0.0125), tensor(0.0394), tensor(-0.0134), tensor(0.0251), tensor(0.0146), tensor(-0.0123), tensor(0.0386), tensor(0.0431), tensor(-0.0103), tensor(0.0428), tensor(0.0358), tensor(-0.0141), tensor(-0.0067), tensor(0.0419), tensor(0.0345), tensor(0.0265), tensor(0.1008), tensor(0.0029), tensor(0.0433), tensor(0.0771), tensor(0.0530), tensor(0.0167), tensor(0.1369), tensor(0.0402), tensor(0.0100), tensor(0.0191), tensor(0.0456), tensor(0.0447), tensor(0.0287), tensor(0.0656), tensor(0.0275), tensor(-0.0274), tensor(0.0125), tensor(0.0317), tensor(0.0607), tensor(0.1178), tensor(0.0145), tensor(0.0563), tensor(0.0229), tensor(0.0561), tensor(0.0085), tensor(-0.0131), tensor(0.0165), tensor(0.3204), tensor(0.0509), tensor(0.0284), tensor(0.0228), tensor(0.0845), tensor(-0.0056), tensor(0.0426), tensor(0.0230), tensor(0.0779), tensor(0.0683), tensor(0.0071), tensor(0.0488), tensor(0.1013), tensor(0.0084), tensor(0.0349), tensor(0.0458), tensor(0.1444), tensor(0.0192), tensor(0.0530), tensor(0.0290), tensor(0.0930), tensor(0.0006), tensor(0.0176), tensor(-0.0177), tensor(0.0838), tensor(-0.0013), tensor(0.0133), tensor(0.0362), tensor(0.0527), tensor(0.0566), tensor(0.0469), tensor(0.0097), tensor(0.0493), tensor(0.0531), tensor(0.0563), tensor(0.0295), tensor(0.0140), tensor(0.0734), tensor(0.0379), tensor(0.0234), tensor(0.0733), tensor(0.0724), tensor(-0.0008), tensor(0.0675), tensor(0.1059), tensor(0.0560), tensor(0.0705), tensor(0.0477), tensor(0.0430), tensor(-0.0068), tensor(0.0957), tensor(0.0753), tensor(0.1303), tensor(0.0227), tensor(0.0449), tensor(0.0206), tensor(0.0657), tensor(0.0329), tensor(0.0543), tensor(0.0528), tensor(0.0028), tensor(0.0487), tensor(0.0314), tensor(0.0057), tensor(0.0506), tensor(0.0521), tensor(0.0521), tensor(0.0030), tensor(0.0260), tensor(-0.0059), tensor(0.0111), tensor(-0.0018), tensor(0.0373), tensor(0.0478), tensor(0.0238), tensor(0.1073), tensor(0.0002), tensor(-0.0077), tensor(0.0530), tensor(0.0881), tensor(0.0259), tensor(0.0315), tensor(0.0473), tensor(0.0708), tensor(0.0090), tensor(0.0934), tensor(0.0523), tensor(0.0009), tensor(0.0308), tensor(0.0592), tensor(-0.0024), tensor(0.0709), tensor(0.0483), tensor(0.1020), tensor(0.0446), tensor(0.0176), tensor(0.0709), tensor(0.0008), tensor(0.0131), tensor(0.0146), tensor(0.0479), tensor(0.4466), tensor(0.0449), tensor(0.0307), tensor(-0.0310), tensor(0.0047), tensor(0.0449), tensor(-0.0077), tensor(0.0106), tensor(0.0590), tensor(0.0389), tensor(0.0546), tensor(0.0516), tensor(0.0403), tensor(0.0816), tensor(0.0198), tensor(0.0116), tensor(0.0110), tensor(0.0717), tensor(-0.0011), tensor(0.0555), tensor(0.0537), tensor(-0.0104), tensor(0.0057), tensor(0.1202), tensor(-0.0128), tensor(0.0079), tensor(0.0429), tensor(-0.0053), tensor(0.1774), tensor(0.0294), tensor(0.0269), tensor(0.1691), tensor(0.0498), tensor(0.0300), tensor(0.0235), tensor(0.0797), tensor(0.0387), tensor(0.0138), tensor(0.1210), tensor(0.0522), tensor(0.0239), tensor(0.0761), tensor(0.0179), tensor(0.0297), tensor(0.0455), tensor(-0.0097), tensor(-0.0099), tensor(0.0531), tensor(0.0622), tensor(0.0229), tensor(0.0118), tensor(0.0589), tensor(0.0603), tensor(0.0133), tensor(0.1213), tensor(0.0270), tensor(0.0383), tensor(0.0277), tensor(-0.0298), tensor(0.0585), tensor(0.0165), tensor(0.0627), tensor(0.0214), tensor(0.0080), tensor(0.0903), tensor(-0.0079), tensor(0.0100), tensor(0.1545), tensor(-0.0348), tensor(-0.0154), tensor(0.0480), tensor(-0.0028), tensor(0.0526), tensor(0.0068), tensor(0.0986), tensor(0.0972), tensor(0.4053), tensor(0.0093), tensor(0.0021), tensor(0.0313), tensor(0.0513), tensor(0.0565), tensor(-0.0217), tensor(0.1174), tensor(0.0040), tensor(0.0061), tensor(0.0610), tensor(-0.0282), tensor(0.0820), tensor(0.0454), tensor(0.0344), tensor(0.0443), tensor(0.0340), tensor(0.0357), tensor(0.0402), tensor(0.0391), tensor(0.0064), tensor(0.0353), tensor(-0.0116), tensor(0.0167), tensor(0.0213), tensor(0.0808), tensor(-0.0049), tensor(0.0267), tensor(0.0546), tensor(0.0592), tensor(0.1061), tensor(0.0480), tensor(0.3670), tensor(0.0060), tensor(0.0539), tensor(0.0702), tensor(0.0194), tensor(0.0409), tensor(0.0705), tensor(0.0140), tensor(0.0257), tensor(0.0294), tensor(0.0396), tensor(-0.0033), tensor(0.0455), tensor(0.0577), tensor(0.0140), tensor(0.2838), tensor(0.1314), tensor(-0.0022), tensor(-0.0343), tensor(0.0756), tensor(0.0610), tensor(0.0471), tensor(0.0309), tensor(0.0566), tensor(0.0085), tensor(0.0155), tensor(-0.0078), tensor(0.0825), tensor(0.0369), tensor(0.1707), tensor(0.0261), tensor(0.0613), tensor(0.0232), tensor(0.2040), tensor(0.0928), tensor(0.0280), tensor(0.0759), tensor(0.0447), tensor(0.0214), tensor(-0.0144), tensor(0.0265), tensor(0.0225), tensor(0.0653), tensor(0.0561), tensor(-0.0132), tensor(0.0094), tensor(0.0265), tensor(0.0410), tensor(0.0066), tensor(0.0012), tensor(0.0018), tensor(0.0873), tensor(0.0047), tensor(0.0070), tensor(0.0644), tensor(0.0741), tensor(-0.0096), tensor(0.0112), tensor(-0.0106), tensor(0.0907), tensor(0.0365), tensor(0.0348), tensor(0.0422), tensor(0.0399), tensor(0.0295), tensor(0.0333), tensor(0.0293), tensor(0.0380), tensor(0.0815), tensor(0.0306), tensor(0.0216), tensor(-0.0104), tensor(0.0137), tensor(0.0935), tensor(0.0223), tensor(0.0557), tensor(0.0512), tensor(0.0194), tensor(0.0559), tensor(0.0707), tensor(-0.0002), tensor(0.0200), tensor(0.0295), tensor(0.0339), tensor(0.0211), tensor(-0.0010), tensor(0.1315), tensor(0.0501), tensor(0.0457), tensor(0.0707), tensor(0.0484), tensor(0.0152), tensor(0.0971), tensor(0.0299), tensor(0.0594), tensor(0.0454), tensor(0.0441), tensor(0.1152), tensor(0.0230), tensor(0.1315), tensor(0.0302), tensor(0.2267), tensor(0.0125), tensor(0.1173), tensor(0.0674), tensor(0.2725), tensor(0.0372), tensor(0.0859), tensor(-0.0159), tensor(0.2380), tensor(0.0358), tensor(0.0706), tensor(0.0147), tensor(-0.0396), tensor(0.1141), tensor(-0.0082), tensor(0.0065), tensor(0.0278), tensor(0.0227), tensor(0.0066), tensor(0.0249), tensor(0.0126), tensor(0.0078), tensor(0.0745), tensor(0.2154), tensor(0.0555), tensor(0.0106), tensor(0.0453), tensor(0.0620), tensor(0.0466), tensor(0.0530), tensor(-0.0078), tensor(0.0217), tensor(0.0111), tensor(0.0374), tensor(0.0452), tensor(0.0320), tensor(0.0486), tensor(-0.0029), tensor(0.0342), tensor(0.0519), tensor(0.0122), tensor(0.0420), tensor(0.0387), tensor(0.0272), tensor(0.0618), tensor(0.0634), tensor(0.0146), tensor(0.0827), tensor(0.0639), tensor(0.0985), tensor(0.0741), tensor(-0.0182), tensor(0.0843), tensor(0.1252), tensor(0.0055), tensor(0.0394), tensor(-0.0166), tensor(0.0115), tensor(0.0246), tensor(0.0155), tensor(-0.0434), tensor(0.0449), tensor(0.1392), tensor(0.0393), tensor(-0.0267), tensor(0.0179), tensor(0.0468), tensor(0.0053), tensor(0.0269), tensor(0.0692), tensor(0.0843), tensor(0.0268), tensor(-0.0311), tensor(-0.0274), tensor(0.0377), tensor(-0.0066), tensor(0.0023), tensor(0.0450), tensor(0.1073), tensor(0.0115), tensor(-0.0047), tensor(0.0382), tensor(-0.0018), tensor(0.0185), tensor(-0.0006), tensor(0.0431), tensor(0.0312), tensor(0.0200), tensor(0.0066), tensor(0.0649), tensor(0.0249), tensor(0.0388), tensor(0.0194), tensor(0.0223), tensor(0.0060), tensor(0.0457), tensor(0.0734), tensor(0.1229), tensor(0.0972), tensor(0.0182), tensor(0.0047), tensor(0.0098), tensor(0.0697), tensor(0.0946), tensor(0.0516), tensor(0.1103), tensor(0.0126), tensor(0.0728), tensor(0.0263), tensor(0.0310), tensor(0.0757), tensor(0.0501), tensor(0.0364), tensor(0.0143), tensor(-0.0227), tensor(0.0995), tensor(0.0568), tensor(0.0441), tensor(-0.0053), tensor(-0.0036), tensor(0.0787), tensor(-0.0373), tensor(0.0845), tensor(0.0419), tensor(0.0440), tensor(-0.0033), tensor(0.0383), tensor(0.0812), tensor(0.0340), tensor(0.0462), tensor(0.0816), tensor(0.0231), tensor(-0.0073), tensor(0.0206), tensor(0.0571), tensor(0.0398), tensor(0.0301), tensor(0.0371), tensor(0.0749), tensor(0.0715), tensor(0.0362), tensor(0.0505), tensor(0.0876), tensor(-0.0076), tensor(-0.0376), tensor(0.0581), tensor(0.0764), tensor(0.0611), tensor(0.0043), tensor(-0.0120), tensor(0.0643), tensor(0.0550), tensor(0.0976), tensor(-0.0593), tensor(0.0552), tensor(0.0074), tensor(0.0533), tensor(0.0746), tensor(-0.0130), tensor(0.0141), tensor(0.1421), tensor(0.0511), tensor(0.0321), tensor(0.0440), tensor(0.0124), tensor(0.0140), tensor(-0.0012), tensor(-0.0163), tensor(0.0168), tensor(-0.0352), tensor(-0.0455), tensor(-0.0185), tensor(0.0144), tensor(0.0028), tensor(-0.0070), tensor(0.0265), tensor(0.0377), tensor(0.0401), tensor(0.0233), tensor(0.0498), tensor(-0.0093), tensor(0.0327), tensor(0.0588), tensor(0.0271), tensor(0.0434), tensor(0.1107), tensor(-0.0069), tensor(0.0472), tensor(0.0303), tensor(0.0414), tensor(0.0225), tensor(0.0772), tensor(0.0390), tensor(0.0220), tensor(0.0606), tensor(-0.0009), tensor(0.0474), tensor(-0.0036), tensor(-0.0250), tensor(0.0442), tensor(0.0181), tensor(0.0330), tensor(-0.0142), tensor(0.0194), tensor(0.0108), tensor(0.0087), tensor(0.0225), tensor(0.0428), tensor(0.0274), tensor(-0.0061), tensor(0.1540), tensor(0.0431), tensor(0.0913), tensor(0.0181), tensor(0.0108), tensor(-0.0027), tensor(0.0041), tensor(0.0664), tensor(0.0508), tensor(0.0400), tensor(-0.0099), tensor(0.0151), tensor(0.0287), tensor(0.0580), tensor(0.0608), tensor(0.0752), tensor(0.0069), tensor(0.0416), tensor(0.4382), tensor(0.0006), tensor(0.1061), tensor(0.0111), tensor(0.0020), tensor(0.0217), tensor(-0.0089), tensor(0.0553), tensor(0.0447), tensor(0.0317), tensor(0.0141), tensor(0.0189), tensor(-0.0451), tensor(0.0570), tensor(0.0183), tensor(0.0003), tensor(0.0266), tensor(0.0127), tensor(0.0362), tensor(0.0286), tensor(0.0358), tensor(0.0276), tensor(0.0263), tensor(0.0183), tensor(0.0189), tensor(0.0590), tensor(0.0288), tensor(0.0625), tensor(0.0313), tensor(0.0230), tensor(-0.0384), tensor(0.0462), tensor(0.0915), tensor(-0.0457), tensor(0.0597), tensor(0.0539), tensor(0.0072), tensor(0.0556), tensor(-0.0049), tensor(0.0357), tensor(0.0363), tensor(0.0585), tensor(0.0656), tensor(0.0467), tensor(0.0676), tensor(0.0342), tensor(0.0478), tensor(0.0182), tensor(0.0413), tensor(-0.0153), tensor(0.0644), tensor(0.0185), tensor(0.0060), tensor(0.0430), tensor(0.0361), tensor(0.0845), tensor(0.0582), tensor(-0.0101), tensor(0.0462), tensor(0.0512), tensor(0.0170), tensor(0.0661), tensor(0.0341), tensor(0.0469), tensor(0.0669), tensor(-0.0007), tensor(0.0099), tensor(0.0369), tensor(0.0861), tensor(-0.0020), tensor(0.0669), tensor(0.0534), tensor(0.0459), tensor(0.0373), tensor(0.0185), tensor(0.0299), tensor(0.0257), tensor(0.0444), tensor(0.0512), tensor(0.0546), tensor(0.0299), tensor(0.0323), tensor(0.0391), tensor(0.0061), tensor(-0.0126), tensor(0.0413), tensor(0.0370), tensor(-0.0032), tensor(0.0459), tensor(0.0801), tensor(0.0214), tensor(0.0784), tensor(0.0890), tensor(-0.0030), tensor(-0.0100), tensor(0.0367), tensor(0.0673), tensor(0.0695), tensor(-0.0133), tensor(-0.0313), tensor(0.0349), tensor(0.5447), tensor(0.0287), tensor(0.0065), tensor(-0.0304), tensor(-0.0433), tensor(0.0452), tensor(0.0487), tensor(-0.0065), tensor(-0.0098), tensor(0.0588), tensor(0.0684), tensor(0.0308), tensor(0.0949), tensor(-0.0450), tensor(0.0345), tensor(0.0365), tensor(0.0105), tensor(0.0328), tensor(0.0620), tensor(0.0475), tensor(0.3371), tensor(0.0166), tensor(0.3357), tensor(0.0063), tensor(0.0673), tensor(0.0064), tensor(0.1523), tensor(0.0263), tensor(0.0379), tensor(0.0433), tensor(0.0107), tensor(0.0614), tensor(0.0289), tensor(0.0002), tensor(0.0385), tensor(0.0098), tensor(0.0911), tensor(0.0678), tensor(0.0072), tensor(0.0007), tensor(0.0100), tensor(0.0642), tensor(0.0279), tensor(0.0226), tensor(0.0632), tensor(0.0437), tensor(0.0179), tensor(0.0661), tensor(-0.0181), tensor(0.0176), tensor(-0.0378), tensor(0.0197), tensor(0.1070), tensor(0.0414), tensor(0.0525), tensor(0.0472), tensor(0.2205), tensor(0.0891), tensor(0.0292), tensor(0.0562), tensor(0.0001), tensor(0.0193), tensor(0.0656), tensor(-0.0129), tensor(0.0480), tensor(0.0094), tensor(0.0270), tensor(0.0342), tensor(0.0391), tensor(0.0045), tensor(0.0015), tensor(0.0416), tensor(0.0687), tensor(0.0196), tensor(0.0416), tensor(0.0073), tensor(0.0301), tensor(0.0388), tensor(0.0869), tensor(-0.0096), tensor(0.0570), tensor(0.0105), tensor(-0.0055), tensor(0.0462), tensor(0.0888), tensor(0.0273), tensor(0.0325), tensor(0.0773), tensor(0.0104), tensor(0.0948), tensor(0.0326), tensor(-0.0097), tensor(0.0978), tensor(0.0528), tensor(0.0755), tensor(0.0920), tensor(0.0265), tensor(0.0392), tensor(0.0390), tensor(0.0181), tensor(0.0526), tensor(0.0006), tensor(0.0114), tensor(0.0011), tensor(-0.0033), tensor(-0.0003), tensor(0.0480), tensor(0.0356), tensor(0.0917), tensor(0.0414), tensor(0.0267), tensor(0.3691), tensor(0.0785), tensor(0.0202), tensor(0.1006), tensor(0.0452), tensor(0.0061), tensor(-0.0333), tensor(0.0730), tensor(0.0433), tensor(0.0738), tensor(0.0011), tensor(0.0399), tensor(0.0849), tensor(0.0176), tensor(0.0904), tensor(-0.0219), tensor(0.0139), tensor(0.2186), tensor(0.0088), tensor(0.0221), tensor(-0.0240), tensor(0.0087), tensor(0.0061), tensor(0.0257), tensor(0.0825), tensor(0.0096), tensor(0.0732), tensor(0.0187), tensor(0.0378), tensor(0.1373), tensor(0.0928), tensor(0.0820), tensor(0.0345), tensor(0.0969), tensor(0.0546), tensor(0.0325), tensor(0.0761), tensor(0.0344), tensor(0.1027), tensor(0.0139), tensor(-0.0407), tensor(-0.0100), tensor(0.0951), tensor(0.0730), tensor(0.0685), tensor(-0.0081), tensor(-0.0309), tensor(0.0377), tensor(0.0463), tensor(0.0712), tensor(0.0255), tensor(0.0804), tensor(0.0241), tensor(0.0016), tensor(0.0384), tensor(0.0569), tensor(0.0396), tensor(0.0153), tensor(0.0731), tensor(0.0632), tensor(0.0191), tensor(0.0592), tensor(0.0193), tensor(-0.0045), tensor(0.0476), tensor(0.0603), tensor(-0.0439), tensor(0.0730), tensor(0.0033), tensor(0.0185), tensor(0.0304), tensor(0.0776), tensor(0.0323), tensor(0.0294), tensor(0.0186), tensor(-0.0379), tensor(0.0131), tensor(0.0514), tensor(0.0378), tensor(0.0682), tensor(0.0192), tensor(0.0876), tensor(0.0178), tensor(0.0061), tensor(-0.0069), tensor(0.0229), tensor(-0.0052), tensor(0.0904), tensor(0.0194), tensor(0.0248), tensor(0.0523), tensor(0.0846), tensor(0.1049), tensor(0.0421), tensor(0.0412), tensor(0.0297), tensor(-0.0195), tensor(0.0635), tensor(0.0487), tensor(0.0306), tensor(0.0342), tensor(0.0549), tensor(0.0200), tensor(0.0063), tensor(0.0305), tensor(0.0777), tensor(0.0412), tensor(0.1037), tensor(0.0967), tensor(-0.0155), tensor(0.0260), tensor(0.0859), tensor(0.0153), tensor(0.0863), tensor(0.0503), tensor(0.0392), tensor(0.0046), tensor(0.0344), tensor(0.0937), tensor(0.0764), tensor(0.0920), tensor(0.0554), tensor(-0.0152), tensor(0.0143), tensor(0.0717), tensor(-0.0379), tensor(0.0002), tensor(0.0019), tensor(0.0733), tensor(0.0295), tensor(-0.0029), tensor(0.1074), tensor(0.1093), tensor(0.0431), tensor(-0.0171), tensor(0.0331), tensor(0.0633), tensor(-0.0292), tensor(0.0614), tensor(0.0151), tensor(0.1478), tensor(0.0356), tensor(0.0516), tensor(0.0599), tensor(0.0658), tensor(0.0069), tensor(0.0165), tensor(-0.0682), tensor(0.0424), tensor(0.0415), tensor(0.3394), tensor(0.0185), tensor(0.0655), tensor(0.0692), tensor(-0.0407), tensor(0.0889), tensor(0.0170), tensor(0.0079), tensor(0.0490), tensor(0.0394), tensor(0.0663), tensor(-0.0077), tensor(-0.0032), tensor(-0.0100), tensor(0.0146), tensor(0.0437), tensor(0.1237), tensor(0.0540), tensor(0.0403), tensor(0.0598), tensor(0.3260), tensor(-0.0081), tensor(0.0853), tensor(0.0076), tensor(0.0763), tensor(0.0488), tensor(0.0096), tensor(0.0231), tensor(0.0041), tensor(-0.0136), tensor(0.1250), tensor(-0.0017), tensor(0.0194), tensor(0.0404), tensor(0.0301), tensor(0.0141), tensor(-0.0015), tensor(0.0203), tensor(0.0332), tensor(0.0906), tensor(0.1001), tensor(0.0294), tensor(0.0161), tensor(-0.0088), tensor(0.0744), tensor(0.0708), tensor(0.0682), tensor(0.0227), tensor(0.0118), tensor(0.0110), tensor(0.0557), tensor(0.0103), tensor(0.0762), tensor(0.0183), tensor(0.0209), tensor(0.0853), tensor(0.0567), tensor(0.0281), tensor(0.0025), tensor(0.1313), tensor(0.0135), tensor(-0.0076), tensor(0.0582), tensor(0.0058), tensor(0.0148), tensor(0.0130), tensor(0.0551), tensor(-0.0023), tensor(0.0617), tensor(0.0116), tensor(0.0996), tensor(0.0545), tensor(0.0551), tensor(0.0370), tensor(0.0333), tensor(0.0042), tensor(0.0415), tensor(0.0299), tensor(0.0140), tensor(0.0545), tensor(-0.0442), tensor(0.0497), tensor(0.0277), tensor(0.0241), tensor(-0.0294), tensor(-0.0357), tensor(0.0531), tensor(-0.0207), tensor(0.0365), tensor(0.0926), tensor(0.0345), tensor(0.0272), tensor(-0.0112), tensor(0.4184), tensor(0.0138), tensor(-0.0030), tensor(0.0976), tensor(0.0269), tensor(0.0626), tensor(0.0435), tensor(0.0423), tensor(0.0130), tensor(0.0579), tensor(-0.0058), tensor(0.0668), tensor(0.0830), tensor(0.0207), tensor(0.0095), tensor(0.0399), tensor(0.1568), tensor(0.0612), tensor(0.0332), tensor(-0.0106), tensor(0.0419), tensor(0.0222), tensor(0.0161), tensor(0.1257), tensor(-0.0052), tensor(0.0441), tensor(0.0481), tensor(0.0219), tensor(0.1001), tensor(0.0689), tensor(0.0865), tensor(0.0573), tensor(0.0120), tensor(0.0316), tensor(0.0072), tensor(0.0520), tensor(0.0287), tensor(0.0733), tensor(0.0097), tensor(0.0493), tensor(0.0037), tensor(0.1312), tensor(0.0486), tensor(0.0023), tensor(0.0324), tensor(0.1207), tensor(-0.0207), tensor(0.0907), tensor(0.0731), tensor(0.0526), tensor(0.0395), tensor(0.1178), tensor(0.4088), tensor(0.0390), tensor(0.1201), tensor(0.0203), tensor(0.0497), tensor(0.0032), tensor(0.0341), tensor(0.0165), tensor(-0.0090), tensor(-0.0342), tensor(-0.0039), tensor(0.0172), tensor(0.0040), tensor(0.0626), tensor(0.0593), tensor(0.1018), tensor(-0.0363), tensor(0.0389), tensor(0.0389), tensor(0.0209), tensor(0.0548), tensor(0.0210), tensor(0.0403), tensor(0.0177), tensor(0.0446), tensor(-0.0086), tensor(0.0327), tensor(0.0797), tensor(0.0425), tensor(0.0286), tensor(0.1404), tensor(0.0490), tensor(0.0359), tensor(0.0614), tensor(0.0355), tensor(0.0623), tensor(0.0236), tensor(0.0622), tensor(0.0191), tensor(0.0144), tensor(0.3302), tensor(0.0778), tensor(0.0052), tensor(0.0855), tensor(0.0550), tensor(0.0014), tensor(0.1056), tensor(-0.0134), tensor(-0.0328), tensor(0.0308), tensor(-0.0012), tensor(0.0376), tensor(0.0664), tensor(0.0115), tensor(0.1012), tensor(0.0904), tensor(0.1266), tensor(0.0182), tensor(0.0011), tensor(0.0321), tensor(0.0782), tensor(0.0761), tensor(0.1246), tensor(0.0095), tensor(0.1003), tensor(-0.0389), tensor(0.0036), tensor(0.0273), tensor(0.0304), tensor(0.0658), tensor(0.0464), tensor(0.0240), tensor(-0.0093), tensor(-0.0073), tensor(0.0587), tensor(0.0832), tensor(0.0368), tensor(-0.0120), tensor(0.0863), tensor(0.0325), tensor(0.1595), tensor(0.0281), tensor(0.0195), tensor(0.0162), tensor(0.0234), tensor(0.0025), tensor(0.0724), tensor(-0.0077), tensor(0.0089), tensor(-0.0011), tensor(0.0011), tensor(-0.0175), tensor(0.0003), tensor(0.0469), tensor(0.0020), tensor(0.0589), tensor(0.0173), tensor(0.0329), tensor(0.0128), tensor(-0.0092), tensor(0.0325), tensor(0.0204), tensor(0.0810), tensor(0.0396), tensor(-0.0059), tensor(-0.0173), tensor(0.0141), tensor(0.0351), tensor(0.0020), tensor(0.1361), tensor(0.0299), tensor(0.0462), tensor(0.0792), tensor(0.4270), tensor(0.0308), tensor(-0.0324), tensor(0.0148), tensor(-0.0114), tensor(0.0595), tensor(0.0283), tensor(0.0529), tensor(0.0552), tensor(0.0694), tensor(0.0522), tensor(-0.0233), tensor(-7.5033e-05), tensor(0.0311), tensor(0.0290), tensor(0.0046), tensor(0.0721), tensor(0.0868), tensor(0.0154), tensor(0.0327), tensor(0.0901), tensor(0.0084), tensor(0.0460), tensor(0.3752), tensor(0.1682), tensor(0.0492), tensor(0.0411), tensor(0.0373), tensor(-0.0089), tensor(0.0016), tensor(0.1200), tensor(-0.0029), tensor(0.0714), tensor(0.0252), tensor(0.3732), tensor(0.0096), tensor(0.0706), tensor(0.0349), tensor(0.0822), tensor(0.0202), tensor(0.0431), tensor(0.1214), tensor(0.0341), tensor(0.0276), tensor(0.0889), tensor(0.0028), tensor(0.0739), tensor(0.0296), tensor(0.0258), tensor(0.0269), tensor(0.0209), tensor(-0.0147), tensor(0.1912), tensor(-0.0003), tensor(0.0781), tensor(0.0872), tensor(0.0874), tensor(0.0687), tensor(0.0913), tensor(0.0581), tensor(-0.0090), tensor(0.0676), tensor(0.0127), tensor(0.0401), tensor(0.0684), tensor(0.0500), tensor(0.0703), tensor(0.0136), tensor(0.1738), tensor(0.0669), tensor(4.0431e-05), tensor(0.0244), tensor(0.0633), tensor(-0.0008), tensor(0.0303), tensor(0.0059), tensor(0.0530), tensor(0.0573), tensor(0.0242), tensor(0.0792), tensor(0.0335), tensor(0.0583), tensor(0.0040), tensor(0.0553), tensor(0.0215), tensor(0.0907), tensor(0.0061), tensor(0.0374), tensor(0.0267), tensor(0.0845), tensor(0.0688), tensor(0.0217), tensor(0.0661), tensor(0.0432), tensor(0.0164), tensor(-0.0052), tensor(0.0690), tensor(0.0357), tensor(0.0739), tensor(0.0824), tensor(-0.0419), tensor(0.0260), tensor(0.0184), tensor(0.0922), tensor(0.0893), tensor(0.1907), tensor(0.0614), tensor(0.0479), tensor(0.0320), tensor(0.0097), tensor(-0.0121), tensor(-0.0060), tensor(0.0338), tensor(0.0710), tensor(-0.0025), tensor(0.0862), tensor(0.0323), tensor(0.0352), tensor(0.0212), tensor(0.0717), tensor(0.0690), tensor(0.1393), tensor(0.0552), tensor(0.0250), tensor(0.0303), tensor(0.0082), tensor(0.0922), tensor(0.0717), tensor(0.0938), tensor(0.0705), tensor(0.0107), tensor(0.0276), tensor(0.0332), tensor(0.0660), tensor(-0.0121), tensor(0.0852), tensor(0.0422), tensor(0.0350), tensor(0.0368), tensor(0.2076), tensor(-0.0326), tensor(-0.0423), tensor(0.0538), tensor(0.0293), tensor(0.0570), tensor(0.0516), tensor(0.0651), tensor(0.0111), tensor(0.0852), tensor(0.0686), tensor(0.0143), tensor(0.1362), tensor(0.1573), tensor(0.0290), tensor(0.0316), tensor(-0.0022), tensor(0.0248), tensor(0.0421), tensor(0.0411), tensor(0.1331), tensor(-0.0340), tensor(0.0145), tensor(0.0002), tensor(0.0253), tensor(-0.0051), tensor(0.0625), tensor(0.0484), tensor(0.0137), tensor(0.0081), tensor(0.1416), tensor(0.0121), tensor(0.0422), tensor(0.0643), tensor(0.0742), tensor(0.0134), tensor(0.1608), tensor(0.0529), tensor(-0.0154), tensor(0.0351), tensor(0.0209), tensor(0.0492), tensor(0.0575), tensor(0.0388), tensor(0.0624), tensor(0.0317), tensor(0.0664), tensor(0.0203), tensor(0.0600), tensor(0.0352), tensor(0.0394), tensor(0.0145), tensor(0.0355), tensor(0.0239), tensor(0.0848), tensor(0.0141), tensor(0.0702), tensor(0.0579), tensor(0.0457), tensor(0.0432), tensor(0.0898), tensor(0.0558), tensor(-0.0301), tensor(0.0761), tensor(-0.0111), tensor(0.0252), tensor(0.0219), tensor(0.1028), tensor(0.0756), tensor(0.0596), tensor(0.0268), tensor(0.0341), tensor(0.0857), tensor(0.0725), tensor(0.0221), tensor(0.2153), tensor(0.0139), tensor(0.0099), tensor(0.0131), tensor(0.0349), tensor(0.0442), tensor(0.0525), tensor(0.0445), tensor(0.0103), tensor(0.0406), tensor(0.0042), tensor(0.0552), tensor(0.0249), tensor(0.0075), tensor(0.0246), tensor(0.0004), tensor(0.0145), tensor(0.0261), tensor(0.0298), tensor(-0.0132), tensor(0.3788), tensor(-0.0429), tensor(0.0229), tensor(-0.0302), tensor(0.4364), tensor(0.0350), tensor(0.0528), tensor(0.0151), tensor(0.0854), tensor(-0.0079), tensor(-0.0414), tensor(0.0479), tensor(-0.0164), tensor(0.0062), tensor(0.0501), tensor(0.0272), tensor(0.0197), tensor(0.0402), tensor(0.0388), tensor(0.0003), tensor(0.0144), tensor(0.0607), tensor(0.0867), tensor(-0.0029), tensor(0.0410), tensor(0.0749), tensor(0.0726), tensor(0.0624), tensor(0.0326), tensor(0.0542), tensor(0.0311), tensor(0.0151), tensor(-0.0023), tensor(-0.0185), tensor(0.0739), tensor(-0.0150), tensor(0.0324), tensor(0.0177), tensor(0.0308), tensor(0.0115), tensor(0.0201), tensor(0.0492), tensor(0.0267), tensor(0.1405), tensor(-0.0232), tensor(0.0615), tensor(0.0041), tensor(0.0043), tensor(0.0294), tensor(0.0255), tensor(0.0535), tensor(0.0057), tensor(0.0424), tensor(0.0050), tensor(0.0935), tensor(-0.0044), tensor(0.0389), tensor(0.0360), tensor(0.1169), tensor(0.0356), tensor(0.0748), tensor(0.0153), tensor(0.0792), tensor(0.0243), tensor(0.0634), tensor(0.1650), tensor(0.0194), tensor(0.0839), tensor(0.0828), tensor(0.0404), tensor(0.0360), tensor(0.0259), tensor(0.0063), tensor(0.0341), tensor(0.0172), tensor(0.0674), tensor(0.0812), tensor(0.0829), tensor(0.0008), tensor(0.0387), tensor(0.0645), tensor(0.0358), tensor(0.0126), tensor(0.0344), tensor(-0.0275), tensor(-0.0061), tensor(0.0397), tensor(0.0835), tensor(0.0253), tensor(0.0175), tensor(0.0377), tensor(0.0168), tensor(-0.0278), tensor(0.0367), tensor(-0.0313), tensor(0.0524), tensor(0.0193), tensor(0.0707), tensor(0.0904), tensor(0.1093), tensor(0.0287), tensor(0.0170), tensor(0.0095), tensor(0.0911), tensor(0.0806), tensor(0.0624), tensor(0.0756), tensor(0.0431), tensor(0.0855), tensor(0.0642), tensor(-0.0148), tensor(0.0874), tensor(-0.0031), tensor(0.0132), tensor(0.0319), tensor(0.0157), tensor(0.0617), tensor(0.0556), tensor(0.0168), tensor(0.0758), tensor(0.0332), tensor(-0.0051), tensor(0.0030), tensor(-0.0112), tensor(0.0393), tensor(0.0163), tensor(0.0110), tensor(-0.0009), tensor(0.0633), tensor(0.0275), tensor(0.0372), tensor(0.0224), tensor(0.0822), tensor(0.0423), tensor(0.0219), tensor(0.0186), tensor(-0.0057), tensor(0.0642), tensor(0.0240), tensor(0.0529), tensor(-0.0148), tensor(0.0594), tensor(0.0568), tensor(0.0484), tensor(0.1119), tensor(0.0809), tensor(0.0047), tensor(0.0291), tensor(0.0536), tensor(0.0426), tensor(0.0481), tensor(0.0482), tensor(-0.0194), tensor(-0.0060), tensor(0.0235), tensor(0.0379), tensor(0.0505), tensor(0.0457), tensor(0.0371), tensor(0.0203), tensor(0.0437), tensor(0.0401), tensor(0.0511), tensor(0.0361), tensor(0.0587), tensor(0.0695), tensor(-0.0103), tensor(0.0691), tensor(0.0423), tensor(-0.0340), tensor(0.0870), tensor(-0.0188), tensor(-0.0558), tensor(-0.0025), tensor(0.0412), tensor(0.0689), tensor(0.0692), tensor(0.1469), tensor(0.0873), tensor(0.0145), tensor(0.0816), tensor(0.1822), tensor(-0.0031), tensor(-0.0502), tensor(0.0677), tensor(0.0747), tensor(0.0152), tensor(0.0222), tensor(0.0277), tensor(0.0330), tensor(0.0547), tensor(-0.0018), tensor(0.0241), tensor(0.0275), tensor(0.0269), tensor(0.0169), tensor(0.0136), tensor(0.0091), tensor(0.0167), tensor(-0.0042), tensor(0.0080), tensor(0.0010), tensor(-0.0091), tensor(0.0563), tensor(0.0122), tensor(0.0174), tensor(0.0242), tensor(0.0657), tensor(0.0421), tensor(0.0015), tensor(0.0960), tensor(0.0241), tensor(0.0236), tensor(0.0178), tensor(0.0664), tensor(0.0516), tensor(0.0130), tensor(0.1039), tensor(0.0175), tensor(0.0215), tensor(0.0675), tensor(0.2685), tensor(0.0905), tensor(0.0207), tensor(0.0602), tensor(0.0931), tensor(0.1075), tensor(0.0194), tensor(0.0499), tensor(0.1019), tensor(0.0314), tensor(0.0474), tensor(0.0246), tensor(0.0467), tensor(-0.0222), tensor(-0.0166), tensor(0.1116), tensor(0.0651), tensor(0.0037), tensor(0.0708), tensor(-0.0351), tensor(0.0071), tensor(0.0200), tensor(-0.0045), tensor(-0.0141), tensor(0.0597), tensor(-0.0406), tensor(0.0446), tensor(0.0172), tensor(-0.0363), tensor(0.0510), tensor(0.0321), tensor(0.0200), tensor(0.4541), tensor(0.0212), tensor(0.1057), tensor(0.0160), tensor(0.0355), tensor(0.0081), tensor(0.0514), tensor(0.0526), tensor(0.0051), tensor(0.0303), tensor(0.0087), tensor(-0.0100), tensor(0.0482), tensor(-0.0082), tensor(0.0084), tensor(0.0576), tensor(0.0522), tensor(0.0148), tensor(0.0078), tensor(0.0237), tensor(0.0040), tensor(0.0911), tensor(0.1054), tensor(0.0427), tensor(0.0630), tensor(0.0198), tensor(-0.0226), tensor(0.0017), tensor(0.0801), tensor(0.0693), tensor(0.0143), tensor(0.0337), tensor(0.0218), tensor(0.4416), tensor(0.0349), tensor(0.0205), tensor(0.0186), tensor(0.0654), tensor(-0.0114), tensor(0.0596), tensor(0.0861), tensor(0.0306), tensor(0.0268), tensor(0.0875), tensor(0.3672), tensor(0.0126), tensor(0.0624), tensor(-0.0285), tensor(0.0689), tensor(0.0730), tensor(-0.0328), tensor(0.0470), tensor(0.0259), tensor(0.0435), tensor(0.0495), tensor(0.0160), tensor(0.0325), tensor(0.0710), tensor(0.0041), tensor(0.0142), tensor(0.0296), tensor(-0.0242), tensor(0.0776), tensor(0.0557), tensor(0.0876), tensor(0.2086), tensor(-0.0420), tensor(0.3279), tensor(0.0011), tensor(-0.0778), tensor(0.0533), tensor(0.0490), tensor(0.0603), tensor(-0.0198), tensor(0.0336), tensor(0.0179), tensor(0.0173), tensor(-0.0083), tensor(0.0171), tensor(0.0003), tensor(0.1046), tensor(0.2066), tensor(0.0167), tensor(0.0655), tensor(0.0509), tensor(0.0044), tensor(0.0672), tensor(0.0485), tensor(0.0136), tensor(0.0470), tensor(0.1861), tensor(0.0002), tensor(0.0337), tensor(0.0184), tensor(0.0108), tensor(0.0806), tensor(0.0251), tensor(0.0413), tensor(0.0694), tensor(0.0126), tensor(0.0423), tensor(0.0648), tensor(0.0496), tensor(0.0318), tensor(0.0458), tensor(0.0265), tensor(-0.0395), tensor(0.0195), tensor(0.0202), tensor(0.0010), tensor(0.0470), tensor(0.0196), tensor(0.0203), tensor(-0.0119), tensor(0.0169), tensor(0.0339), tensor(0.0223), tensor(-0.0160), tensor(0.0019), tensor(0.0194), tensor(0.0409), tensor(0.1253), tensor(0.0479), tensor(0.0088), tensor(0.1162), tensor(0.0071), tensor(-0.0087), tensor(0.0283), tensor(0.0358), tensor(0.0199), tensor(0.0663), tensor(0.0381), tensor(0.0221), tensor(0.0334), tensor(0.0504), tensor(0.0152), tensor(0.0682), tensor(0.0688), tensor(0.0349), tensor(0.4176), tensor(0.0206), tensor(0.0056), tensor(-0.0053), tensor(0.0486), tensor(0.0185), tensor(0.0471), tensor(0.0139), tensor(0.0601), tensor(0.0558), tensor(0.0247), tensor(0.0092), tensor(0.0492), tensor(0.0648), tensor(0.0503), tensor(0.3206), tensor(0.0576), tensor(0.2689), tensor(-0.0068), tensor(0.0624), tensor(-0.0015), tensor(0.0887), tensor(0.0483), tensor(0.0212), tensor(0.0351), tensor(0.0167), tensor(0.0276), tensor(0.0197), tensor(0.0263), tensor(-0.0435), tensor(0.0501), tensor(0.0341), tensor(0.1107), tensor(0.0067), tensor(0.0121), tensor(0.0739), tensor(-0.0138), tensor(0.0618), tensor(0.0897), tensor(0.0598), tensor(-0.0193), tensor(0.0658), tensor(0.0638), tensor(0.1158), tensor(-0.0380), tensor(0.0177), tensor(0.3615), tensor(0.0489), tensor(0.0453), tensor(0.0332), tensor(0.0988), tensor(0.0440), tensor(0.0217), tensor(0.0370), tensor(0.0495), tensor(0.0326), tensor(0.0668), tensor(0.0378), tensor(0.0762), tensor(0.0878), tensor(0.3991), tensor(0.0457), tensor(0.0501), tensor(0.0398), tensor(0.0659), tensor(0.0411), tensor(-0.0021), tensor(0.0290), tensor(0.0586), tensor(0.0092), tensor(0.1872), tensor(0.0398), tensor(-0.0100), tensor(0.0453), tensor(0.0127), tensor(0.0029), tensor(0.0031), tensor(0.0622), tensor(0.0430), tensor(0.1529), tensor(0.0742), tensor(0.0442), tensor(0.0469), tensor(0.0123), tensor(0.0815), tensor(0.0262), tensor(-0.0060), tensor(0.1122), tensor(0.0351), tensor(0.0765), tensor(-0.0013), tensor(0.0279), tensor(0.0637), tensor(0.0516), tensor(0.0695), tensor(0.0552), tensor(-0.0122), tensor(0.0054), tensor(0.0663), tensor(0.0359), tensor(0.0097), tensor(0.0512), tensor(0.0536), tensor(0.0309), tensor(0.0732), tensor(0.0217), tensor(0.0154), tensor(0.1109), tensor(0.0667), tensor(0.0408), tensor(0.0442), tensor(0.0177), tensor(1.6970e-05), tensor(0.0298), tensor(0.0368), tensor(-0.0079), tensor(0.0819), tensor(0.0453), tensor(0.0904), tensor(0.0521), tensor(0.0537), tensor(-0.0442), tensor(0.0631), tensor(0.0344), tensor(0.0428), tensor(-0.0218), tensor(0.0485), tensor(0.0198), tensor(0.0557), tensor(0.0285), tensor(0.0179), tensor(0.0634), tensor(0.1250), tensor(0.0137), tensor(0.0569), tensor(0.0380), tensor(0.0356), tensor(0.0265), tensor(0.0496), tensor(0.0732), tensor(0.0436), tensor(0.0021), tensor(0.0296), tensor(0.0294), tensor(0.0182), tensor(0.0852), tensor(-0.0277), tensor(0.0468), tensor(0.0686), tensor(0.0672), tensor(0.1311), tensor(0.0668), tensor(0.0498), tensor(0.3977), tensor(0.0176), tensor(0.2789), tensor(0.0185), tensor(0.1056), tensor(0.0440), tensor(0.0409), tensor(0.0405), tensor(0.0038), tensor(0.4835), tensor(-0.0307), tensor(0.0650), tensor(0.2296), tensor(0.0614), tensor(0.0336), tensor(0.0136), tensor(0.0647), tensor(0.0297), tensor(0.0286), tensor(0.0952), tensor(0.0672), tensor(0.0126), tensor(0.0801), tensor(0.0489), tensor(0.0423), tensor(0.3176), tensor(0.0377), tensor(0.0406), tensor(-0.0197), tensor(0.0422), tensor(0.0141), tensor(0.0206), tensor(0.2233), tensor(0.0576), tensor(0.0035), tensor(0.0251), tensor(0.0785), tensor(0.0995), tensor(0.0919), tensor(0.0192), tensor(0.0445), tensor(0.0035), tensor(0.0579), tensor(0.0524), tensor(0.0454), tensor(0.0319), tensor(-7.2356e-05), tensor(0.0091), tensor(0.0015), tensor(0.0159), tensor(0.0289), tensor(0.1134), tensor(0.0362), tensor(0.0093), tensor(0.0762), tensor(0.0219), tensor(0.1338), tensor(0.0666), tensor(0.0693), tensor(0.0006), tensor(0.1024), tensor(0.0033), tensor(-0.0365), tensor(-0.0948), tensor(0.0663), tensor(0.1057), tensor(-0.0179), tensor(0.0632), tensor(-0.0063), tensor(0.0567), tensor(-0.0303), tensor(0.0397), tensor(0.0048), tensor(0.0211), tensor(0.0855), tensor(0.0670), tensor(0.0963), tensor(0.1057), tensor(0.0020), tensor(-0.0007), tensor(0.0485), tensor(0.0771), tensor(0.0275), tensor(0.0283), tensor(0.0512), tensor(0.0174), tensor(0.0442), tensor(0.0235), tensor(0.0448), tensor(0.1063), tensor(0.0097), tensor(0.1056), tensor(0.0433), tensor(-0.0068), tensor(0.0117), tensor(0.0500), tensor(0.0776), tensor(0.1925), tensor(0.0414), tensor(0.0470), tensor(0.0488), tensor(0.1176), tensor(0.0570), tensor(0.0288), tensor(0.0263), tensor(0.0384), tensor(0.0966), tensor(-0.0054), tensor(0.0765), tensor(0.0694), tensor(0.0965), tensor(0.0029), tensor(-0.0058), tensor(0.0458), tensor(0.0820), tensor(-0.0459), tensor(0.0122), tensor(0.0540), tensor(0.0591), tensor(0.0917), tensor(0.0021), tensor(0.1806), tensor(0.0166), tensor(-0.0216), tensor(0.0578), tensor(0.0269), tensor(0.0380), tensor(0.0412), tensor(0.0020), tensor(0.0608), tensor(0.0242), tensor(0.0015), tensor(0.0447), tensor(0.2597), tensor(0.0258), tensor(0.0046), tensor(0.0667), tensor(0.0017), tensor(0.0252), tensor(0.0871), tensor(0.0398), tensor(0.0086), tensor(0.0627), tensor(0.3981), tensor(0.5292), tensor(0.0381), tensor(0.0421), tensor(0.0173), tensor(0.0625), tensor(0.0802), tensor(0.1043), tensor(0.0482), tensor(0.0129), tensor(0.0952), tensor(0.0179), tensor(0.0383), tensor(0.0473), tensor(0.0325), tensor(0.0316), tensor(0.0285), tensor(0.0400), tensor(0.0464), tensor(-0.0050), tensor(0.0278), tensor(0.3089), tensor(-0.0080), tensor(0.3852), tensor(0.0089), tensor(0.0320), tensor(0.0186), tensor(0.0174), tensor(0.0519), tensor(0.0744), tensor(0.0135), tensor(-0.0006), tensor(0.2949), tensor(0.0119), tensor(0.0133), tensor(-0.0494), tensor(0.0858), tensor(0.0539), tensor(-0.0393), tensor(0.0491), tensor(0.0630), tensor(0.4313), tensor(0.0867), tensor(0.0992), tensor(0.1885), tensor(-0.0020), tensor(-0.0123), tensor(0.0214), tensor(0.0528), tensor(0.0106), tensor(0.0076), tensor(0.0753), tensor(0.0379), tensor(0.0340), tensor(0.0578), tensor(0.0625), tensor(0.0046), tensor(0.0762), tensor(0.0433), tensor(0.0611), tensor(0.0516), tensor(0.0303), tensor(0.0007), tensor(0.0339), tensor(0.1153), tensor(0.0208), tensor(-0.0041), tensor(-0.0429), tensor(0.0914), tensor(0.1188), tensor(-0.0134), tensor(0.1265), tensor(0.0556), tensor(0.0344), tensor(-0.0096), tensor(0.0448), tensor(0.0370), tensor(0.0930), tensor(-0.0477), tensor(0.0332), tensor(0.0025), tensor(-0.0452), tensor(0.1129), tensor(0.0408), tensor(-0.0240), tensor(0.0492), tensor(0.0383), tensor(0.0306), tensor(0.0410), tensor(-0.0183), tensor(0.0021), tensor(0.0106), tensor(0.0403), tensor(0.0345), tensor(0.0344), tensor(0.0615), tensor(0.1060), tensor(0.0736), tensor(-0.0054), tensor(0.0239), tensor(0.1332), tensor(0.0005), tensor(0.0580), tensor(0.0407), tensor(0.0275), tensor(0.0388), tensor(0.0595), tensor(0.0802), tensor(0.1187), tensor(0.0772), tensor(0.0336), tensor(0.0288), tensor(0.0415), tensor(0.0077), tensor(0.0174), tensor(0.0851), tensor(0.0367), tensor(0.0178), tensor(0.0604), tensor(0.0942), tensor(0.0538), tensor(0.0363), tensor(0.0957), tensor(-0.0099), tensor(0.0681), tensor(0.0402), tensor(0.0315), tensor(0.0001), tensor(0.2509), tensor(-0.0264), tensor(0.0347), tensor(0.0374), tensor(0.0019), tensor(0.0715), tensor(0.0760), tensor(0.0511), tensor(0.0334), tensor(0.0757), tensor(0.0157), tensor(0.0215), tensor(0.0242), tensor(0.0808), tensor(0.0092), tensor(0.0333), tensor(0.0240), tensor(0.0595), tensor(0.1667), tensor(0.0507), tensor(0.0901), tensor(0.0001), tensor(0.0443), tensor(0.0362), tensor(-0.0087), tensor(0.0133), tensor(0.0265), tensor(0.0309), tensor(0.0025), tensor(0.0758), tensor(0.0947), tensor(0.0154), tensor(-0.0178), tensor(0.0446), tensor(0.1118), tensor(0.0280), tensor(0.0173), tensor(-0.0049), tensor(0.0251), tensor(0.0616), tensor(0.4360), tensor(0.0370), tensor(0.0205), tensor(-0.0025), tensor(0.0055), tensor(0.0344), tensor(0.0703), tensor(0.0148), tensor(0.0076), tensor(0.0807), tensor(0.0281), tensor(0.0468), tensor(0.0932), tensor(-0.0063), tensor(0.0285), tensor(0.0531), tensor(0.0347), tensor(0.0663), tensor(0.0568), tensor(-0.0463), tensor(0.0839), tensor(0.0627), tensor(0.0059), tensor(0.0193), tensor(0.0443), tensor(-0.0482), tensor(0.1433), tensor(0.0282), tensor(0.0396), tensor(-0.0185), tensor(0.0395), tensor(0.0098), tensor(0.0213), tensor(0.1256), tensor(0.0372), tensor(0.0269), tensor(0.0597), tensor(0.1505), tensor(0.0923), tensor(0.0258), tensor(0.0677), tensor(0.0421), tensor(-0.0163), tensor(0.0537), tensor(0.0262), tensor(0.0583), tensor(-0.0087), tensor(0.0134), tensor(0.0493), tensor(-0.0329), tensor(0.0348), tensor(0.0244), tensor(0.0184), tensor(0.2512), tensor(-0.0207), tensor(0.0800), tensor(0.0529), tensor(0.0644), tensor(-0.0080), tensor(0.1174), tensor(0.0420), tensor(0.0168), tensor(0.0337), tensor(0.1231), tensor(0.0361), tensor(0.0464), tensor(-0.0268), tensor(0.0349), tensor(0.0289), tensor(-0.0241), tensor(0.0078), tensor(0.0045), tensor(0.0176), tensor(0.0606), tensor(0.0384), tensor(0.0787), tensor(0.0761), tensor(0.0649), tensor(-0.0029), tensor(0.0521), tensor(0.0676), tensor(0.0034), tensor(0.0646), tensor(0.0152), tensor(0.1189), tensor(0.0860), tensor(0.0415), tensor(0.0547), tensor(0.0788), tensor(0.0278), tensor(-0.0298), tensor(0.0272), tensor(0.0109), tensor(0.0953), tensor(0.0314), tensor(0.3812), tensor(-0.0074), tensor(0.0762), tensor(0.0860), tensor(0.0484), tensor(0.0106), tensor(0.1098), tensor(0.0010), tensor(0.0419), tensor(0.0721), tensor(0.0151), tensor(0.0735), tensor(0.0115), tensor(0.0097), tensor(0.0410), tensor(0.0493), tensor(-0.0091), tensor(-0.0041), tensor(0.0575), tensor(0.0237), tensor(0.0810), tensor(0.0176), tensor(0.0679), tensor(0.1185), tensor(0.0430), tensor(0.0595), tensor(-0.0294), tensor(0.0631), tensor(0.1056), tensor(0.0649), tensor(0.0299), tensor(0.0628), tensor(-0.0189), tensor(0.0225), tensor(0.0345), tensor(0.1050), tensor(0.0357), tensor(0.1141), tensor(0.0174), tensor(0.0479), tensor(0.0331), tensor(0.0142), tensor(0.1367), tensor(0.0752), tensor(0.0202), tensor(-0.0174), tensor(0.0609), tensor(0.0749), tensor(-0.0154), tensor(0.0833), tensor(0.0094), tensor(0.0434), tensor(0.0293), tensor(0.0111), tensor(0.0391), tensor(0.0169), tensor(0.0043), tensor(0.0386), tensor(0.0389), tensor(0.0603), tensor(0.0172), tensor(0.0565), tensor(0.0715), tensor(0.0202), tensor(-0.0085), tensor(0.1006), tensor(0.1582), tensor(0.0628), tensor(0.0765), tensor(0.0006), tensor(0.0819), tensor(0.0775), tensor(0.0255), tensor(0.0055), tensor(0.0348), tensor(0.0504), tensor(0.0306), tensor(-0.0128), tensor(0.0343), tensor(-0.0146), tensor(0.0516), tensor(0.0403), tensor(0.0444), tensor(0.0171), tensor(-0.0079), tensor(-0.0417), tensor(0.0397), tensor(-0.0206), tensor(0.0005), tensor(0.0969), tensor(0.0778), tensor(0.0688), tensor(0.1158), tensor(-0.0272), tensor(0.0262), tensor(0.0518), tensor(0.0765), tensor(0.0319), tensor(-0.0332), tensor(0.0008), tensor(0.0465), tensor(0.0790), tensor(0.0294), tensor(0.0597), tensor(-0.0315), tensor(0.1103), tensor(0.1288), tensor(-0.0218), tensor(0.0207), tensor(0.0080), tensor(0.0747), tensor(0.0138), tensor(0.0205), tensor(0.0112), tensor(0.3410), tensor(0.0583), tensor(0.0604), tensor(0.0491), tensor(-0.0141), tensor(0.0048), tensor(0.0034), tensor(0.0264), tensor(0.0104), tensor(0.0626), tensor(0.0203), tensor(0.0754), tensor(0.0186), tensor(0.0126), tensor(0.0569), tensor(0.0203), tensor(0.1759), tensor(0.0326), tensor(-0.0116), tensor(0.1326), tensor(-0.0093), tensor(0.0576), tensor(0.0320), tensor(0.0693), tensor(0.0733), tensor(0.0526), tensor(0.0809), tensor(0.0900), tensor(0.0424), tensor(0.0503), tensor(0.0075), tensor(0.0136), tensor(0.1417), tensor(0.0579), tensor(0.0484), tensor(-0.0063), tensor(0.0832), tensor(0.1303), tensor(0.0285), tensor(0.0014), tensor(0.0407), tensor(0.0372), tensor(0.1216), tensor(0.0530), tensor(0.0106), tensor(0.0198), tensor(0.0157), tensor(-0.0184), tensor(-0.0104), tensor(0.0024), tensor(0.0153), tensor(0.0279), tensor(0.1172), tensor(0.0134), tensor(0.0013), tensor(0.0326), tensor(0.0167), tensor(-0.0048), tensor(0.0090), tensor(0.0165), tensor(0.0515), tensor(0.0731), tensor(0.0310), tensor(0.0409), tensor(0.0415), tensor(0.0153), tensor(0.0597), tensor(0.0070), tensor(0.0738), tensor(0.0678), tensor(0.0017), tensor(0.0019), tensor(0.0431), tensor(0.1157), tensor(0.0145), tensor(0.0267), tensor(0.0393), tensor(0.0300), tensor(0.1215), tensor(-0.0088), tensor(0.1167), tensor(0.0383), tensor(0.0898), tensor(0.0413), tensor(-0.0412), tensor(0.0269), tensor(-0.0298), tensor(0.0649), tensor(-0.0175), tensor(0.0084), tensor(0.0320), tensor(0.0081), tensor(0.0551), tensor(0.0536), tensor(0.0274), tensor(-0.0388), tensor(0.0682), tensor(0.0119), tensor(0.0375), tensor(0.0263), tensor(0.0916), tensor(0.0169), tensor(0.0498), tensor(0.0378), tensor(0.0425), tensor(0.0443), tensor(-0.0589), tensor(0.0520), tensor(-0.0041), tensor(0.0358), tensor(0.0160), tensor(0.0827), tensor(0.0957), tensor(0.0592), tensor(-0.0266), tensor(0.1384), tensor(-0.0307), tensor(0.0960), tensor(-0.0037), tensor(0.1126), tensor(0.0590), tensor(0.0286), tensor(0.0156), tensor(-0.0145), tensor(0.0683), tensor(0.0140), tensor(0.0584), tensor(0.0480), tensor(0.0560), tensor(-0.0150), tensor(0.0879), tensor(0.0724), tensor(0.0485), tensor(0.0500), tensor(0.0268), tensor(0.0332), tensor(0.0066), tensor(0.1382), tensor(0.0633), tensor(-0.0127), tensor(0.0192), tensor(0.0348), tensor(0.0161), tensor(0.1078), tensor(0.0152), tensor(-0.0025), tensor(0.0608), tensor(0.0084), tensor(-0.0148), tensor(-0.0021), tensor(0.0123), tensor(0.0009), tensor(0.0522), tensor(0.1383), tensor(0.0442), tensor(0.0544), tensor(0.0529), tensor(0.0502), tensor(0.0266), tensor(0.0969), tensor(0.0145), tensor(0.0635), tensor(0.0576), tensor(0.1814), tensor(0.0986), tensor(0.0791), tensor(0.0205), tensor(0.0412), tensor(-0.0109), tensor(0.0640), tensor(0.0064), tensor(0.0182), tensor(0.0717), tensor(0.0674), tensor(0.0360), tensor(-0.0015), tensor(-0.0076), tensor(-0.0031), tensor(0.0125), tensor(0.0200), tensor(0.0848), tensor(0.0630), tensor(0.0222), tensor(0.0525), tensor(-0.0019), tensor(0.0417), tensor(0.0337), tensor(0.0084), tensor(0.0998), tensor(-0.0099), tensor(0.0165), tensor(0.0714), tensor(0.0132), tensor(0.0007), tensor(0.0212), tensor(0.0736), tensor(0.0165), tensor(0.0184), tensor(0.0728), tensor(0.0806), tensor(0.0061), tensor(-0.0062), tensor(0.0091), tensor(0.0144), tensor(-0.0518), tensor(0.0206), tensor(0.0226), tensor(0.0398), tensor(0.0727), tensor(-0.0052), tensor(0.0448), tensor(-0.0080), tensor(0.0620), tensor(-0.0329), tensor(0.0075), tensor(0.0947), tensor(0.0282), tensor(0.1143), tensor(0.0072), tensor(0.0399), tensor(-0.0170), tensor(0.0079), tensor(-0.0307), tensor(0.0333), tensor(0.0499), tensor(0.1094), tensor(0.0798), tensor(0.0431), tensor(0.0200), tensor(0.0194), tensor(0.1991), tensor(0.0011), tensor(-0.0104), tensor(0.0301), tensor(0.1207), tensor(0.0462), tensor(0.1315), tensor(0.0119), tensor(0.0514), tensor(0.1413), tensor(0.0343), tensor(0.0803), tensor(-0.0120), tensor(0.0461), tensor(0.0363), tensor(0.0707), tensor(0.0926), tensor(0.0483), tensor(0.0654), tensor(-0.0061), tensor(0.0496), tensor(0.0099), tensor(0.0827), tensor(0.0553), tensor(0.0484), tensor(0.1031), tensor(0.0690), tensor(0.0057), tensor(0.0731), tensor(0.2351), tensor(0.0424), tensor(0.0460), tensor(0.0078), tensor(0.0327), tensor(0.0761), tensor(-0.0027), tensor(-0.0084), tensor(0.0023), tensor(0.0020), tensor(0.0135), tensor(0.0236), tensor(0.0155), tensor(0.0106), tensor(0.0868), tensor(0.0988), tensor(0.0555), tensor(0.0627), tensor(0.0480), tensor(0.0671), tensor(0.0473), tensor(0.0836), tensor(0.0061), tensor(-0.0244), tensor(0.0230), tensor(0.0841), tensor(0.0303), tensor(-0.0395), tensor(0.0333), tensor(0.0378), tensor(0.0774), tensor(0.0375), tensor(0.0995), tensor(0.0287), tensor(0.0947), tensor(0.0136), tensor(0.0078), tensor(0.2726), tensor(0.1114), tensor(0.0997), tensor(0.1180), tensor(0.3184), tensor(0.0662), tensor(0.1054), tensor(-0.0086), tensor(0.0074), tensor(0.0055), tensor(-0.0032), tensor(0.0870), tensor(0.0627), tensor(0.0154), tensor(0.0097), tensor(0.0425), tensor(-0.0389), tensor(0.0447), tensor(0.0744), tensor(0.0185), tensor(0.0950), tensor(0.0369), tensor(-0.0134), tensor(0.1202), tensor(0.0090), tensor(0.0132), tensor(0.0126), tensor(0.0216), tensor(0.0148), tensor(0.0408), tensor(0.0373), tensor(0.0599), tensor(0.0813), tensor(0.0094), tensor(-0.0048), tensor(0.0172), tensor(0.0441), tensor(0.1128), tensor(0.0245), tensor(0.0225), tensor(-0.0052), tensor(0.0736), tensor(0.0937), tensor(0.0212), tensor(0.0287), tensor(0.0345), tensor(0.1243), tensor(0.0716), tensor(0.0415), tensor(0.0435), tensor(0.0586), tensor(0.0958), tensor(-0.0018), tensor(0.0200), tensor(0.0410), tensor(0.0220), tensor(0.0480), tensor(0.0220), tensor(0.0696), tensor(0.0720), tensor(0.0182), tensor(0.0074), tensor(0.0275), tensor(0.0143), tensor(0.1431), tensor(0.0391), tensor(0.0229), tensor(0.0619), tensor(0.0464), tensor(0.0619), tensor(0.1386), tensor(0.0287), tensor(-0.0147), tensor(0.0629), tensor(0.0540), tensor(0.0175), tensor(0.0516), tensor(0.0732), tensor(0.0726), tensor(-0.0002), tensor(0.0455), tensor(0.1427), tensor(0.0687), tensor(-0.0309), tensor(-0.0037), tensor(0.0932), tensor(0.0052), tensor(-0.0107), tensor(0.0427), tensor(0.0203), tensor(0.0527), tensor(0.0419), tensor(0.0388), tensor(0.0610), tensor(0.0454), tensor(0.0248), tensor(0.0194), tensor(0.0245), tensor(0.0528), tensor(0.1207), tensor(0.0459), tensor(0.0520), tensor(0.0089), tensor(0.0750), tensor(0.0192), tensor(0.3679), tensor(0.0018), tensor(0.2870), tensor(0.0370), tensor(0.0324), tensor(0.0110), tensor(-0.0223), tensor(0.0253), tensor(0.0437), tensor(0.0291), tensor(0.0829), tensor(0.0363), tensor(0.4565), tensor(0.0305), tensor(0.0452), tensor(0.0240), tensor(0.0478), tensor(-0.0494), tensor(0.0398), tensor(0.0237), tensor(0.0678), tensor(0.0266), tensor(0.0675), tensor(-0.0154), tensor(0.0624), tensor(0.0161), tensor(0.0473), tensor(0.0860), tensor(0.1425), tensor(0.0054), tensor(0.0380), tensor(0.0040), tensor(0.0263), tensor(0.1137), tensor(-0.0835), tensor(-0.0129), tensor(-0.0026), tensor(0.0119), tensor(0.0311), tensor(0.0214), tensor(0.0750), tensor(0.0611), tensor(0.0453), tensor(0.0532), tensor(0.0273), tensor(0.0952), tensor(0.0880), tensor(-0.0106), tensor(0.0379), tensor(0.0475), tensor(0.0129), tensor(0.0646), tensor(0.0379), tensor(0.0471), tensor(0.0084), tensor(0.0548), tensor(0.0397), tensor(-0.0105), tensor(0.0234), tensor(0.0136), tensor(0.0329), tensor(-0.0297), tensor(0.0266), tensor(0.0908), tensor(0.0268), tensor(0.0378), tensor(0.0654), tensor(-0.0059), tensor(0.0029), tensor(-0.0171), tensor(-0.0046), tensor(0.0275), tensor(0.0303), tensor(-0.0015), tensor(0.4271), tensor(0.0113), tensor(0.0542), tensor(0.0107), tensor(-0.0151), tensor(-0.0115), tensor(0.0750), tensor(-0.0044), tensor(0.0183), tensor(0.0920), tensor(0.0203), tensor(-0.0083), tensor(-0.0344), tensor(0.0132), tensor(0.0214), tensor(0.0587), tensor(0.0778), tensor(-0.0477), tensor(-0.0134), tensor(0.0329), tensor(0.0322), tensor(0.0274), tensor(0.0534), tensor(-0.0170), tensor(0.0665), tensor(0.0567), tensor(0.0496), tensor(0.0324), tensor(0.0310), tensor(0.0539), tensor(0.0649), tensor(0.0095), tensor(0.0020), tensor(0.0256), tensor(0.0164), tensor(0.0023), tensor(0.0379), tensor(0.0834), tensor(-0.0005), tensor(0.0222), tensor(0.0210), tensor(0.0431), tensor(0.0142), tensor(0.0646), tensor(-0.0074), tensor(0.0153), tensor(0.0696), tensor(-0.0172), tensor(-0.0136), tensor(0.0249), tensor(0.0506), tensor(0.0605), tensor(0.0579), tensor(0.0250), tensor(0.0319), tensor(-0.0262), tensor(0.0701), tensor(0.0066), tensor(0.0053), tensor(0.0185), tensor(0.0642), tensor(0.0927), tensor(0.0085), tensor(0.0456), tensor(0.0146), tensor(0.0160), tensor(0.0496), tensor(-0.0149), tensor(0.0693), tensor(0.0007), tensor(0.0587), tensor(0.0733), tensor(0.0325), tensor(0.0426), tensor(0.0038), tensor(0.0727), tensor(0.0224), tensor(0.0563), tensor(0.0340), tensor(0.0142), tensor(0.0139), tensor(0.0462), tensor(0.0035), tensor(0.0591), tensor(0.0347), tensor(0.0466), tensor(0.0138), tensor(0.1030), tensor(0.0269), tensor(0.0402), tensor(0.0589), tensor(0.0350), tensor(0.0359), tensor(-0.0041), tensor(0.0408), tensor(0.0834), tensor(0.0525), tensor(0.0419), tensor(0.0386), tensor(0.0860), tensor(0.0186), tensor(0.0380), tensor(0.0426), tensor(0.0862), tensor(0.0077), tensor(-0.0107), tensor(0.0419), tensor(0.0612), tensor(-0.0116), tensor(0.0151), tensor(-0.0374), tensor(0.0233), tensor(0.0396), tensor(0.0399), tensor(0.1411), tensor(0.0413), tensor(-0.0198), tensor(0.0511), tensor(0.0704), tensor(0.0239), tensor(-0.0345), tensor(-0.0465), tensor(0.0894), tensor(0.0307), tensor(0.0587), tensor(0.0517), tensor(0.1161), tensor(0.0510), tensor(0.0991), tensor(0.1192), tensor(0.0584), tensor(0.0262), tensor(0.0310), tensor(0.0007), tensor(0.0448), tensor(-0.0163), tensor(0.0362), tensor(0.0967), tensor(0.0602), tensor(0.0303), tensor(0.0300), tensor(0.0285), tensor(0.0188), tensor(0.0554), tensor(0.0759), tensor(0.0365), tensor(0.0177), tensor(0.0470), tensor(0.0354), tensor(0.0965), tensor(-0.0023), tensor(0.0839), tensor(0.0372), tensor(0.0414), tensor(0.0751), tensor(-0.0208), tensor(0.1624), tensor(0.0475), tensor(0.0732), tensor(0.0086), tensor(0.0141), tensor(0.0628), tensor(0.0360), tensor(0.0227), tensor(0.0227), tensor(0.2701), tensor(0.0310), tensor(0.0315), tensor(0.0083), tensor(0.0230), tensor(0.0293), tensor(0.0328), tensor(0.0860), tensor(0.0409), tensor(0.0431), tensor(0.1112), tensor(0.0407), tensor(0.0515), tensor(0.0469), tensor(0.0909), tensor(0.0064), tensor(-0.0610), tensor(0.0550), tensor(0.0363), tensor(0.0196), tensor(0.0092), tensor(0.0409), tensor(0.0208), tensor(-0.0505), tensor(0.0277), tensor(-0.0092), tensor(0.0033), tensor(-0.0179), tensor(0.0069), tensor(-0.0243), tensor(0.0576), tensor(0.0730), tensor(0.0418), tensor(0.0447), tensor(0.0269), tensor(0.0230), tensor(-0.0003), tensor(0.0382), tensor(0.0340), tensor(0.0562), tensor(0.0517), tensor(-0.0110), tensor(0.1069), tensor(-0.0086), tensor(0.1719), tensor(0.0524), tensor(0.0337), tensor(0.1134), tensor(0.0120), tensor(0.0231), tensor(0.0227), tensor(0.0100), tensor(0.0228), tensor(0.0575), tensor(0.0968), tensor(-0.0033), tensor(0.0206), tensor(0.0158), tensor(0.0064), tensor(0.1091), tensor(-0.0142), tensor(0.0530), tensor(0.0543), tensor(0.0232), tensor(0.0143), tensor(0.0921), tensor(0.0210), tensor(0.0864), tensor(0.0656), tensor(0.0841), tensor(-0.0009), tensor(0.0876), tensor(0.0691), tensor(0.0065), tensor(0.0868), tensor(0.0602), tensor(0.0897), tensor(0.2180), tensor(0.0262), tensor(0.1577), tensor(-0.0679), tensor(0.0142), tensor(0.0938), tensor(0.0476), tensor(0.0521), tensor(0.0580), tensor(-0.0304), tensor(0.0343), tensor(0.0137), tensor(0.0580), tensor(0.0263), tensor(0.0769), tensor(0.0392), tensor(0.0243), tensor(0.0652), tensor(0.0047), tensor(-0.0124), tensor(-0.0152), tensor(-0.0010), tensor(0.0145), tensor(-0.0097), tensor(-0.0211), tensor(0.0629), tensor(0.0024), tensor(0.0433), tensor(-0.0075), tensor(0.0388), tensor(0.0519), tensor(-0.0046), tensor(0.0504), tensor(0.0257), tensor(0.0555), tensor(0.1559), tensor(0.0586), tensor(0.0695), tensor(0.0155), tensor(0.0136), tensor(0.0400), tensor(0.0953), tensor(-0.0259), tensor(-0.0246), tensor(-8.5491e-05), tensor(0.0261), tensor(0.0307), tensor(0.0653), tensor(0.0505), tensor(0.0641), tensor(-0.0077), tensor(0.0168), tensor(0.0144), tensor(0.1021), tensor(0.4007), tensor(0.0621), tensor(0.0212), tensor(0.0288), tensor(0.0689), tensor(0.0152), tensor(0.0048), tensor(0.0728), tensor(0.0774), tensor(0.0661), tensor(0.0405), tensor(-0.0248), tensor(0.0420), tensor(0.0244), tensor(0.0225), tensor(0.0643), tensor(0.0895), tensor(0.0309), tensor(0.0458), tensor(0.0256), tensor(0.0937), tensor(0.0192), tensor(0.0383), tensor(0.0214), tensor(0.0125), tensor(0.0223), tensor(0.1240), tensor(-0.0140), tensor(0.0406), tensor(-0.0068), tensor(0.0019), tensor(-0.0023), tensor(0.0201), tensor(-0.0076), tensor(0.0119), tensor(0.0071), tensor(0.0270), tensor(0.0930), tensor(0.0321), tensor(0.0257), tensor(0.0313), tensor(0.1136), tensor(0.0009), tensor(0.0272), tensor(0.0165), tensor(0.0002), tensor(0.0555), tensor(0.0214), tensor(0.5897), tensor(0.0008), tensor(0.0645), tensor(0.0023), tensor(0.0540), tensor(0.0421), tensor(0.0356), tensor(0.0689), tensor(0.0159), tensor(0.1386), tensor(0.0206), tensor(0.0534), tensor(0.1079), tensor(0.0192), tensor(0.0271), tensor(0.0190), tensor(0.0378), tensor(0.0586), tensor(0.0582), tensor(0.0135), tensor(0.0427), tensor(0.1092), tensor(0.0681), tensor(-0.0325), tensor(0.0301), tensor(0.0279), tensor(0.0689), tensor(0.0101), tensor(0.0508), tensor(0.0675), tensor(0.0266), tensor(0.0579), tensor(0.0582), tensor(0.0513), tensor(0.1107), tensor(0.0346), tensor(-0.0155), tensor(0.0276), tensor(0.0128), tensor(0.0600), tensor(0.0669), tensor(0.0280), tensor(-0.0472), tensor(0.0602), tensor(0.0195), tensor(0.0233), tensor(0.3225), tensor(0.0822), tensor(0.0103), tensor(-0.0214), tensor(0.0113), tensor(0.0137), tensor(0.0558), tensor(0.4902), tensor(-0.0209), tensor(0.0356), tensor(0.0399), tensor(0.0834), tensor(0.0304), tensor(0.0144), tensor(-0.0022), tensor(0.0114), tensor(-0.0266), tensor(0.0008), tensor(0.0108), tensor(-0.0171), tensor(0.0365), tensor(0.0818), tensor(0.4011), tensor(0.0420), tensor(0.0656), tensor(0.0494), tensor(0.0029), tensor(0.0602), tensor(0.0591), tensor(0.0664), tensor(0.0698), tensor(-0.0116), tensor(-0.0213), tensor(0.0675), tensor(0.0057), tensor(-0.0088), tensor(0.0825), tensor(-0.0110), tensor(0.0646), tensor(0.0002), tensor(-0.0031), tensor(0.0552), tensor(0.0617), tensor(-0.0035), tensor(0.0079), tensor(0.1612), tensor(0.0174), tensor(0.0444), tensor(0.0269), tensor(0.0025), tensor(0.0035), tensor(0.0270), tensor(0.0477), tensor(0.0119), tensor(0.0300), tensor(0.0248), tensor(0.1346), tensor(0.0511), tensor(0.0776), tensor(0.1240), tensor(0.0808), tensor(0.1032), tensor(0.0796), tensor(0.0282), tensor(0.0532), tensor(0.0874), tensor(0.0454), tensor(0.0756), tensor(-0.0170), tensor(0.0507), tensor(0.0086), tensor(-0.0124), tensor(0.0036), tensor(0.0890), tensor(0.0107), tensor(0.0611), tensor(-0.0027), tensor(-0.0095), tensor(-0.0017), tensor(0.0111), tensor(-0.0104), tensor(0.0550), tensor(0.0025), tensor(0.0088), tensor(0.0410), tensor(-0.0067), tensor(-0.0461), tensor(0.0410), tensor(0.1243), tensor(0.0744), tensor(0.0315), tensor(0.0565), tensor(0.0615), tensor(0.0406), tensor(0.0281), tensor(0.3451), tensor(0.0173), tensor(0.0224), tensor(0.0590), tensor(0.0674), tensor(-0.0468), tensor(0.0969), tensor(0.0125), tensor(0.0779), tensor(0.0007), tensor(0.0016), tensor(-0.0054), tensor(-0.0030), tensor(0.4024), tensor(0.0600), tensor(0.0631), tensor(-0.0080), tensor(0.0224), tensor(0.0446), tensor(0.0371), tensor(0.0434), tensor(0.0202), tensor(0.0173), tensor(0.0172), tensor(0.1028), tensor(0.0287), tensor(0.0427), tensor(0.0031), tensor(0.0749), tensor(0.0566), tensor(-0.0049), tensor(-0.0005), tensor(0.0490), tensor(0.0335), tensor(0.0024), tensor(0.0113), tensor(0.0354), tensor(0.0461), tensor(0.0540), tensor(-0.0143), tensor(0.0486), tensor(0.3880), tensor(0.5089), tensor(-0.0108), tensor(0.0428), tensor(0.0190), tensor(0.0546), tensor(0.0529), tensor(-0.0051), tensor(0.0154), tensor(0.0262), tensor(0.0614), tensor(0.0450), tensor(0.0812), tensor(0.0739), tensor(0.0582), tensor(0.0756), tensor(0.0337), tensor(-0.0364), tensor(0.5502), tensor(0.0404), tensor(0.0424), tensor(0.0309), tensor(0.0211), tensor(0.0833), tensor(0.0506), tensor(0.0394), tensor(0.0129), tensor(0.0697), tensor(-0.0228), tensor(0.0904), tensor(0.0916), tensor(0.0659), tensor(0.0675), tensor(0.0636), tensor(0.0076), tensor(0.0386), tensor(0.1026), tensor(0.0322), tensor(0.0355), tensor(0.0244), tensor(0.0567), tensor(0.0040), tensor(0.0495), tensor(-0.0052), tensor(0.0193), tensor(0.0557), tensor(0.0383), tensor(0.0219), tensor(-0.0074), tensor(0.0512), tensor(0.0838), tensor(-0.0097), tensor(0.0995), tensor(0.0414), tensor(0.0269), tensor(0.0973), tensor(0.0464), tensor(-0.0028), tensor(0.0176), tensor(-0.0072), tensor(-0.0233), tensor(0.1078), tensor(0.0731), tensor(0.0531), tensor(0.0434), tensor(0.0218), tensor(-0.0261), tensor(0.1656), tensor(0.0196), tensor(0.5382), tensor(0.0058), tensor(0.0550), tensor(0.0854), tensor(0.0187), tensor(-0.0222), tensor(0.1481), tensor(-0.0076), tensor(-0.0113), tensor(0.0555), tensor(0.4170), tensor(0.0666), tensor(-0.0124), tensor(0.0406), tensor(-0.0291), tensor(0.0700), tensor(0.0954), tensor(0.0376), tensor(0.0164), tensor(0.0634), tensor(-0.0114), tensor(0.0318), tensor(0.0582), tensor(0.0351), tensor(0.0375), tensor(0.0313), tensor(0.0502), tensor(0.0188), tensor(0.0158), tensor(0.0074), tensor(0.0152), tensor(0.0187), tensor(0.0496), tensor(0.0150), tensor(-0.0172), tensor(0.0283), tensor(0.0760), tensor(0.3795), tensor(-0.0003), tensor(0.0200), tensor(0.0584), tensor(0.0353), tensor(0.0573), tensor(0.0802), tensor(0.0156), tensor(0.0340), tensor(0.0036), tensor(0.0299), tensor(0.0431), tensor(0.8760), tensor(0.0236), tensor(0.0189), tensor(0.0709), tensor(0.0591), tensor(0.0332), tensor(-0.0356), tensor(0.3037), tensor(0.0734), tensor(0.0078), tensor(0.0208), tensor(0.0981), tensor(0.0440), tensor(0.0079), tensor(0.0619), tensor(0.0044), tensor(0.0738), tensor(-0.0384), tensor(0.0458), tensor(0.0418), tensor(0.0685), tensor(0.0792), tensor(0.0479), tensor(0.0313), tensor(0.0929), tensor(-0.0264), tensor(0.0610), tensor(0.0641), tensor(0.0133), tensor(0.0541), tensor(0.0390), tensor(0.0249), tensor(0.0943), tensor(-0.0153), tensor(0.1846), tensor(-0.0310), tensor(0.0748), tensor(0.0653), tensor(-0.0062), tensor(0.0476), tensor(0.0188), tensor(-0.0127), tensor(0.0335), tensor(-0.0073), tensor(0.0821), tensor(-0.0048), tensor(0.0821), tensor(0.0519), tensor(0.0021), tensor(0.0870), tensor(0.0442), tensor(0.1395), tensor(0.0070), tensor(0.0597), tensor(-0.0007), tensor(0.0930), tensor(0.0990), tensor(0.0533), tensor(0.0902), tensor(0.0375), tensor(0.0050), tensor(0.0530), tensor(0.0339), tensor(0.0689), tensor(0.0287), tensor(-0.0182), tensor(0.1943), tensor(0.0172), tensor(0.0085), tensor(0.0940), tensor(0.0416), tensor(0.6659), tensor(0.0578), tensor(0.0740), tensor(0.0253), tensor(0.0234), tensor(5.8577e-05), tensor(0.0296), tensor(0.0319), tensor(0.0359), tensor(0.0215), tensor(0.0324), tensor(0.0134), tensor(-0.0239), tensor(0.0583), tensor(0.0590), tensor(0.6019), tensor(0.0182), tensor(0.0244), tensor(-0.0048), tensor(0.0963), tensor(0.0209), tensor(-0.0045), tensor(0.0702), tensor(0.0089), tensor(-0.0130), tensor(0.0546), tensor(0.1458), tensor(0.0183), tensor(0.0892), tensor(0.0138), tensor(0.0288), tensor(0.0300), tensor(0.0122), tensor(0.0831), tensor(0.0329), tensor(0.0112), tensor(0.0818), tensor(-0.0008), tensor(0.0052), tensor(0.0197), tensor(0.0178), tensor(0.0072), tensor(0.0167), tensor(0.0194), tensor(0.0400), tensor(0.0024), tensor(0.0271), tensor(-0.0019), tensor(0.0344), tensor(-0.0289), tensor(0.0364), tensor(0.0905), tensor(0.0657), tensor(0.0193), tensor(0.1065), tensor(0.0137), tensor(0.5610), tensor(0.0831), tensor(0.0757), tensor(0.0460), tensor(-0.0302), tensor(0.0397), tensor(0.0816), tensor(0.0432), tensor(0.0200), tensor(0.0564), tensor(0.0092), tensor(0.0425), tensor(0.0235), tensor(0.0391), tensor(0.0429), tensor(0.0348), tensor(0.0405), tensor(0.0590), tensor(0.0028), tensor(0.0344), tensor(-0.0116), tensor(0.1527), tensor(0.0315), tensor(0.0381), tensor(0.1552), tensor(0.0143), tensor(0.0244), tensor(0.2240), tensor(0.0979), tensor(-0.0037), tensor(0.0593), tensor(0.0353), tensor(0.0102), tensor(0.0426), tensor(0.0182), tensor(0.0331), tensor(0.0186)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdd3gV1dbA4d9KQhJ6CUWkI0gREDRSlSpFQLAjKl75UCkiCF4QBYQLFlRE6UVF9HIVrwXlAqKAKCBFOtJBQAi9hpqQ5Kzvj3NII+VAckqS9T4PDzN79sysMykrs/fM3qKqGGOMMakJ8HUAxhhj/JslCmOMMWmyRGGMMSZNliiMMcakyRKFMcaYNFmiMMYYkyZLFMa4QUQaichuEbkgIg948DzlRURFJMhT5zDmelmiMFmKiOwXkcuuX9hHRWSGiORLVqehiPwiIudFJFJE/ici1ZPVKSAiH4rIAdex/nKtF03l1COACaqaT1W/99TnM8YfWaIwWdH9qpoPqA3UAV69ukFEGgA/Az8ANwMVgE3A7yJS0VUnGFgM3Aa0AQoADYBTQN1UzlkO2HojwdrdgcnqLFGYLEtVjwI/4UwYV70LfK6qY1X1vKqeVtUhwCpguKvO00BZ4EFV3aaqDlU9rqojVXV+8vOIyF9AReB/rruPEBG5WUTmiMhpEdkjIs8lqj9cRL4RkZkicg54JoVj5haR90Xkb9ddz3IRyZ1Cva4ist11d7RXRLon2lZUROaKyFlXHMtEJMC17RUROeTab6eItLje62vMVfaXjsmyRKQ0cB/wi2s9D9AQeD2F6v8F3nIt3wssUNUL7pxHVW8Rkf3As6q6yHWuWcAWnHctVYGFIvKXqv7i2q0j8CjOpBSSwmFH47yjaQgcBeoBjhTqHQfaA3uBxsCPIrJGVdcDLwMRQDFX3fqAikgVoDdwl6oeFpHyQKA7n9WYlFiiMFnR9yKiQD6cSWKYq7wIzrvkIynscwS42v8QBqy70ZOLSBmgEdBOVaOAjSLyMc6kcDVRrEzUl3E52f4BwP8B9VX1kKt4hWtbknOp6rxEq7+JyM/APcB6IAYoCZRT1T3AMtcx4nAmp+oickJV99/oZzUGrOnJZE0PqGp+oCnOv+avJoAzOP8qL5nCPiWBk67lU6nUcdfNwGlVPZ+o7G+gVKL1g2nsXxQIBf5K70Qicp+IrHI1LZ0F2pLwed8D9gA/u5qlBgG4ksZLOJvajovILBG52b2PZsy1LFGYLEtVfwNm4GzGQVUvAitxNvkk9xjODmyARUBrEcl7g6c+DBQRkfyJysoChxKtpzUs80kgCrglrZOISAjwLc7PV0JVCwHzAQFw9cG8rKoVgQ5A/6t9Ear6harejbMTXoF3ruPzGZOEJQqT1X0ItBSR213rg4B/iEgfEckvIoVF5A2cTzX9y1Xn3zj/4v9WRKqKSICIhInIayLSNr0TqupBnE1Fb4tIqIjUAroBM90JWFUdwHRgjKtTPFBEGrgSQ2LBOJuQTgCxInIf0OrqRhFpLyKVxNleFQnEAQ4RqSIizV3Hi8LZ9JVS/4cxbrFEYbI0VT0BfI6rA1tVlwOtgYdw9kv8jfMR2rtVdberTjTODu0dwELgHPAHziad1W6eujNQHufdxWxg2NWObjf9E/gTWAOcxvkXf5KfR1fTVh+cHfFngCeAOYmqVMZ5d3QB553UJFVdgjO5jMJ553IUKE6iR4iNuV5iExcZY4xJi91RGGOMSZMlCmOMMWmyRGGMMSZNliiMMcakKcu9mV20aFEtX768r8MwxpgsZd26dSdVtVj6Na+V5RJF+fLlWbt2ra/DMMaYLEVE/r7Rfa3pyRhjTJosURhjjEmTJQpjjDFpskRhjDEmTZYojDHGpMkShTHGmDR5LFGIyHQROS4iW1LZLiIyzjXf8GYRucNTsRhjjLlxnryjmAG0SWP7fTiHSa4MPA9M9mAsxhiTM6ly5a8lGTqEx164U9WlrkndU9MR+Fyd45yvEpFCIlJSVVOa79gYY8z1OLQCZjViwP9asuFwRmb+9e2b2aVIOq9whKvsmkQhIs/jvOugbNmyXgnOGGOypLgY+DA4frXGTccZt7xehg6ZJTqzVXWaqoaranixYjc0VIkxxmRvu76F319n26BSzFxXK7746a512bm7f4YO7cs7ikNAmUTrpUk6Ob0xxpjE4mLg2DrYNx80DiKWQf7SsONLLl3JxRuLGvPerz0IDFDq31GQSv9ciuCcszcjfJko5gC9RWQWUA+ItP4JY4xJJu4KHF0LsxqlWuXH7ZV4YXY79p0uDEC3TkUJ6/ZTpoXgsUQhIl8CTYGiIhIBDANyAajqFGA+0BbYA1wCunoqFmOMyTLOH4JDy2HNO3B8Q+r1bqrLoUIP8dL70XyzRAGoVasEU6a0o0GDMqnvdwM8+dRT53S2K/CCp85vjDFZxtE1sHYMHFgEl0+mXu/ut6Deq/GrLzwwix+W7CRPnlyMGNGUvn3rExSU+V3PWW4+CmOMydLOR8CR1XB6O/z+OqAp16twHwSGQtMxULB8fHFsrCM+Gbzzzr3kyhXI+++3omzZgh4L2RKFMcZ4QmwU/L0Izh+AI6tg71yIOpP2PuVbw80N4Y6+EJL0F39kZBRDhvzCrl2nWbDgSUSEKlWK8vXXj3rwQzhZojDGmMx0YAl83Tz9egXKQZFqULw23PUKhBZKsZqq8vXX23jppQUcOXKBwEBh48aj1KmTsZforoclCmOMyajd38HyIc7mpORCCkGZphBaBMo0g0odITi/W4f966/T9O79IwsW7AGgQYPSTJnSnlq1SmRi8OmzRGGMMTfqyGr4on7K2+4ZBXVfueFDjx69gqFDlxAVFUuhQqG88869PPvsHQQEyA0f80ZZojDGGHepA1aOhA3jIOr0tdurdoZ73nY2K2XQpUsxREXF0qVLLUaPbkXx4nkzfMwbZYnCGGPScuJP+KIuFK3pfIw1JaXuhofmu92klOJpTlxk585T3H23czy7V15pRNOm5WncOONJJ6MsURhjTHL7f4JfXoQzuxPKkieJ+792PsKaK2N/6TscyvTpGxg4cCFBQQHs2NGbIkVyExIS5BdJAixRGGNMgphLMC6VX/z5y0CLiVCyPuTJnMFJt2w5To8ec/n9d+dA2i1bVuTSpRiKFMmdKcfPLJYojDEGnHcP029NWtZgOIT3z1CTUkouXrzCiBG/MWbMKmJjHZQokZcPP2xDp063IeL9zur0WKIwxuRsJ7fCZzWSlhW7HZ7e6LFTPvLI1yxYsAcR6NUrnDffbEGhQqEeO19GWaIwxuQ8qrBhAizpc+22O/s5h83woFdeacSxYxeYPLkd9eqV9ui5MoMlCmNMznDlPBxeCd+2Tnl72G3QZT0EBqe8/QbFxjoYP341+/efZezY+wBo2rQ8a9c+75N3Im6EJQpjTPZyfCMsHegcV+nYWmf/wpXzqdfv+D3c0gE80Dfwxx+H6N59Lhs3HgXg+efv5LbbigNkmSQBliiMMdnFqe0wo/q15SkliYrtoP1XGX60NTVnz0bx2muLmTJlLapQrlxBJkxoG58kshpLFMaYrEsVfuoKWz+7dlv1LlC6KZRq5HxTOjAYJPPnakhu1qwtvPTSAo4du0hQUAAvv9yAoUMbkzdv5jZpeZMlCmNM1hEXA2tHO4fsPrwi5TplW8AjP3slKaTk55//4tixizRqVIbJk9tRs6Z3B/DzBEsUxpis4fBK+LJh6tu7bHAO2e1l0dGxHDp0nooVnfNVv/tuS+65pyz/+EftLNUPkRZLFMYY/xWxFP6aC2vfu3Zbw3/BTXWddxCBubwfG/DLL/vo2XMeAQHCpk09CA4OpGjRPHTtWscn8XiKJQpjjH9RhdVvwe9DUt7efALUecG7MSVz7NgF/vnPhcycuRmAqlWLEhFxLv6uIruxRGGM8Q8Xj8KUVGZtq/ywaya4gZn+nsP1cDiUjz5ax6BBizl7NorQ0CCGDLmHAQMaERwc6LO4PM0ShTHG9yKWwldNri1/cg3cFO79eFLx4INfMWfOTgBat76FiRPbcsstRXwcledZojDG+MaV8zD7foj4LWl5WHV4ZCHku9k3caXhoYeq8scfhxg7tg2PPlrdLwfw8wRLFMYY74k+B9v+Des/gLN/Xbu9/VdQ5THvx5WKOXN2EhFxjl697gLg6adv56GHqpE/f4iPI/MuSxTGGM+KuQTzn4LIfXAihRFZi1R1zg5XsIL3Y0vFgQOR9OnzIz/8sJOQkEDatKlExYqFEZEclyTAEoUxxhNOboVNU2DnLLh88trtNzeC8q0g/GWPDaNxI2Ji4hg3bjXDhv3KxYsx5M8fzBtvNKdcuYK+Ds2nLFEYYzLufAT8vQjWfwgnNqVer+MPULZZpk8ElBlWrYqge/e5bN58DIBHH63OBx+0plSpAj6OzPcsURhjbowqLB8Mf7ydep3QMOfcDtWegAD//nUzdOgSNm8+RoUKhZgwoS1t21b2dUh+w7+/csYY/3TkD/ii3rXlBSs455SuP8T59JIfU1XOn79CgQLOPocJE+7j8883MXhwY/Lk8c2b3v7KEoUx5vpE7rs2SXReATc38E08N2DnzpP06jUfEVi4sAsiQpUqRXnzzRa+Ds0vWaIwxqTv0gk4tAz+XujspL7qwXlQsa3v4rpOUVGxvP32MkaN+p0rV+IIC8vN/v1nqVAhew69kVksURhjUhexDL5qnPK2Vh9nqSSxcOFf9Oo1nz17TgPwf/9Xm3ffbUlYWB4fR+b/PJooRKQNMBYIBD5W1VHJtpcFPgMKueoMUtX5nozJGOOGzR/BwuevLQ8Ng3L3QoPhEFbV62HdCFWlW7c5fPqp8x2O6tWLMWVKO+65p5yPI8s6PJYoRCQQmAi0BCKANSIyR1W3Jao2BPivqk4WkerAfKC8p2IyxqThwmFY8hLs+vrabY3egPqDvR9TJhARypcvRO7cQbz+ehP692+QrQfw8wRP3lHUBfao6l4AEZkFdAQSJwoFrj6kXBA47MF4jDHJOeJgy3RY+S+4cOja7U+thxJZb26FjRuPcuTIee67z/mI6yuvNKJLl1rWF3GDPJkoSgEHE61HAMmfpxsO/CwiLwJ5gXtTOpCIPA88D1C2bNlMD9SYHOnCEZiawsB7BcpBi4lQsZ33Y8qg8+ejGTbsV8aOXU1YWG527OhNkSK5CQkJsiSRAb7uzO4MzFDV90WkAfBvEamhqo7ElVR1GjANIDw8XH0QpzHZw8VjMOWmFDYINH0f7ujrs7mmM0JV+f77HfTps4CIiHMEBAhPPFGTXLmy3mfxR55MFIeAMonWS7vKEusGtAFQ1ZUiEgoUBY57MC5jcqZp5eD8gWvL670Gd7/p/Xgyyd9/n6V37x+ZO3cXAOHhNzN1anvuuCOVSZDMdfNkolgDVBaRCjgTxOPAE8nqHABaADNEpBoQCpzwYEzG5CyOOFgxDFYnSwS58sEzW6FA1m7KVVUefvi/rFt3hAIFQnjrreb06BFOYKDdSWQmjyUKVY0Vkd7ATzgffZ2uqltFZASwVlXnAC8DH4lIP5wd28+oqjUtGZMRqrB/AXyXyjsO/eOyZPNSYg6HEhAgiAijR7diypS1fPBBa0qW9L/BBrMDyWq/l8PDw3Xt2rW+DsMY/xKxHBZ1h1PbUt4eEATNxkLtXt6NK5OdOnWJQYMWAfDRRx18HE3WIiLrVPWG5pX1dWe2MSYjrlyAGbel3PcAzhnjKj8MAVn7vQFV5fPPN/HPfy7k5MlLBAcHMmxYU0qXtiHAvcEShTFZkSr8+REs7J60vO0XcFO48xHXwGDfxJbJtm8/Qc+e8/jtt78BaNq0PJMnt7Mk4UWWKIzJas4dhI+SdUIXKA/d9mT5O4fEVJXXX1/CO+/8TkyMg6JF8/D++63o0qUWIuLr8HIUSxTGZAVbP3dOK7rvx2u3dfoNSqcycF8WJiIcOnSemBgHzz13B6NG3UuRIrl9HVaOZInCGH92eBV8mco8D+VawiM/ezceDzt8+DwnT16iVq0SALz7bku6datDo0ZZ+zHerM4ShTH+KO4KTCsLl44lLa83GCp1hJvu8k1cHhIX52Dy5LUMHvwLpUrlZ+PGHgQHB1K0aB6KFrUk4WuWKIzxF45YOLwSVo10ThCUWOvpUKOrb+LysPXrj9C9+1zWrnWOCdq4cTnOnYumaFGbJ8JfWKIwxh9s/hgWPpfythdOQ2j2G9Du3Llohg79hQkT1uBwKKVLF2DcuDY88EBV66z2M24nChHJo6qXPBmMMTnO2vfht38mLQstDGVbwN1vQ+FKvonLw1SVxo0/ZdOmYwQGCv3712f48Kbkzx/i69BMCtJNFCLSEPgYyAeUFZHbge6qmrVf8TTG1xb3ho0Tk5Y9tQ5K3OGbeLxIROjXrz6TJq1l6tT21K6d0oi2xl+kO4SHiKwGHgHmqGodV9kWVa3hhfiuYUN4mCwt7grMfxJ2fZO0/Nm9ULCCb2LygitX4hgzZiWBgcKAAY0A512Fw6E2gJ+XeHwID1U9mKzNMO5GTmZMjhUd6UwQe+ddu63rjmydJJYt+5sePeaxbdsJQkICefrp2ylRIh8iQmCg9UVkBe4kioOu5icVkVxAX2C7Z8MyJhtQhQ3jYUnfa7eFhsETq7JtHwTAyZOXGDhwIZ9+uhGAypWLMGlSO0qUyOfjyMz1cidR9ADG4pza9BDwM2D9E8akJjbKOVBf5N5rtxW+FTp8B0Vv835cXqKqzJixkQEDFnLq1GWCgwN59dW7GTTobkJD7UHLrMidr1oVVX0ycYGINAJ+90xIxmRRp7bDjOopb2s4AuoPgRzy2OfMmX9y6tRlmjevwKRJbalSpaivQzIZ4E6iGA8kfwwjpTJjcp7YaFg6EDaMS3l7r5OQO8y7MfnApUsxREZGUbJkfkSESZPasmbNYZ58sqa9E5ENpJooRKQB0BAoJiL9E20qgHPGOmNytq2fw4J/XFt+10Bo/I734/GRH3/czQsvzKdixcIsXNgFEaFKlaJ2F5GNpHVHEYzz3YkgIPH8gudwPi5rTM5y6Tis+wAun4I/P8Y5e28i7WZBlcdyTPPSoUPneOmln/jmG+esevnzh3Dq1GUbeiMbSjVRqOpvwG8iMkNV//ZiTMb4n2PrYGYqj6B3Wgql7/FuPD4UF+dg4sQ1DBnyC+fPXyFv3lyMGNGMPn3qERRk70RkR+70UVwSkfeA24DQq4Wq2txjURnjTxY8A1s/S1gvWR/Kt4b8ZaF6FwjM5bPQvM3hUJo0mcHvvx8E4IEHqjJ2bBvKli3o48iMJ7mTKP4DfAW0x/mo7D+AE54MyhifUwdsmQE/d0ta/uA8qNjWJyH5g4AAoVWrWzhwIJIJE9rSoUMVX4dkvMCdITzWqeqdIrJZVWu5ytaoqk8GxLchPIzHqMLmqbCoZ8rbnz8I+Ut7NyYfU1X++9+tBAUF8PDDzkd/o6NjiYlxkC9f9piTO6fw9BAeMa7/j4hIO+AwUORGTmaM37p8Cial8pTO/d/ArQ97Nx4/8Ndfp+nVaz4///wXxYrloXnzChQunJuQkCBCbJDXHMWdRPGGiBQEXsb5/kQB4CWPRmWMt0Sfgy3T4dd+Scsbvwd39oOAnPckeHR0LO+9t4I331xGVFQshQuH8uabzSlYMDT9nU22lG6iUNW5rsVIoBnEv5ltTNb2x7uw7JWkZaUbQ6fffBOPH/j11/307DmPHTtOAtClSy1Gj25F8eJ5fRyZ8aW0XrgLBB7DOcbTAlXdIiLtgdeA3EAd74RoTCZTB4xJdqdQvA40GgkV2/kmJj8QF+egVy9nkqhSJYzJk9vRrFn2HdXWuC+tO4pPgDLAH8A4ETkMhAODVPV7bwRnTKa7dBwml0ha1mE2VH7AN/H4mMOhREXFkidPLgIDA5g8uR1Ll/7NwIGNCAmxAfyMU1rfCeFALVV1iEgocBS4RVVPeSc0YzJRxFL4qsm15f3jQHLmS2J//nmMHj3mUbVqGJ980hGAJk3K06RJed8GZvxOWoniiqo6AFQ1SkT2WpIwWdJXTSEiWb9Djf+D1p/4JBxfu3jxCiNG/MaYMauIjXWwb98Zzpy5TOHCuX0dmvFTaSWKqiKy2bUswC2udQH06jsVxvitK+dhfIGkZfWHQN3XIFfO/KX4v//tpHfvHzlwIBIR6NUrnDffbEGhQvZEk0ldWomimteiMCaz7f4O5iR796H3WQjJmUNNxMY66NTpG777zjk5Ze3aNzF1anvq1i3l48hMVpDWoIA2EKDJei4ehSklk5aVbw0PL/BNPH4iKCiAggVDyJcvmJEjm9G7d10bwM+4Ld0hPDJ0cJE2OKdRDQQ+VtVRKdR5DBiOc8zmTar6RFrHtCE8TIrWjoHfXr62vNMyKH239+PxA6tXRwBQr55z2JFTpy5x+XIspUsXSGs3k015egiPG+J6D2Mi0BKIANaIyBxV3ZaoTmXgVaCRqp4RkeKeisdkU6owLi/EXk5aXqaZ8y4iMOeNR3T2bBSvvrqIqVPXUbVqUTZu7EFwcCBhYTZPhLkxbiUKEckNlFXVnddx7LrAHlXd6zrGLKAjsC1RneeAiap6BkBVj1/H8U1Od+UCjM+ftKzVJ1DjmRz5yKuq8uWXW+jf/yeOHbtIUFAAHTpUIS7OgU1KaTIi3UQhIvcDo3HOeFdBRGoDI1S1Qzq7lgIOJlqPAOolq3Or6xy/4/xOHq6qObsx2bhn9/cw58GkZTm4s3r37lP06jWfRYv2AtCoURmmTGlPjRp2k24yzp07iuE47w5+BVDVjSKSWe/1BwGVgaZAaWCpiNRU1bOJK4nI88DzAGXLls2kU5ssZ81o5zDgZ/ckLS9SFbpu901MfiAmJo7mzT8nIuIcRYrk5t1376Vr1zoEBOSMKVmN57k1zLiqRkrSeYDd6QE/hHMIkKtKu8oSiwBWq2oMsE9EduFMHGuSnEx1GjANnJ3ZbpzbZCentsGM21Le1mws3NHHu/H4CVVFRMiVK5A332zOkiX7effdeylWzAbwM5nLnUSxVUSeAAJdnc99gBVu7LcGqOy6+zgEPA4kf6Lpe6Az8KmIFMXZFLXX3eBNNndmDyzqDgd+SVre6hMody8UyJl3l8eOXeCf/1zIrbcWYehQ57AkTz99O08/fbuPIzPZlTuJ4kVgMBANfAH8BLyR3k6qGisivV31A4HpqrpVREYAa1V1jmtbKxHZBsQBA2yYEENsFIxN4c3pFhOhdi/vx+MnHA7lo4/WMWjQYs6ejaJQoVBeeqk++fPbLELGs9yZCvUOVV3vpXjSZe9RZHOz74e9c5OWVWznvIvIWyLlfXKATZuO0qPHPFatcr4b0aZNJSZObEvFioV9HJnJKjz9HsX7InIT8A3wlapuuZETGZMmRyx8kCtpWWhheOG0b+LxEzExcbz66mI+/HAVcXFKyZL5GDu2DY88Up1k/YbGeIw7M9w1cyWKx4CpIlIAZ8JIt/nJGLd8GAJxV5KW9YvNkdOQJhcUFMCGDUdxOJQXX6zLyJHNbEpS43XXNYSHiNQEBgKdVNUnr7xa01M2ogpjkr0Yd9Nd8OQfvonHTxw4EElcnIMKFZzNSrt3nyIyMprw8Jt9HJnJyjza9CQi1YBOwMPAKeArIIVBdYy5Dqe2w4zqScv6OyAHN6fExMQxduxqhg37lQYNSrNwYRdEhMqVw3wdmsnh3OmjmI4zObRW1cMejsfkBD89C1uSTRr0cs5+PWblyoP06DGPzZuPAVCkSG4uXYohb96cN1aV8T/u9FE08EYgJgfY/zN82zpp2V0DofE7vonHD5w5c5lBgxYxbZrzwcIKFQoxcWJb7ruvso8jMyZBqolCRP6rqo+JyJ8kfRPbZrgz188Rd22S6Hkc8hTzTTx+IDo6ltq1p3LgQCS5cgUwYEBDBg9uTJ48udLf2RgvSuuOoq/r//beCMRkc4lfoGs0Euq+muOfagoJCaJbtzosXryPyZPbUb16zk2axr+lOhazqh5xLfZS1b8T/wNy7uux5vot7A6OGOdyUB7nvNU5MElERcUybNgSvvjiz/iy1167h19//YclCePX3Bm0v2UKZfdldiAmm4qNhs3TEtZfjPRdLD60cOFf1Kw5mREjltKv309cvuxMnEFBAfbinPF7afVR9MR551BRRDYn2pQf+N3TgZlsYmyil8N6noAAj02q6JeOHr1A//4/8eWXzgENbrutGFOmtCd3buuHMFlHWj+1XwA/Am8DgxKVn1fVnD2ugkmfIxY+r520LE9R38TiA3FxDqZOXcdrry0mMjKa3LmDGDasCf36NSA4OOc1u5msLa1Eoaq6X0ReSL5BRIpYsjApUoVvWsKBxUnL+8X4Jh4fiYtTxo//g8jIaNq2rcyECffFv2ltTFaT3h1Fe2AdzsdjEzekKlDRg3GZrOjyaZiU7C3i0DDoeSxHdF6fPx9NXJxSqFAowcGBfPTR/Rw7doGHHqpm/RAmS0s1Uahqe9f/mTXtqcnO4mKuTRLPR0D+Ur6Jx4tUldmzd9Cnz4+0bn0Ln3zSEYC7786ZEyuZ7Cfdp55EpJGI5HUtPyUiY0TEfgJMgt3fwYeJhpqo1d05JEcOSBL795+lQ4dZPPzwfzl06DxbtpwgKirW12EZk6nceTx2MnBJRG7HORjgX8C/PRqVyRr2zoP3BeY8nFAWVh1aTvFdTF4SExPHO+8sp3r1icydu4sCBUKYMOE+Vqz4P0JDc9aTXSb7c+c7OlZVVUQ6AhNU9RMR6ebpwIyfi9wHs5O9tP/IQudc1tncpUsx1K//MX/+eRyAxx+vwZgxrShZMr+PIzPGM9xJFOdF5FWgC3CPiAQA9hB4TvdxomcZOnwHlR7IMUOE58mTi/Dwm7l0KYZJk9rRqtUtvg7JGI9yJ1F0Ap4A/k9Vj7r6J97zbFjGr31eJ2G5Qluo/KDvYvECVeXzzzdxyy1F4juoP/igNcHBgfbinMkR0u2jUNWjwH+AgiLSHohS1c89HpnxT4t7w4mNCesdv/ddLF6wffsJmjX7jGee+YHnn/8fV67EAVCwYKglCZNjuDPD3WM47yB+xfkuxXgRGaCq33g4NuNvxgSBxgtNs+oAACAASURBVCWsvxQNgdnzl+XlyzG8+eYy3n33d2JiHBQrlodXX72bXLncef7DmOzFnaanwcBdqnocQESKAYsASxQ5SdSZpEni2X0QmD1nX1uwYA8vvDCfvXvPAPDcc3cwatS9FCmSO509jcme3EkUAVeThMsp3Hus1mQnXzZMWM7Gc1tfuHCFLl1mc/LkJWrUKM6UKe1o1MheGzI5mzuJYoGI/AR86VrvBMz3XEjG72z9DE7vSFjPZkkiLs6Bw6HkyhVIvnzBjB3bhoiIc/TrV59cubL/0CPGpMedObMHiMhDwN2uommqOtuzYRm/oQoLnklY73XSZ6F4wrp1h+nefS4dO1Zh6NAmADzxRE0fR2WMf0lrPorKwGjgFuBP4J+qeshbgRk/cO4gfJSo2eWpdZA7LPX6Wci5c9EMHfoLEyasweFQzp2LZtCgu+0OwpgUpNXXMB2YCzyMcwTZ8V6JyPiPj5K1zZe4wzdxZCJV5euvt1K16gTGjfsDEejfvz7r13e3JGFMKtJqesqvqh+5lneKyHpvBGT8QGwUjE30hE/pxtDpN9/Fk0nOn4+mU6dv+PHHPQDUq1eKKVPaU7v2TT6OzBj/llaiCBWROiTMQ5E78bqqWuLIjvYtgO+STYn+2K8+CSWz5csXTHR0HAULhjBq1L08//ydBARkr455YzwhrURxBBiTaP1oonUFmnsqKOMjf06HnxON95ivNHQ/6Lt4MsHSpX9TsmQ+KlcOQ0SYPr0DoaFBlCiRz9ehGZNlpDVxUTNvBmJ8bGIYRCWa3fbBeVCxre/iyaCTJy8xcOBCPv10Iy1aVGDhwi6ICOXKFfJ1aMZkOTZwvoF5TyRNEo8vh1KNfBdPBjgcyowZGxkwYCGnT18mODiQe+4pS1ycEhRkzUzG3AiPJgoRaQOMBQKBj1V1VCr1HsY5JMhdqrrWkzGZZM7+BTu+TFjPwm9db916nJ4957Fs2QEAWrSowKRJ7bj11uzxSK8xvuKxRCEigcBEoCUQAawRkTmqui1ZvfxAX2C1p2IxKdjxFcx7PGnZo4uzbJKIjIyifv1PuHDhCsWL52XMmFY88URNJIt+HmP8iTujxwrwJFBRVUe45qO4SVX/SGfXusAeVd3rOs4soCOwLVm9kcA7wIDrDd7coHN/X5skqj0JZZr6JJyMUFVEhIIFQ3nllUYcOnSOt95qQeHCNoCfMZnFnTuKSYAD51NOI4DzwLfAXensVwpI/MhMBFAvcQURuQMoo6rzRCTVRCEizwPPA5QtawO03TBV+LQKnNmdUHbfv6H6U76L6QYdOnSOvn0X0LFjFbp0uR2AwYPvsTsIYzzAnVFg66nqC0AUgKqeATI8vrRrStUxwMvp1VXVaaoarqrhxYoVy+ipc6aja2BMQNIk0fjdLJckYmMdjB27iqpVJ/Ltt9sZNuxX4uIcAJYkjPEQd+4oYlz9DQrx81E43NjvEFAm0XppV9lV+YEawK+uH/CbgDki0sE6tDPZin/ByuFJy3qfhZCCPgnnRq1Zc4gePeaxfv0RAB54oCrjxrUhMNBGvTfGk9xJFOOA2UBxEXkTeAQY4sZ+a4DKIlIBZ4J4HOfc2wCoaiRQ9Oq6iPyKc+BBSxKZaWF32DwtYb3RSKjvzpfPf1y8eIVXXlnEpElrUIWyZQsyfvx9dOhQxdehGZMjuDPM+H9EZB3QAufwHQ+o6nY39osVkd7ATzgfj52uqltFZASwVlXnZDB2k54ds5ImiR5HIG/WG9coKCiARYv2EhAg9O/fgGHDmpA3b/acXc8YfySqmnYF51NO11DVAx6JKB3h4eG6dq3ddKTr5Bb4LNG8Cn0uQK68vovnOv3112kKFQolLCwP4Gx2Cg0NombNEj6OzJisSUTWqWr4jezrTtPTPJz9EwKEAhWAncBtN3JC4wURy+GrexLWn96cZZJEdHQs7723gjffXMaTT9bk4487AHDXXaV8HJkxOZc7TU9JpvtyPdLay2MRmYyJPpc0SbSZAcWyxoxtv/66n54957Fjh3MWvdhYB3FxDuusNsbHrvvNbFVdLyL10q9pvC76HExI9CRT6+lw2z98F4+bjh+/yIABC/n8800AVKkSxuTJ7WjWrIKPIzPGgHtvZvdPtBoA3AEc9lhE5vrFXII/3oZVbySU5S8LNbr6LiY3nTx5iWrVJnL69GVCQgIZPPgeBg5sREiIjVdpjL9w56cxf6LlWJx9Ft96Jhxz3RxxMC5Z/0O1J6HtTN/Ec52KFs1Dx45ViIg4x6RJ7ahUqYivQzLGJJNmonC9aJdfVf/ppXjM9YiOhAmJ51cQaP8VVHnUZyGl5+LFK4wY8Rvt2t1K48blAJg0qR0hIYH2ZrUxfirVRCEiQa53IbLmxATZWWw07PwKFiTqfwjODy+e811Mbvjf/3bSu/ePHDgQybx5u9m8uScBAUJoqDUzGePP0voJ/QNnf8RGEZkDfA1cvLpRVb/zcGwmJSf+hM9rJS2r1R1aTvFNPG44eDCSvn0XMHv2DgDq1LmJqVPb23zVxmQR7vwpFwqcwjl67NX3KRSwROFtczvDzlkJ67mLQYuJftvUFBvrYNy41bz++hIuXowhX75g3nijGS+8UJegIHvk1ZisIq1EUdz1xNMWEhLEVWm/zm0yV1wMfJhsyIosMDz4uXPRvP32ci5ejOHhh6vx4YdtKF26gK/DMsZcp7QSRSCQj6QJ4ipLFN6ijmuTRM/jkMc/h1s/ezaK3LmDCAkJokiR3Eyd2p6QkEDatbvV16EZY25QWoniiKqO8FokJmXLE430WqQqdE13PEafUFW+/HIL/fr9RO/edzF0aBMAHnqomo8jM8ZkVFqJwnoa/cEfbycs+2mS2LXrFL16zWPx4n0ALF16IH6KUmNM1pdWomjhtShMyqZXTVh+xv+SRFRULO+8s5y33lrOlStxFCmSm/fea8kzz9S2JGFMNpJqolDV094MxCTzWU04szNhPaxq6nV94OjRCzRu/Cm7dzu/TZ55pjbvvdeSokXz+DgyY0xmszed/NHBX53zSVzV46jPQklNiRJ5KVOmIEFBAUye3I4mTcr7OiRjjIdYovBHPzyQsNwvBgJ8/2VyOJSPPlpHs2YVuPXWMESEL754iMKFcxMcHOjr8IwxHmRvPfmb3wY4x3ACKHuvXySJTZuO0qjRdHr0mEevXvO4OitiiRL5LEkYkwP4/reQSfBtG9j/U8J6B98O0nvhwhWGD/+VDz9cRVyccvPN+enR44ZmUjTGZGGWKPyFIy5pkuhxBEJ89xbz99/v4MUXfyQi4hwBAcKLL9bljTeaU6BAiM9iMsb4hiUKf/FBoi9Fn4uQy3dPDx06dI7HH/+G6Og47ryzJFOmtCc8/GafxWOM8S1LFP7gz0+SrvsgScTExBEUFICIUKpUAd58sznBwYH06nWXzVltTA5nvwF87XwE/Pxswnr/OK+HsGLFQe68cxozZ26OL3v55Ya8+GI9SxLGGEsUPhV9DqaVSVhv+x8Q731JTp++TPfu/6NRo+n8+edxJk1aG/9EkzHGXGVNT77iiIUJBRPWq3SCak945dSqysyZm3n55Z85ceISuXIFMHBgIwYPvseG3jDGXMMSha98kCthudTd0H5W6nUz0bFjF+jc+VuWLNkPQJMm5Zg8uR3VqvnnsOXGGN+zROELiYcOD8gFjy722qkLFQrlyJELFC2ah9GjW/L007fbXYQxJk2WKHxh9ZsJy/2uePx0Cxf+xR13lCQsLA8hIUF8/fWjlCyZj7AwG8DPGJM+68z2tm0zE5Y7LfPoqY4cOU/nzt/SqtVMXnllUXx5jRrFLUkYY9xmdxTedPEY/NglYb303R45TVycg6lT1/Hqq4s5dy6a3LmDqFIlzCYTMsbcEEsU3vTdfQnL7TzTeb1+/RF69JjLmjWHnadpV5kJE9pSvnwhj5zPGJP9WaLwllVvwvENzuUC5aFqp0w/xf79Z6lb9yPi4pRSpfIzbtx9PPhgVbuLMMZkiEcThYi0AcYCgcDHqjoq2fb+wLNALHAC+D9V/duTMfnM74medHpytUdOUb58Ibp2rU3+/CH8619NyZ/fBvAzxmScxzqzRSQQmAjcB1QHOotI9WTVNgDhqloL+AZ411Px+NSFIwnLz/0NeYpnymH37z/L/fd/yW+/7Y8vmzbtfsaMaW1JwhiTaTx5R1EX2KOqewFEZBbQEdh2tYKqLklUfxXwlAfj8Z3N0xKWC5TN8OFiYuIYM2Yl//rXb1y+HMvJk5dYubIbgDUzGWMynScTRSngYKL1CKBeGvW7AT+mtEFEngeeByhbNuO/aL0qNhpWDncuhxRMs6o7li8/QI8ec9m69QQAjz9egzFjWmX4uMYYkxq/6MwWkaeAcKBJSttVdRowDSA8PDxrjVr3cfmE5YcW3PBhzpy5zIABC/nkE2eH+C23FGbSpHa0anVLBgM0xpi0eTJRHAISDY1KaVdZEiJyLzAYaKKq0R6MxzcuHk1Yvrn+DR/G4VB++GEnuXIFMGjQ3bz66t3kzp0r/R2NMSaDPJko1gCVRaQCzgTxOJBkeFQRqQNMBdqo6nEPxuIbxzYkLPe5dN2779hxkgoVChESEkRYWB7+85+HKFu2IFWrFs3EII0xJm0ee+pJVWOB3sBPwHbgv6q6VURGiEgHV7X3gHzA1yKyUUTmeCoen5h5R8Jyrtxu73bpUgyDBy+mVq3JvPvu7/HlrVrdYknCGON1Hu2jUNX5wPxkZa8nWr7Xk+f3qbhEg/3d0dft3RYs2EOvXvPYt+8sACdPXv+diDHGZCa/6MzOltaPTVi+Z1Tq9VwOHz7PSy8t4OuvnU8P16xZnClT2tOwYZl09jTGGM+yROEpSwc6/5cACApNs+quXacID5/G+fNXyJMnF8OHN+Gll+qTK1egFwI1xpi0WaLwhB1fJSzXfiHd6pUrF+Guu0qRN28uxo+/j3LlbAA/Y4z/sESR2aLOwrzHE9abjb2myrlz0bz++hJ69bqLW28NQ0SYM+dx8uYN9mKgxhjjHksUmW1i4YTlZ/dCoiE1VJVvvtlG374LOHLkAjt2nGTBAueoJZYkjDH+yhJFZtryacJyiXAoWCF+de/eM/TuPZ8ff9wDQP36pXnnnez70JcxJvuwRJGZDvySsPzkHwBcuRLH6NErGDlyKVFRsRQqFMqoUS147rk7CQiwAfyMMf7PEkVmOfgrbHfNh127d3yT08GDkYwY8RvR0XE8+WRN3n+/FSVK5PNdnMYYc50sUWSGi8fgv83iV89U7EEh1/zUt9xShLFj21CpUhFatKjowyCNMebGeGwIjxxlyk0AOBzCdP5LpfB5zJy5OX5z9+7hliSMMVmWJYqMcg3VsfVoMZp+8iLd/rmN06cvx3daG2NMVmdNTxl06dh+Rs5vwehfGxLrCKR48bx88EFrOneu4evQjDEmU1iiyIBdu07R+p5P2X/8HkSUHj3u5K23WlC4sPsjxRpjjL+zRJEB5coVJJTz3H7zWaY8vZb6bw/3dUjGj8TExBAREUFUVJSvQzE5SGhoKKVLlyZXrsyb2MwSxXWIjXUwZcpaOneuQVjhYEJ+6caCZ3+gVMHzBD3xm6/DM34mIiKC/PnzU758eUTsnRnjearKqVOniIiIoEKFCunv4CZLFG76449D9Ogxlw0bjrJx41E+rtYRgHJFXBVKNfJdcMYvRUVFWZIwXiUihIWFceLEiUw9riWKdERGRjF48C9MmrQGVShbtiAdq28FTVSp+6EkYzoZc5UlCeNtnvies0SRClXlq6+20q/fTxw9eoGgoAD696/P609dIu/PDyRU7O+wJGGMydbsPYpUbNp0jM6dv+Xo0Qs0bFiG9euf5513WiZNEl02WJIwfi0wMJDatWtTo0YN7r//fs6ePRu/bevWrTRv3pwqVapQuXJlRo4ciWrCrfKPP/5IeHg41atXp06dOrz88su++Ahp2rBhA926dfN1GKmKjo6mU6dOVKpUiXr16rF///4U6509e5ZHHnmEqlWrUq1aNVauXAnA119/zW233UZAQABr166Nr//nn3/yzDPPeOETOFmiSCQuzhG/XLv2TfTrV5+PPrqfZcu6UrNmCUj0Q0SH2VC8tg+iNMZ9uXPnZuPGjWzZsoUiRYowceJEAC5fvkyHDh0YNGgQO3fuZNOmTaxYsYJJkyYBsGXLFnr37s3MmTPZtm0ba9eupVKlSpkaW2xsbIaP8dZbb9GnTx+vnvN6fPLJJxQuXJg9e/bQr18/XnnllRTr9e3blzZt2rBjxw42bdpEtWrVAKhRowbfffcdjRs3TlK/Zs2aREREcODAAY9/BrCmp3hLluyjV6/5TJ3ansaNywEwZkzrpJUO/Z6wXLGtF6MzWd77HrrzfFnTr+PSoEEDNm92Di3zxRdf0KhRI1q1agVAnjx5mDBhAk2bNuWFF17g3XffZfDgwVStWhVw3pn07NnzmmNeuHCBF198kbVr1yIiDBs2jIcffph8+fJx4cIFAL755hvmzp3LjBkzeOaZZwgNDWXDhg00atSI7777jo0bN1KokHNWx8qVK7N8+XICAgLo0aNH/C/CDz/8kEaNkj4wcv78eTZv3sztt98OwB9//EHfvn2Jiooid+7cfPrpp1SpUoUZM2bw3XffceHCBeLi4pg/fz4vvvgiW7ZsISYmhuHDh9OxY0f2799Ply5duHjxIgATJkygYcOGbl/flPzwww8MHz4cgEceeYTevXujrnHgroqMjGTp0qXMmDEDgODgYIKDnfPTXE0YKbn//vuZNWsWAwcOzFCM7sjxieL48YsMGLCQzz/fBMCYMSvjE8U1vronYTnQJhoyWUdcXByLFy+Ob6bZunUrd955Z5I6t9xyCxcuXODcuXNs2bLFraamkSNHUrBgQf78808Azpw5k+4+ERERrFixgsDAQOLi4pg9ezZdu3Zl9erVlCtXjhIlSvDEE0/Qr18/7r77bg4cOEDr1q3Zvn17kuOsXbuWGjUSRkCoWrUqy5YtIygoiEWLFvHaa6/x7bffArB+/Xo2b95MkSJFeO2112jevDnTp0/n7Nmz1K1bl3vvvZfixYuzcOFCQkND2b17N507d07S3HPVPffcw/nz568pHz16NPfem3SOmUOHDlGmTBkAgoKCKFiwIKdOnaJo0aLxdfbt20exYsXo2rUrmzZt4s4772Ts2LHkzZs3zesYHh7OqFGjLFF4ksOhfPLJel55ZRFnzkQREhLIkCGNGTAglb8gHIluWW97xisxmmzkOv7yz0yXL1+mdu3aHDp0iGrVqtGyZctMPf6iRYuYNWtW/HrhwoXTqO306KOPEhgYCECnTp0YMWIEXbt2ZdasWXTq1Cn+uNu2bYvf59y5c1y4cIF8+RKG6D9y5AjFihWLX4+MjOQf//gHu3fvRkSIiYmJ39ayZUuKFHE+y/7zzz8zZ84cRo8eDTgfYz5w4AA333wzvXv3ZuPGjQQGBrJr164U41+2bFm6n/F6xMbGsn79esaPH0+9evXo27cvo0aNYuTIkWnuV7x4cQ4fPpypsaQmRyaKffvO8NRTs1mx4iAArVrdwsSJbalUqUjqOy0fnLDc+hMPR2hM5rjaR3Hp0iVat27NxIkT6dOnD9WrV2fp0qVJ6u7du5d8+fJRoEABbrvtNtatWxffrHO9EjetJH8zPfFfyg0aNGDPnj2cOHGC77//niFDhgDgcDhYtWoVoaGhaX62xMceOnQozZo1Y/bs2ezfv5+mTZumeE5V5dtvv6VKlSpJjjd8+HBKlCjBpk2bcDgcqZ77eu4oSpUqxcGDByldujSxsbFERkYSFhaWpE7p0qUpXbo09erVA5xNVKNGjUr1c191tYnNG3JkZ3aBAiHs2nWKm27Kx6xZD7NgwZNpJwmANe8mLEuOvGwmC8uTJw/jxo3j/fffJzY2lieffJLly5ezaNEiwHnn0adPn/hmjAEDBvDWW2/F/1XtcDiYMmXKNcdt2bJlfAc5JDQ9lShRgu3bt+NwOJg9e3aqcYkIDz74IP3796datWrxv0RbtWrF+PHj4+tt3Ljxmn2rVavGnj0JozRHRkZSqlQpgPj2/pS0bt2a8ePHxz/htWHDhvj9S5YsSUBAAP/+97+Ji4tLcf9ly5axcePGa/4lTxIAHTp04LPPPgOcfTXNmze/5j2Hm266iTJlyrBz504AFi9eTPXq1VON/6pdu3YlaXrzpBzzG++nn/YQHe1sPgoLy8OcOY+zY8cLdOpUw70XVK72STT70INRGuM5derUoVatWnz55Zfkzp2bH374gTfeeIMqVapQs2ZN7rrrLnr37g1ArVq1+PDDD+ncuTPVqlWjRo0a7N2795pjDhkyhDNnzlCjRg1uv/12lixZAsCoUaNo3749DRs2pGTJkmnG1alTJ2bOnBnf7AQwbtw41q5dS61atahevXqKSapq1apERkbG/3U/cOBAXn31VerUqZPm001Dhw4lJiaGWrVqcdtttzF06FAAevXqxWeffcbtt9/Ojh070u0jcEe3bt04deoUlSpVYsyYMfF3CocPH6Zt24QHYsaPH8+TTz5JrVq12LhxI6+99hoAs2fPpnTp0qxcuZJ27drRunXCAzZLliyhXbt2GY7RHZL4uemsIDw8XFPqYErNwYOR9OmzgO+/38HIkc0YMqRx+jslt+JfsHK4c/m5/VAglc5uYxLZvn17mk+tmIz74IMPyJ8/P88++6yvQ/Gq6OhomjRpwvLlywkKurYHIaXvPRFZp6rhN3K+bHtHERvrYMyYlVSrNpHvv99BvnzBFClyA+15H4YmJAmA/GUzLUZjTMb07NmTkJAQX4fhdQcOHGDUqFEpJglPyJad2atWRdCjx1w2bToGwMMPV2Ps2DaUKlXg+g7kiIW46IT1p9bbm9jG+JHQ0FC6dOni6zC8rnLlylSuXNlr58t2iWL16ggaNvwEVShfvhATJtxHu3a33tjBZt+fsNw/zjqxzXVL/nKVMZ7mie6EbJco6tYtRevWlahT5yaGDGlMnjw3OHnHrm9h/wLXiliSMNctNDSUU6dOERYWZsnCeMXV+SjSeqz4RmT5RLF79yn69fuJMWNac+utzh/IefOeICAgAz+Y6oD/PZKw3v1QxgM1OU7p0qWJiIjI9LkBjEnL1RnuMlOWTRTR0bGMGrWct99eTnR0HKGhQXzzzWMAGUsScVfgs0TPJj/2K+RL+/E+Y1KSK1euTJ1lzBhf8WiiEJE2wFggEPhYVUcl2x4CfA7cCZwCOqnq/vSOu3jxXnr1ms+uXacA6Nq1Nu++m8GhCWIuw3/uglNbk5aXaZKx4xpjTBbnsfcoRCQQ2AW0BCKANUBnVd2WqE4voJaq9hCRx4EHVbVTigd0CStcXk+f7QpAtcp5mTKqOo3ru8aXUQU04f+r09AlL7/6ma+cg0PLnf+O/pH0RIWrwBMrITT9sWuMMcbfZeQ9Ck/eUdQF9qjqXgARmQV0BLYlqtMRGO5a/gaYICKiaWSvM2evEBoUw+stf+PlJisJ3hcH+zIx6pCC0O0vyB2Wfl1jjMkBPHlH8QjQRlWfda13Aeqpau9Edba46kS41v9y1TmZ7FjPA8+7VmsAWzwSdNZTFDiZbq2cwa5FArsWCexaJKiiqvlvZMcs0ZmtqtOAaQAisvZGb5+yG7sWCexaJLBrkcCuRQIRcX/so2Q8+XLAIaBMovXSrrIU64hIEFAQZ6e2McYYP+HJRLEGqCwiFUQkGHgcmJOszhzgH67lR4Bf0uqfMMYY430ea3pS1VgR6Q38hPPx2OmqulVERgBrVXUO8AnwbxHZA5zGmUzSM81TMWdBdi0S2LVIYNcigV2LBDd8LbLcMOPGGGO8ywYwMsYYkyZLFMYYY9Lkt4lCRNqIyE4R2SMig1LYHiIiX7m2rxaR8t6P0jvcuBb9RWSbiGwWkcUikm2n4EvvWiSq97CIqIhk20cj3bkWIvKY63tjq4h84e0YvcWNn5GyIrJERDa4fk7apnScrE5EpovIcdc7ailtFxEZ57pOm0XkDrcOrKp+9w9n5/dfQEUgGNgEVE9WpxcwxbX8OPCVr+P24bVoBuRxLffMydfCVS8/sBRYBYT7Om4ffl9UBjYAhV3rxX0dtw+vxTSgp2u5OrDf13F76Fo0Bu4AtqSyvS3wIyBAfWC1O8f11zuK+OE/VPUKcHX4j8Q6Ap+5lr8BWkj2HPQ/3WuhqktU9ZJrdRXOd1ayI3e+LwBGAu8AUd4MzsvcuRbPARNV9QyAqh73coze4s61UODqFJcFgcNejM9rVHUpzidIU9MR+FydVgGFRCTd4bH9NVGUAg4mWo9wlaVYR1VjgUggOw7Q5M61SKwbzr8YsqN0r4XrVrqMqs7zZmA+4M73xa3ArSLyu4isco3mnB25cy2GA0+JSAQwH3jRO6H5nev9fQJkkSE8jHtE5CkgHMiRY6OLSAAwBnjGx6H4iyCczU9Ncd5lLhWRmqp61qdR+UZnYIaqvi8iDXC+v1VDVR2+Diwr8Nc7Chv+I4E71wIRuRcYDHRQ1WgvxeZt6V2L/DgHjfxVRPbjbIOdk007tN35vogA5qhqjKruwznsf2UvxedN7lyLbsB/AVR1JRCKc8DAnMat3yfJ+WuisOE/EqR7LUSkDjAVZ5LIru3QkM61UNVIVS2qquVVtTzO/poOqnrDg6H5MXd+Rr7HeTeBiBTF2RS115tBeok71+IA0AJARKrhTBQ5cY7aOcDTrqef6gORqnokvZ38sulJPTf8R5bj5rV4D8gHfO3qzz+gqh18FrSHuHktcgQ3r8VPQCsR2QbEAQNUNdvddbt5LV4GPhKRfjg7tp/Jjn9YisiXOP84KOrqjxkG5AJQ1Sk4+2faAnuAS0BXt46bDa+VMcaYTOSvTU/GGGP8hCUKY4wxabJEYYwxJk2WKIwxxqTJEoUxxpg0WaIw1ZfOXQAABDBJREFUfklE4kRkY6J/5dOoeyETzjdDRPa5zrXe9fbu9R7jYxGp7lp+Ldm2FRmN0XWcq9dli4j8T0QKpVO/dnYdKdV4jz0ea/ySiFxQ1XyZXTeNY8wA5qrqNyLSChitqrUycLwMx5TecUXkM+D/2zu7EKuqKI7//tioMzecHozIh0Awk6AwFF/EPihMEiJxYpAiBoJCzIhMfEgKZDJLCgyfUmQEJcPKXsSvYoYZyhxlZpzRPubF1z4eemhsAqHVw1oHT3o83iDQwfWDzdln373PXvteOOvsve/5r3Eze6emfheuoPvK/21LcuuQM4pkSiDp9oi1MSRpTNJVqrGS7pbUX3riXhblyyWdjLYHJV3vBt4PzIu2r8e1zkl6Lcoakg5LOhvlnVHeJ2mxpG1Aa9ixPz6biOMBSStLNvdI6pA0TdJ2SacjTsDLTXwtJwlBN0lLYozDkr6VdF+8pbwF6AxbOsP2PZIGo26V+m6S/JsbrZ+eKVNVwt8kHol0CFcRmBWfzcbfLC1mxBNx3AC8GflpuPbTbPzG34jyTcBbFf31AB2RfxY4BSwCxoAG/ub7eeAhYDWwq9S2PY59RPyLwqZSncLGVcDeyE/HlTxbgZeAzVE+AzgDzK2wc6I0voPAijifBdwW+SeAzyPfBewstd8KPB/5O3D9p8aN/r0z3dzpppTwSBJg0swWFieSWoCtkh4G/safpO8Cfi61OQ3sibpfmtmIpEfwQDXfhLzJdPxJvIrtkjbjGkAv4tpAh8zsYtjwBbAMOAp8IOk9fLlq4D+M6wiwQ9IMYAXQb2aTsdz1oKSOqNeOC/hduKJ9q6SRGP8PwIlS/b2S7sUlKlqu0f9y4GlJb8T5TOCeuFaSVJKOIpkqPAfcCSwys0tyddiZ5Qpm1h+OZCXQI+lD4HfghJmtaaKPjWb2WXEi6fGqSmY2Lo978RTQLelrM9vSzCDM7C9JfcCTQCceZAc84th6Mzt2nUtMmtlCSW24ttE64CM8WFOvma2Kjf++a7QXsNrMfmrG3iSB3KNIpg7twK/hJB4DrooLLo8V/ouZ7QJ24yEhvwOWSir2HBqS5jfZ5wDwjKQ2SQ182WhA0hzgTzPbhwsyVsUdvhQzmyo+xcXYitkJ+E1/bdFG0vzosxLziIavAht0WWa/kIvuKlX9A1+CKzgGrFdMr+TKw0lSSzqKZKqwH1gsaQx4Afixos6jwFlJw/jT+g4z+w2/cX4iaRRfdlrQTIdmNoTvXQziexa7zWwYeAAYjCWgt4HuiuYfA6PFZvYVHMeDS31lHroT3LF9DwxJOofLxtfO+MOWUTwoz/vAuzH2crte4P5iMxufebSEbefjPElqyb/HJkmSJLXkjCJJkiSpJR1FkiRJUks6iiRJkqSWdBRJkiRJLekokiRJklrSUSRJkiS1pKNIkiRJavkHEi+ovTG1eVAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "j3ZaChaLfQbD",
        "outputId": "5dddc6f0-83f0-4fdf-c862-8b5520dd1096"
      },
      "source": [
        "actuals, class_probabilities = x(model_ft, device, dataloaders['val'])\n",
        "\n",
        "print(actuals)\n",
        "print(class_probabilities)\n",
        "fpr, tpr, _ = roc_curve(actuals, class_probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC for class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1)]\n",
            "[tensor(0.0446), tensor(0.0639), tensor(0.1158), tensor(-0.0088), tensor(-0.0061), tensor(0.0428), tensor(0.0261), tensor(0.0759), tensor(0.0300), tensor(0.0252), tensor(0.0507), tensor(0.0312), tensor(0.0576), tensor(0.0796), tensor(0.0658), tensor(0.0471), tensor(-0.0115), tensor(0.0312), tensor(0.0065), tensor(0.0754), tensor(0.1099), tensor(-0.0570), tensor(0.0541), tensor(0.0641), tensor(0.0388), tensor(-0.0095), tensor(0.0200), tensor(0.0370), tensor(0.0569), tensor(-0.0206), tensor(0.0792), tensor(0.0417), tensor(-0.0506), tensor(0.1131), tensor(0.0041), tensor(-0.0510), tensor(0.0309), tensor(0.0393), tensor(0.0451), tensor(0.0442), tensor(0.0466), tensor(0.0477), tensor(0.0196), tensor(0.0234), tensor(0.0529), tensor(0.0136), tensor(-0.0486), tensor(0.0471), tensor(0.0440), tensor(0.0159), tensor(0.0421), tensor(0.0442), tensor(0.0116), tensor(0.0260), tensor(0.0128), tensor(0.0352), tensor(0.0597), tensor(0.0280), tensor(9.2996e-05), tensor(0.0256), tensor(0.0222), tensor(0.0410), tensor(0.0530), tensor(0.0322), tensor(0.0136), tensor(0.0246), tensor(0.0729), tensor(-0.0145), tensor(0.0420), tensor(-0.0392), tensor(0.0146), tensor(0.0253), tensor(0.0568), tensor(0.0401), tensor(-0.0088), tensor(0.0664), tensor(0.0245), tensor(0.0244), tensor(0.0804), tensor(0.0111), tensor(0.0536), tensor(-0.0028), tensor(0.0283), tensor(0.0375), tensor(0.0463), tensor(0.0451), tensor(0.0552), tensor(0.0414), tensor(0.0653), tensor(0.0215), tensor(0.0355), tensor(0.0052), tensor(0.0615), tensor(0.0414), tensor(0.0344), tensor(0.0294), tensor(-0.0036), tensor(0.0506), tensor(-0.0162), tensor(-0.0102), tensor(0.0049), tensor(0.0669), tensor(-0.0092), tensor(0.0017), tensor(0.0064), tensor(0.0197), tensor(0.0055), tensor(0.0700), tensor(0.0447), tensor(-0.0191), tensor(0.0399), tensor(0.0379), tensor(0.0070), tensor(0.0001), tensor(0.0329), tensor(0.0351), tensor(0.0193), tensor(-0.0077), tensor(0.0315), tensor(0.0319), tensor(0.0153), tensor(0.0729), tensor(0.0759), tensor(0.0579), tensor(0.0643), tensor(0.0385), tensor(0.0532), tensor(0.0452), tensor(0.0344), tensor(0.0206), tensor(-0.0151), tensor(0.0141), tensor(0.0168), tensor(0.0445), tensor(0.1039), tensor(0.0370), tensor(0.0499), tensor(-0.0248), tensor(0.0448), tensor(0.0455), tensor(0.0240), tensor(0.1183), tensor(0.0197), tensor(0.0372), tensor(0.0429), tensor(0.0564), tensor(0.1035), tensor(0.0469), tensor(0.0440), tensor(0.0853), tensor(0.0179), tensor(0.0354), tensor(0.0368), tensor(-0.0320), tensor(0.0492), tensor(0.0855), tensor(-0.0133), tensor(0.0518), tensor(0.0249), tensor(0.0777), tensor(0.0793), tensor(0.0598), tensor(0.0447), tensor(0.0427), tensor(0.0423), tensor(0.0237), tensor(0.0058), tensor(0.0202), tensor(0.1606), tensor(0.0324), tensor(0.0342), tensor(-0.0077), tensor(0.0587), tensor(-0.0499), tensor(0.1302), tensor(0.0506), tensor(0.0713), tensor(0.0385), tensor(0.0580), tensor(0.0289), tensor(0.0101), tensor(0.0728), tensor(-0.0022), tensor(0.0038), tensor(0.0497), tensor(0.0230), tensor(0.0254), tensor(0.0779), tensor(0.0408), tensor(-0.0039), tensor(0.0360), tensor(0.0429), tensor(0.0338), tensor(0.0404), tensor(-0.0135), tensor(-0.0036), tensor(0.0106), tensor(0.0935), tensor(0.0286), tensor(0.1153), tensor(0.0400), tensor(-0.0175), tensor(0.0501), tensor(0.0427), tensor(0.0417), tensor(0.0536), tensor(-0.0071), tensor(0.0224), tensor(0.0251), tensor(0.0295), tensor(0.0106), tensor(0.0554), tensor(0.0540), tensor(0.0518), tensor(-0.0195), tensor(0.0113), tensor(0.0771), tensor(0.0487), tensor(0.0019), tensor(0.0394), tensor(0.0353), tensor(0.0378), tensor(0.0181), tensor(0.0445), tensor(0.0153), tensor(-0.0072), tensor(0.0213), tensor(0.0326), tensor(-2.1607e-07), tensor(0.0223), tensor(0.0537), tensor(0.0383), tensor(-0.0253), tensor(-0.0200), tensor(0.0421), tensor(0.0295), tensor(0.0844), tensor(0.0933), tensor(-0.0086), tensor(0.0379), tensor(-0.0298), tensor(-0.0074), tensor(0.0208), tensor(0.0237), tensor(0.0195), tensor(0.0941), tensor(0.0233), tensor(0.0538), tensor(0.0044), tensor(0.0171), tensor(0.0548), tensor(0.0590), tensor(-0.0228), tensor(-0.0193), tensor(-0.0041), tensor(0.0215), tensor(0.0343), tensor(0.0504), tensor(0.0100), tensor(0.0342), tensor(0.0269), tensor(0.0101), tensor(0.0506), tensor(0.0350), tensor(0.0766), tensor(0.0545), tensor(0.0337), tensor(0.0225), tensor(0.1830), tensor(0.0058), tensor(0.0602), tensor(0.0869), tensor(0.0789), tensor(0.0374), tensor(-0.0187), tensor(0.0619), tensor(0.0243), tensor(0.0722), tensor(0.0640), tensor(0.0357), tensor(0.0669), tensor(0.0570), tensor(0.0264), tensor(0.0496), tensor(0.0883), tensor(0.0077), tensor(0.0448), tensor(0.0048), tensor(0.1134), tensor(0.0629), tensor(0.0522), tensor(0.0796), tensor(0.0309), tensor(0.0360), tensor(0.0860), tensor(-0.0152), tensor(-0.0326), tensor(0.0105), tensor(0.0564), tensor(0.0401), tensor(0.0664), tensor(0.0584), tensor(0.0522), tensor(0.0005), tensor(-0.0471), tensor(0.0459), tensor(0.0412), tensor(0.1257), tensor(0.0066), tensor(0.0225), tensor(-0.0027), tensor(0.0166), tensor(-0.0191), tensor(0.1020), tensor(-0.0216), tensor(0.0203), tensor(0.0422), tensor(0.0177), tensor(0.0255), tensor(0.0495), tensor(0.0522), tensor(-0.0336), tensor(0.0318), tensor(-0.0474), tensor(0.0564), tensor(0.0372), tensor(0.0812), tensor(0.0864), tensor(0.0014), tensor(0.0298), tensor(-0.0022), tensor(-0.0436), tensor(0.0228), tensor(0.0139), tensor(0.0313), tensor(0.0368), tensor(0.1805), tensor(0.0235), tensor(0.0593), tensor(0.0561), tensor(0.0817), tensor(0.0630), tensor(0.0811), tensor(0.0385), tensor(0.0937), tensor(8.6464e-06), tensor(0.0317), tensor(0.0635), tensor(-0.0006), tensor(0.0120), tensor(0.0127), tensor(0.0667), tensor(0.0463), tensor(0.0242), tensor(0.0508), tensor(0.0475), tensor(0.0889), tensor(0.0497), tensor(0.0834), tensor(0.0585), tensor(0.0374), tensor(0.0624), tensor(0.0402), tensor(0.0379), tensor(0.0162), tensor(0.0389), tensor(-0.0024), tensor(0.0088), tensor(0.0126), tensor(0.0112), tensor(0.0535), tensor(0.0622), tensor(0.0130), tensor(-0.0135), tensor(0.0554), tensor(0.0143), tensor(-0.0161), tensor(0.0579), tensor(0.0090), tensor(0.0391), tensor(0.0199), tensor(0.0709), tensor(0.0573), tensor(0.0067), tensor(0.0627), tensor(0.0118), tensor(0.0144), tensor(-0.0129), tensor(0.0324), tensor(0.0375), tensor(0.0466), tensor(0.0370), tensor(0.0200), tensor(0.0665), tensor(0.0565), tensor(0.0472), tensor(-0.0139), tensor(0.0826), tensor(0.0041), tensor(-0.0123), tensor(0.0464), tensor(0.0151), tensor(0.0041), tensor(0.0207), tensor(0.0419), tensor(0.0404), tensor(0.0327), tensor(0.0429), tensor(-0.0121), tensor(0.0329), tensor(0.0567), tensor(0.0344), tensor(0.0252), tensor(-0.0018), tensor(0.0320), tensor(0.0352), tensor(0.0174), tensor(0.0377), tensor(0.0262), tensor(0.0401), tensor(0.0248), tensor(0.0430), tensor(0.0425), tensor(0.0860), tensor(0.0597), tensor(0.0420), tensor(-0.0052), tensor(0.0129), tensor(0.0293), tensor(0.0234), tensor(0.0476), tensor(0.0133), tensor(0.0472), tensor(0.0541), tensor(0.0330), tensor(0.0520), tensor(0.0416), tensor(0.0207), tensor(-0.0251), tensor(0.0378), tensor(0.0013), tensor(0.0340), tensor(-0.0241), tensor(0.0758), tensor(0.0626), tensor(0.0216), tensor(0.0347), tensor(0.0226), tensor(0.0332), tensor(0.0490), tensor(-0.0550), tensor(-0.0265), tensor(0.0144), tensor(0.0440), tensor(0.0317), tensor(0.0452), tensor(0.0470), tensor(0.0129), tensor(0.0261), tensor(0.0501), tensor(0.0425), tensor(0.0422), tensor(0.0874), tensor(-0.0006), tensor(0.0995), tensor(0.0748), tensor(0.0886), tensor(0.0251), tensor(-0.0306), tensor(0.0223), tensor(0.2116), tensor(0.0550), tensor(0.0328), tensor(-0.0342), tensor(0.0693), tensor(0.1060), tensor(0.0495), tensor(0.0194), tensor(-0.0172), tensor(0.0805), tensor(0.0486), tensor(-0.0045), tensor(-0.0107), tensor(0.0095), tensor(0.0696), tensor(0.0357), tensor(-0.0247), tensor(0.0579), tensor(0.0149), tensor(-0.0266), tensor(0.0239), tensor(0.0259), tensor(0.0470), tensor(0.0408), tensor(-0.0529), tensor(0.1810), tensor(0.0407), tensor(0.0154), tensor(0.0098), tensor(-0.0268), tensor(0.0919), tensor(0.0042), tensor(0.0451), tensor(-0.0369), tensor(0.0257), tensor(0.0837), tensor(0.0379), tensor(0.0567), tensor(0.0202), tensor(0.0147), tensor(-0.0268), tensor(0.0630), tensor(0.0398), tensor(0.0734), tensor(0.0916), tensor(0.0154), tensor(0.1505), tensor(0.1156), tensor(0.0378), tensor(-0.0043), tensor(0.0248), tensor(0.0192), tensor(0.0343), tensor(0.0741), tensor(0.0437), tensor(0.0017), tensor(0.0406), tensor(0.0426), tensor(0.0105), tensor(0.0116), tensor(0.0123), tensor(0.0714), tensor(0.0525), tensor(0.0662), tensor(0.0280), tensor(0.0230), tensor(0.0258), tensor(0.0435), tensor(-0.0291), tensor(0.0388), tensor(0.0504), tensor(0.0176), tensor(0.0290), tensor(0.0131), tensor(0.0294), tensor(0.0675), tensor(0.0497), tensor(0.0191), tensor(0.0455), tensor(0.0387), tensor(0.0444), tensor(0.1294), tensor(0.0424), tensor(0.0011), tensor(0.0567), tensor(0.0351), tensor(0.0583), tensor(0.0328), tensor(0.0318), tensor(0.0115), tensor(0.0385), tensor(0.0012), tensor(-0.0126), tensor(0.0463), tensor(-0.0257), tensor(0.0186), tensor(0.0752), tensor(0.0299), tensor(0.0204), tensor(0.0963), tensor(0.0944), tensor(-0.0093), tensor(0.0413), tensor(0.0249), tensor(0.0260), tensor(0.0462), tensor(0.0076), tensor(-0.0107), tensor(0.0382), tensor(0.0056), tensor(0.0209), tensor(0.0636), tensor(0.1725), tensor(-0.0003), tensor(0.0466), tensor(0.0285), tensor(0.0222), tensor(0.0256), tensor(0.0246), tensor(0.0484), tensor(0.0439), tensor(0.0311), tensor(0.0059), tensor(0.0529), tensor(0.0362), tensor(0.0139), tensor(0.0388), tensor(0.1053), tensor(0.0551), tensor(0.0698), tensor(-0.0300), tensor(0.0377), tensor(0.0391), tensor(0.0073), tensor(0.0526), tensor(-0.0007), tensor(0.0138), tensor(0.0531), tensor(0.0664), tensor(0.0736), tensor(0.0804), tensor(0.0274), tensor(0.0258), tensor(0.0636), tensor(0.0433), tensor(0.0575), tensor(-0.0102), tensor(0.0479), tensor(-0.0145), tensor(0.0361), tensor(0.0247), tensor(0.0152), tensor(0.0800), tensor(0.0481), tensor(0.0284), tensor(0.0255), tensor(-0.0033), tensor(0.0839), tensor(-0.0123), tensor(0.0224), tensor(0.0463), tensor(0.0530), tensor(0.0205), tensor(-0.0361), tensor(0.0527), tensor(0.0413), tensor(0.0083), tensor(0.0363), tensor(0.0506), tensor(-0.0143), tensor(0.0535), tensor(0.0721), tensor(0.0379), tensor(0.0306), tensor(0.0162), tensor(0.0516), tensor(0.0060), tensor(0.0048), tensor(0.0090), tensor(0.0401), tensor(0.0128), tensor(0.0119), tensor(-0.0072), tensor(0.0485), tensor(0.0719), tensor(0.0032), tensor(0.0399), tensor(0.0033), tensor(-0.0221), tensor(0.0445), tensor(0.0046), tensor(0.0138), tensor(0.0223), tensor(0.0530), tensor(0.0287), tensor(-0.0447), tensor(0.1199), tensor(0.0225), tensor(-0.0237), tensor(0.1005), tensor(-0.0026), tensor(0.0110), tensor(0.0254), tensor(0.0235), tensor(0.0381), tensor(-0.0043), tensor(0.0326), tensor(0.0626), tensor(-0.0118), tensor(-0.0121), tensor(-0.0049), tensor(0.0134), tensor(0.0384), tensor(0.0515), tensor(0.0482), tensor(0.0343), tensor(-0.0005), tensor(0.0914), tensor(-0.0113), tensor(0.0237), tensor(0.0466), tensor(0.0465), tensor(-0.0205), tensor(-0.0048), tensor(0.0028), tensor(0.0846), tensor(-0.0128), tensor(0.0058), tensor(0.0238), tensor(0.0323), tensor(0.0268), tensor(0.0110), tensor(0.0489), tensor(0.0054), tensor(-0.0156), tensor(0.0200), tensor(0.0905), tensor(0.0468), tensor(-0.0013), tensor(0.0503), tensor(0.0996), tensor(0.0349), tensor(0.0548), tensor(0.0302), tensor(0.0629), tensor(0.0227), tensor(-0.0326), tensor(0.0310), tensor(0.0393), tensor(0.0576), tensor(-0.0014), tensor(0.0543), tensor(0.0080), tensor(0.0991), tensor(0.0429), tensor(0.0531), tensor(0.0215), tensor(0.0699), tensor(-0.0025), tensor(0.0698), tensor(0.1064), tensor(0.0243), tensor(0.0255), tensor(-0.0062), tensor(0.0246), tensor(0.0279), tensor(0.0537), tensor(0.0859), tensor(0.0369), tensor(0.0036), tensor(0.0027), tensor(0.0352), tensor(0.0262), tensor(0.0879), tensor(0.0377), tensor(0.0252), tensor(0.1023), tensor(0.0230), tensor(0.0207), tensor(0.0540), tensor(0.0344), tensor(0.0661), tensor(0.0625), tensor(0.1194), tensor(0.0515), tensor(0.0429), tensor(0.0431), tensor(-0.0190), tensor(0.0069), tensor(-0.0125), tensor(0.0689), tensor(0.0650), tensor(0.0325), tensor(0.0290), tensor(0.0274), tensor(0.0034), tensor(0.0487), tensor(0.0645), tensor(-0.0115), tensor(0.0166), tensor(0.0615), tensor(0.0935), tensor(-0.0037), tensor(0.1178), tensor(0.0226), tensor(-0.0323), tensor(0.0179), tensor(0.0396), tensor(0.1196), tensor(0.0010)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "rSqz54r925E-",
        "outputId": "127eddf1-1fc1-44ab-a74c-00792395a2a1"
      },
      "source": [
        "actuals, class_probabilities = x(model_ft, device, dataloaders['test'])\n",
        "\n",
        "print(actuals)\n",
        "print(class_probabilities)\n",
        "fpr, tpr, _ = roc_curve(actuals, class_probabilities)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(fpr, tpr, color='darkorange',\n",
        "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC for class')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(0), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(0), tensor(0), tensor(1), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0), tensor(1), tensor(0), tensor(1), tensor(0), tensor(0), tensor(1), tensor(1), tensor(1), tensor(1), tensor(0)]\n",
            "[tensor(0.0071), tensor(0.0144), tensor(0.0927), tensor(0.0265), tensor(0.0355), tensor(0.0096), tensor(0.0206), tensor(-0.0306), tensor(-0.0197), tensor(0.0167), tensor(-0.0221), tensor(-0.0008), tensor(0.0354), tensor(0.0238), tensor(0.0306), tensor(0.0480), tensor(-0.0307), tensor(0.0168), tensor(0.0314), tensor(0.0138), tensor(0.0422), tensor(0.0375), tensor(0.0076), tensor(0.0207), tensor(-0.0148), tensor(0.0229), tensor(-0.0072), tensor(0.0013), tensor(0.0398), tensor(0.0563), tensor(-0.0150), tensor(0.0110), tensor(0.0499), tensor(0.0273), tensor(0.0514), tensor(-0.0190), tensor(0.0492), tensor(0.0058), tensor(0.0404), tensor(0.0056), tensor(0.0319), tensor(0.0071), tensor(-0.0165), tensor(0.0322), tensor(0.0260), tensor(0.0657), tensor(0.0690), tensor(0.0707), tensor(-0.0153), tensor(0.0280), tensor(0.0863), tensor(0.0465), tensor(0.0428), tensor(0.0067), tensor(0.0603), tensor(-0.0187), tensor(0.0588), tensor(-0.0169), tensor(0.0014), tensor(0.0513), tensor(0.0128), tensor(0.0150), tensor(0.0525), tensor(0.0111), tensor(0.0489), tensor(0.0262), tensor(0.0204), tensor(0.0306), tensor(0.0973), tensor(-0.0124), tensor(0.0107), tensor(0.0616), tensor(0.0143), tensor(0.0606), tensor(0.0660), tensor(0.0141), tensor(0.0072), tensor(-0.0063), tensor(-0.0122), tensor(0.0439), tensor(0.0068), tensor(0.0011), tensor(-0.0122), tensor(0.0654), tensor(0.1659), tensor(0.0345), tensor(0.0595), tensor(0.0242), tensor(-0.0025), tensor(0.0851), tensor(0.0335), tensor(0.1440), tensor(0.0686), tensor(0.0307), tensor(0.0564), tensor(0.0248), tensor(0.0596), tensor(-0.0113), tensor(0.0056), tensor(0.0352), tensor(0.0240), tensor(0.0077), tensor(0.0796), tensor(0.0264), tensor(0.0230), tensor(0.0132), tensor(-0.0021), tensor(0.0280), tensor(0.0781), tensor(0.0334), tensor(0.0344), tensor(-0.0025), tensor(0.0092), tensor(0.1234), tensor(0.0176), tensor(-0.0546), tensor(0.0204), tensor(0.0120), tensor(-0.0360), tensor(-0.0127), tensor(0.0788), tensor(0.0980), tensor(0.0470), tensor(-0.0273), tensor(0.0291), tensor(0.0825), tensor(0.0595), tensor(0.1559), tensor(0.0116), tensor(-0.0162), tensor(0.0214), tensor(0.0685), tensor(0.0382), tensor(0.0345), tensor(-0.0221), tensor(0.1765), tensor(0.0290), tensor(0.0571), tensor(0.0385), tensor(0.0062), tensor(0.0530), tensor(0.0021), tensor(0.0511), tensor(0.0488), tensor(0.0468), tensor(0.0315), tensor(0.0440), tensor(0.0003), tensor(0.0152), tensor(0.0038), tensor(-0.0502), tensor(0.0877), tensor(0.0588), tensor(0.0507), tensor(0.0362), tensor(0.0060), tensor(0.0660), tensor(0.0845), tensor(0.0119), tensor(0.0060), tensor(0.0770), tensor(-0.0005), tensor(-0.0227), tensor(-0.0068), tensor(0.0251), tensor(0.0490), tensor(0.0425), tensor(0.0928), tensor(0.0650), tensor(0.0256), tensor(-0.0134), tensor(0.0095), tensor(-0.0280), tensor(0.0297), tensor(-0.0121), tensor(0.0388), tensor(0.0445), tensor(0.0097), tensor(0.1644), tensor(0.0151), tensor(0.0640), tensor(0.0206), tensor(-0.0119), tensor(0.0678), tensor(0.0383), tensor(0.0596), tensor(0.0436), tensor(-0.0002), tensor(-0.0358), tensor(0.0221), tensor(-0.0232), tensor(0.0115), tensor(0.0689), tensor(0.0505), tensor(0.0813), tensor(-0.0036), tensor(-0.0108), tensor(0.0386), tensor(0.0274), tensor(0.1189), tensor(0.0047), tensor(0.0275), tensor(0.0613), tensor(0.0281), tensor(0.1060), tensor(-0.0075), tensor(0.0619), tensor(0.0214), tensor(0.1105), tensor(0.0070), tensor(0.0805), tensor(-0.0388), tensor(-0.0008), tensor(-0.0073), tensor(0.0186), tensor(0.0057), tensor(0.0400), tensor(0.0179), tensor(0.0375), tensor(0.0012), tensor(-0.0269), tensor(0.0529), tensor(0.0844), tensor(0.0466), tensor(0.0080), tensor(0.0253), tensor(0.0202), tensor(0.0264), tensor(0.0464), tensor(0.0944), tensor(0.1605), tensor(0.0414), tensor(0.0125), tensor(0.0185), tensor(0.1079), tensor(0.0362), tensor(-0.0252), tensor(-0.0132), tensor(0.0276), tensor(0.0160), tensor(-0.0153), tensor(0.0536), tensor(0.0024), tensor(-0.0154), tensor(0.0070), tensor(0.0736), tensor(0.0278), tensor(0.0350), tensor(-0.0156), tensor(-0.0110), tensor(0.0810), tensor(0.0262), tensor(-0.0035), tensor(-0.0434), tensor(0.0003), tensor(0.0177), tensor(0.0844), tensor(0.0172), tensor(0.0594), tensor(0.0224), tensor(-0.0062), tensor(0.0130), tensor(0.0175), tensor(0.0477), tensor(0.0023), tensor(0.0696), tensor(0.0195), tensor(0.0732), tensor(0.0252), tensor(0.0463), tensor(-0.0096), tensor(0.0330), tensor(-0.0181), tensor(0.0500), tensor(0.0626), tensor(0.0283), tensor(0.0324), tensor(0.0852), tensor(-0.0034), tensor(0.0226), tensor(0.1427), tensor(0.0066), tensor(-0.0014), tensor(0.0577), tensor(0.0185), tensor(0.0338), tensor(0.0470), tensor(-0.0077), tensor(-0.0062), tensor(-0.0393), tensor(0.0705), tensor(-0.0397), tensor(0.0237), tensor(0.0053), tensor(0.0095), tensor(0.0535), tensor(0.0016), tensor(0.0163), tensor(0.0078), tensor(0.0650), tensor(-0.0581), tensor(-0.0215), tensor(0.0308), tensor(-0.0012), tensor(0.0436), tensor(0.0219), tensor(0.0048), tensor(-0.0315), tensor(0.0224), tensor(0.0032), tensor(0.1437), tensor(0.0382), tensor(-0.0117), tensor(0.0089), tensor(0.0413), tensor(0.0012), tensor(0.0707), tensor(0.0026), tensor(0.0195), tensor(0.0226), tensor(0.0231), tensor(-0.0233), tensor(0.2049), tensor(0.0253), tensor(0.0486), tensor(0.0018), tensor(0.0539), tensor(0.0132), tensor(0.0395), tensor(-0.0085), tensor(-0.0161), tensor(0.0147), tensor(-0.0250), tensor(0.0236), tensor(0.0622), tensor(0.0231), tensor(0.0542), tensor(-0.0021), tensor(0.0127), tensor(0.0096), tensor(-0.0018), tensor(0.0700), tensor(0.0772), tensor(-0.0198), tensor(-0.0104), tensor(0.0761), tensor(0.0031), tensor(-0.0062), tensor(0.0541), tensor(-0.0128), tensor(0.0045), tensor(0.1059), tensor(0.0321), tensor(-0.0161), tensor(0.0101), tensor(0.0967), tensor(0.1260), tensor(0.0454), tensor(0.0901), tensor(0.0586), tensor(-0.0268), tensor(0.0876), tensor(0.0297), tensor(0.0648), tensor(-0.0241), tensor(0.0826), tensor(0.0497), tensor(0.0724), tensor(0.0304), tensor(-0.0097), tensor(0.0229), tensor(-0.0033), tensor(0.0059), tensor(0.0335), tensor(0.0412), tensor(0.0181), tensor(0.0397), tensor(0.0120), tensor(0.0534), tensor(0.0384), tensor(0.0876), tensor(0.0079), tensor(0.0387), tensor(0.0298), tensor(0.0632), tensor(0.0768), tensor(-0.0347), tensor(0.0559), tensor(0.0773), tensor(0.0756), tensor(0.0299), tensor(0.0097), tensor(0.1224), tensor(0.0252), tensor(0.0245), tensor(-0.0099), tensor(0.0511), tensor(0.0570), tensor(0.0016), tensor(0.0180), tensor(0.0396), tensor(0.0697), tensor(0.0227), tensor(0.0743), tensor(0.1400), tensor(-0.0100), tensor(0.0269), tensor(0.0070), tensor(0.0620), tensor(0.0056), tensor(0.0370), tensor(0.0042), tensor(0.0702), tensor(0.0064), tensor(-0.0051), tensor(0.0237), tensor(0.0595), tensor(0.0783), tensor(0.0717), tensor(0.0260), tensor(0.0301), tensor(-0.0036), tensor(-0.0016), tensor(0.0069), tensor(0.0690), tensor(0.0762), tensor(0.0608), tensor(0.0316), tensor(0.0136), tensor(-0.0270), tensor(-0.0152), tensor(0.0030), tensor(0.0147), tensor(0.0664), tensor(0.0206), tensor(0.0695), tensor(-0.0035), tensor(0.0313), tensor(0.0393), tensor(0.0044), tensor(-0.0004), tensor(0.0423), tensor(0.0064), tensor(0.0314), tensor(0.0191), tensor(-0.0060), tensor(0.0467), tensor(0.0992), tensor(0.0121), tensor(-0.0936), tensor(0.0149), tensor(-0.0081), tensor(0.0193), tensor(-0.0066), tensor(-0.0055), tensor(-0.0087), tensor(0.0035), tensor(0.0138), tensor(0.0888), tensor(0.0197), tensor(-0.0050), tensor(-0.0065), tensor(0.0157), tensor(0.0402), tensor(0.0080), tensor(-0.0264), tensor(0.0736), tensor(0.0509), tensor(0.0218), tensor(0.0620), tensor(0.0788), tensor(0.0156), tensor(0.1301), tensor(0.0118), tensor(0.0366), tensor(0.0167), tensor(0.0907), tensor(0.0123), tensor(0.0757), tensor(0.0571), tensor(0.0057), tensor(0.0261), tensor(-0.0175), tensor(0.1368), tensor(0.0101), tensor(0.0265), tensor(0.0187), tensor(-0.0105), tensor(0.0229), tensor(-0.0003), tensor(-0.0061), tensor(-0.0194), tensor(0.0234), tensor(0.0492), tensor(0.0675), tensor(0.0197), tensor(0.0166), tensor(0.0678), tensor(-0.0161), tensor(0.0074), tensor(0.0338), tensor(0.0593), tensor(0.0480), tensor(0.0340), tensor(0.0211), tensor(0.0274), tensor(-0.0145), tensor(0.0097), tensor(-0.0042), tensor(0.0390), tensor(0.0037), tensor(0.0547), tensor(0.1639), tensor(0.0375), tensor(0.0257), tensor(-0.0166), tensor(0.0054), tensor(0.0353), tensor(0.0296), tensor(0.0393), tensor(0.0484), tensor(0.0484), tensor(-0.0199), tensor(0.0003), tensor(0.0549), tensor(0.0336), tensor(0.0109), tensor(0.0277), tensor(0.0085), tensor(0.0230), tensor(0.0229), tensor(0.0093), tensor(0.0505), tensor(-0.0258), tensor(0.0292), tensor(0.0453), tensor(0.0160), tensor(0.0578), tensor(-0.0096), tensor(0.0240), tensor(0.0010), tensor(-0.0088), tensor(-0.0208), tensor(-0.0120), tensor(-0.0070), tensor(0.0665), tensor(-0.0051), tensor(0.0240), tensor(-0.0276), tensor(0.1134), tensor(-0.0071), tensor(0.0891), tensor(0.0207), tensor(0.0006), tensor(0.1381), tensor(0.0585), tensor(0.0337), tensor(0.0493), tensor(0.0060), tensor(0.0204), tensor(0.1133), tensor(0.0017), tensor(0.0152), tensor(0.0409), tensor(0.0265), tensor(0.0201), tensor(0.0175), tensor(0.0888), tensor(-0.0301), tensor(0.0084), tensor(0.0026), tensor(-0.0027), tensor(0.0075), tensor(0.1268), tensor(0.0213), tensor(0.1582), tensor(0.0631), tensor(-0.0058), tensor(0.0006), tensor(-0.0024), tensor(0.0361), tensor(-0.0084), tensor(0.0269), tensor(0.0793), tensor(0.0242), tensor(0.0535), tensor(-0.0202), tensor(0.0176), tensor(0.0136), tensor(-0.0245), tensor(0.0879), tensor(-0.0228), tensor(0.1220), tensor(0.0007), tensor(0.0137), tensor(0.0895), tensor(0.0192), tensor(0.0045), tensor(-0.0131), tensor(0.0044), tensor(0.0347), tensor(-0.0234), tensor(0.0888), tensor(0.0667), tensor(0.0144), tensor(0.0777), tensor(0.0380), tensor(0.0149), tensor(0.0208), tensor(0.0568), tensor(0.0275), tensor(0.0031), tensor(0.0295), tensor(0.0131), tensor(0.0088), tensor(0.0298), tensor(0.0046), tensor(0.0031), tensor(0.0222), tensor(0.0587), tensor(-0.0074), tensor(0.0439), tensor(0.0695), tensor(0.0036), tensor(0.0113), tensor(0.0059), tensor(0.0381), tensor(0.0193), tensor(0.0708), tensor(0.0180), tensor(0.0066), tensor(0.0777), tensor(0.0561), tensor(-0.0067), tensor(0.0170), tensor(0.0192), tensor(0.0307), tensor(0.0148), tensor(0.0610), tensor(0.0300), tensor(-0.0306), tensor(0.0753), tensor(0.0531), tensor(0.0228), tensor(0.0062), tensor(0.0393), tensor(0.0059), tensor(0.0347), tensor(0.0524), tensor(0.0050), tensor(-0.0218), tensor(0.0612), tensor(0.0563), tensor(-0.0102), tensor(0.0178), tensor(0.0057), tensor(0.1153), tensor(-0.0419), tensor(0.0524), tensor(0.0908), tensor(0.0767), tensor(0.1957), tensor(0.0221), tensor(-0.0061), tensor(-0.0101), tensor(0.0082), tensor(0.0487), tensor(0.0739), tensor(0.0149), tensor(0.0121), tensor(0.0373), tensor(0.0677), tensor(-0.0132), tensor(0.0868), tensor(0.0229), tensor(0.0002), tensor(-0.0041), tensor(-0.0088), tensor(-0.0039), tensor(-0.0138), tensor(0.0968), tensor(-0.0026), tensor(0.0558), tensor(0.0882), tensor(0.0258), tensor(0.0788), tensor(0.0575), tensor(0.0335), tensor(0.0916), tensor(0.0301), tensor(0.0357), tensor(0.0389), tensor(0.0031), tensor(0.0304), tensor(0.0370), tensor(-0.0029), tensor(-0.0212), tensor(0.0284), tensor(-0.0137), tensor(0.0203), tensor(0.0502), tensor(0.0253), tensor(-0.0324), tensor(0.0563), tensor(0.0304), tensor(0.0066), tensor(0.0767), tensor(0.0511), tensor(0.0120), tensor(-0.0120), tensor(-0.0262), tensor(0.0076), tensor(0.0788), tensor(-0.0078), tensor(0.0859), tensor(-0.0056), tensor(0.0822), tensor(-0.0409), tensor(0.0022), tensor(0.0506), tensor(0.0318), tensor(-0.0128), tensor(0.0071), tensor(0.0223), tensor(0.0339), tensor(0.0401), tensor(0.0195), tensor(-0.0058), tensor(0.0580), tensor(0.0393), tensor(0.0198), tensor(-0.0404), tensor(0.0223), tensor(0.0116), tensor(0.0529), tensor(0.0449), tensor(-0.0165), tensor(0.0754), tensor(0.0571), tensor(0.0429), tensor(-0.0180), tensor(0.0612), tensor(0.0241), tensor(0.0520), tensor(0.0401), tensor(0.0059), tensor(0.0069), tensor(0.0320), tensor(0.0435), tensor(-0.0086), tensor(0.1972), tensor(0.0348), tensor(-0.0180), tensor(0.0834), tensor(-0.0203), tensor(0.1145), tensor(-0.0218), tensor(0.0520), tensor(0.0931), tensor(0.0519), tensor(0.0399), tensor(-0.0038), tensor(0.0139), tensor(0.0410), tensor(0.0732), tensor(0.0184), tensor(0.0472), tensor(0.0647), tensor(0.0115), tensor(0.0493), tensor(0.0613), tensor(-0.0045), tensor(0.0557), tensor(0.0001), tensor(-0.0079), tensor(0.0020), tensor(-0.0266), tensor(0.0383), tensor(0.0116), tensor(0.1212), tensor(-0.0125), tensor(0.0602), tensor(0.0629), tensor(0.0028), tensor(0.0500), tensor(0.0286), tensor(0.0530), tensor(0.0523), tensor(0.0160), tensor(-0.0151), tensor(0.0064), tensor(0.0349), tensor(0.0636), tensor(0.0483), tensor(0.1611), tensor(-0.0064), tensor(-0.0118), tensor(0.1474), tensor(0.0015), tensor(0.0090), tensor(0.0475), tensor(0.0949), tensor(0.0047), tensor(-0.0114), tensor(0.0485), tensor(0.0235), tensor(-0.0098), tensor(-0.0253), tensor(0.0970), tensor(0.0074), tensor(0.0864), tensor(-0.0189), tensor(-0.0127), tensor(0.0619), tensor(0.1593), tensor(0.0326), tensor(0.0163), tensor(0.0401), tensor(0.0356), tensor(-0.0035), tensor(-0.0029), tensor(-0.0113), tensor(0.0886), tensor(0.0282), tensor(0.1004), tensor(-0.0061), tensor(0.0009), tensor(0.0405), tensor(-0.0131), tensor(0.0554), tensor(0.0050), tensor(0.0651), tensor(-0.0070), tensor(0.0491), tensor(0.0099), tensor(-0.0032), tensor(0.0208), tensor(0.0672), tensor(0.0648), tensor(0.0875), tensor(0.0292), tensor(0.0518), tensor(0.0652), tensor(0.1249), tensor(0.0569), tensor(0.0992), tensor(-0.0225), tensor(0.0109), tensor(0.0636), tensor(0.0547), tensor(0.0186), tensor(-0.0344), tensor(0.0926), tensor(-0.0319), tensor(-0.0073), tensor(0.0155), tensor(0.0442), tensor(-0.0075), tensor(0.0759), tensor(0.0184), tensor(-0.0224), tensor(0.0448), tensor(0.0323), tensor(0.0570), tensor(0.0510), tensor(0.0199), tensor(0.0103), tensor(0.0422), tensor(-0.0077), tensor(-0.0435), tensor(0.0310), tensor(0.0275), tensor(0.0502), tensor(0.0220), tensor(0.0255), tensor(0.0644), tensor(0.0066), tensor(0.0401), tensor(-0.0288), tensor(-0.0201), tensor(0.0222), tensor(0.0214), tensor(0.0036), tensor(0.0473), tensor(0.0746), tensor(0.0924), tensor(-0.0306), tensor(-0.0025), tensor(-0.0152), tensor(0.0386), tensor(-0.0162), tensor(0.0843), tensor(0.0970), tensor(0.0002), tensor(0.0144), tensor(-0.0016), tensor(0.0522), tensor(0.0019), tensor(0.0902), tensor(0.0420), tensor(0.0866), tensor(-0.0420), tensor(0.0132), tensor(-0.0042), tensor(-6.0389e-05), tensor(0.0457), tensor(-0.0013), tensor(-0.0214), tensor(0.0158), tensor(-0.0193), tensor(0.1038), tensor(0.0224), tensor(-0.0205), tensor(0.0115), tensor(0.0247), tensor(0.0740), tensor(-0.0273), tensor(-0.0027), tensor(0.0146), tensor(0.0192), tensor(0.0155), tensor(-0.0214), tensor(0.2139), tensor(0.0289), tensor(0.0350), tensor(0.0161), tensor(0.0082), tensor(0.0237), tensor(0.0533), tensor(0.0273), tensor(0.0716), tensor(0.1086), tensor(0.0403), tensor(0.0894), tensor(0.1042), tensor(0.0409), tensor(0.0915), tensor(0.0297), tensor(0.0053), tensor(-0.0447), tensor(0.0134), tensor(0.0506), tensor(0.0921), tensor(-0.0033), tensor(0.0475), tensor(0.0221), tensor(0.0764), tensor(0.0588), tensor(0.1029), tensor(0.0220), tensor(-0.0024), tensor(0.0144), tensor(0.0734), tensor(0.0180), tensor(0.0941), tensor(0.0191), tensor(-0.0311), tensor(0.0277), tensor(-0.0040), tensor(0.0174), tensor(0.0468), tensor(-0.0143), tensor(0.0172), tensor(-0.0184), tensor(0.0064), tensor(0.0293), tensor(0.0607), tensor(0.1192), tensor(0.0477), tensor(-0.0026), tensor(0.0489), tensor(0.0032), tensor(-0.0115), tensor(-0.0426), tensor(-0.0040), tensor(0.0217), tensor(0.0121), tensor(0.0877), tensor(-0.0004), tensor(0.0264), tensor(0.0669), tensor(0.0272), tensor(0.0066), tensor(0.0470), tensor(0.0256), tensor(0.0353), tensor(-0.0484), tensor(-0.0163), tensor(0.0375), tensor(-0.0191), tensor(0.0302), tensor(0.0309), tensor(-0.0188), tensor(-0.0139), tensor(0.0312), tensor(0.0450), tensor(-0.0295), tensor(0.0331), tensor(-0.0388), tensor(0.0039), tensor(0.0620), tensor(0.0385), tensor(0.0716), tensor(0.1204), tensor(-0.0350), tensor(0.1059), tensor(-0.0054), tensor(0.0358), tensor(0.0128), tensor(0.0294), tensor(0.0352), tensor(0.0773), tensor(0.0591), tensor(0.0276), tensor(0.0575), tensor(-0.0130), tensor(0.0431), tensor(0.0063), tensor(0.0293), tensor(0.0174), tensor(0.0407), tensor(0.0052), tensor(0.0194), tensor(-0.0031), tensor(0.0634), tensor(0.0462), tensor(0.0721), tensor(0.0278), tensor(0.0723), tensor(0.0766), tensor(0.0711), tensor(-0.0116), tensor(0.0380), tensor(-0.0012), tensor(0.0015), tensor(-0.0257), tensor(0.0838), tensor(-0.0090), tensor(-0.0251), tensor(0.0251), tensor(0.1570), tensor(0.1198), tensor(0.0439), tensor(-0.0074), tensor(0.0184), tensor(0.0529), tensor(0.0617), tensor(-0.0030), tensor(0.0175), tensor(0.0405), tensor(0.0592), tensor(0.0613), tensor(0.0574), tensor(0.0104), tensor(-0.0126), tensor(-0.0147), tensor(0.0589), tensor(0.0492), tensor(-0.0148), tensor(0.0646), tensor(-0.0018), tensor(0.0062), tensor(0.0110), tensor(0.0325), tensor(0.0062), tensor(0.0485), tensor(0.0288), tensor(0.0413), tensor(0.0542), tensor(0.0354), tensor(0.0657), tensor(-0.0241), tensor(-0.0024), tensor(0.1070), tensor(0.0169), tensor(0.0471), tensor(0.0234), tensor(-0.0102), tensor(-0.0128), tensor(0.0166), tensor(-0.0402), tensor(0.0074), tensor(0.0247), tensor(0.1446), tensor(0.0660), tensor(0.0015), tensor(-0.0149), tensor(0.0263), tensor(0.0225), tensor(-0.0744), tensor(0.0254), tensor(0.0240), tensor(0.0565), tensor(0.0137), tensor(-0.0093), tensor(0.0686), tensor(0.0188), tensor(0.0193), tensor(0.0804), tensor(-0.0300), tensor(0.0196), tensor(-0.0012), tensor(0.0180), tensor(-0.0042), tensor(0.0331), tensor(0.0066), tensor(-0.0007), tensor(0.0405), tensor(0.0663), tensor(0.0477), tensor(0.0624), tensor(0.0043), tensor(0.0741), tensor(0.0428), tensor(-0.0001), tensor(0.0326), tensor(0.0587), tensor(0.0871), tensor(-0.0004), tensor(0.0477), tensor(0.0389), tensor(0.0190), tensor(0.0078), tensor(0.0788), tensor(-0.0720), tensor(0.0007), tensor(0.0014), tensor(0.0659), tensor(0.0026), tensor(0.0481), tensor(0.0090), tensor(0.0012), tensor(0.0188), tensor(0.0265), tensor(-0.0076), tensor(0.0444), tensor(0.0349), tensor(0.0089), tensor(0.0027), tensor(-0.0288), tensor(0.0074), tensor(0.0003), tensor(-0.0057), tensor(0.0744), tensor(0.0755), tensor(0.1157), tensor(0.0107), tensor(0.0282), tensor(-0.0124), tensor(0.0320), tensor(0.0430), tensor(0.0442), tensor(0.0277), tensor(0.0420), tensor(0.0403), tensor(0.0070), tensor(0.0645), tensor(-0.0306), tensor(0.0182), tensor(0.0360), tensor(0.0826), tensor(0.0121), tensor(0.0681), tensor(-0.0248), tensor(-0.0194), tensor(-0.0131), tensor(0.0197), tensor(-0.0037), tensor(0.0160), tensor(-0.0154), tensor(0.0572), tensor(-0.0020), tensor(0.0505), tensor(-0.0082), tensor(-0.0254), tensor(0.0395), tensor(0.0653), tensor(0.0383), tensor(0.0531), tensor(0.0672), tensor(0.0207), tensor(0.0874), tensor(-0.0049), tensor(0.0163), tensor(0.0622), tensor(-0.0024), tensor(-0.0226), tensor(0.0316), tensor(0.0122), tensor(0.0311), tensor(0.0657), tensor(-0.0024), tensor(0.1113), tensor(0.0095), tensor(0.0422), tensor(0.0125), tensor(0.0022), tensor(0.0081), tensor(0.0331), tensor(0.0357), tensor(-0.0005), tensor(-0.0229), tensor(0.1050), tensor(-0.0068), tensor(0.0338), tensor(0.0581), tensor(0.0427), tensor(-0.0061), tensor(0.0714), tensor(0.0289), tensor(0.0353), tensor(0.0285), tensor(0.0766), tensor(0.0730), tensor(-0.0101), tensor(0.0350), tensor(0.0096), tensor(0.0597), tensor(0.0611), tensor(0.0592), tensor(0.0676), tensor(0.0471), tensor(0.0203), tensor(0.0351), tensor(-0.0057), tensor(0.0320), tensor(-0.0226), tensor(0.1038), tensor(0.0280), tensor(-0.0124), tensor(0.0189), tensor(0.0481), tensor(0.0280), tensor(-0.0010), tensor(0.0468), tensor(0.0545), tensor(0.0767), tensor(0.0158), tensor(0.0299), tensor(0.0830), tensor(0.0573), tensor(0.0384), tensor(0.0170), tensor(0.0668), tensor(-0.0177), tensor(0.1083), tensor(-0.0005), tensor(0.0087), tensor(0.0257), tensor(-0.0021), tensor(0.0087), tensor(0.0626), tensor(0.0422), tensor(-0.0481), tensor(0.0762), tensor(-0.0127), tensor(0.0398), tensor(0.0248), tensor(0.1035), tensor(0.0092), tensor(0.0261), tensor(0.0120), tensor(0.0755), tensor(0.0688), tensor(-0.0111), tensor(-0.0178), tensor(0.0165), tensor(0.0106), tensor(0.0883), tensor(0.0168), tensor(0.0165), tensor(-0.0150), tensor(-0.0185), tensor(0.0365), tensor(0.0578), tensor(-0.0098), tensor(-0.0266), tensor(-0.0396), tensor(0.0604), tensor(0.0217), tensor(0.0755), tensor(0.0712), tensor(0.0233), tensor(0.1097), tensor(-0.0093), tensor(0.0351), tensor(0.0560), tensor(0.0002), tensor(0.0581), tensor(-0.0300), tensor(-0.0019), tensor(0.0318), tensor(0.0347), tensor(0.0494), tensor(0.0101), tensor(0.0654), tensor(-0.0194), tensor(0.0613), tensor(0.0225), tensor(0.0244), tensor(0.0705), tensor(0.0283), tensor(0.0023), tensor(0.0010), tensor(0.0348), tensor(0.0474), tensor(0.0252), tensor(-0.0227), tensor(0.0497), tensor(0.1123), tensor(-0.0024), tensor(-0.0001), tensor(0.0215), tensor(0.0431), tensor(0.0051), tensor(0.0476), tensor(0.0196), tensor(-0.0320), tensor(0.0414), tensor(0.0245), tensor(0.0321), tensor(0.0565), tensor(0.0466), tensor(0.1178), tensor(0.0285), tensor(0.0323), tensor(0.0539), tensor(0.0567), tensor(0.0390), tensor(0.0036), tensor(0.0219), tensor(0.0323), tensor(0.0583), tensor(0.0069), tensor(0.0707), tensor(0.0031), tensor(0.0975), tensor(-0.0152), tensor(0.0131), tensor(0.0107), tensor(0.0332), tensor(0.0412), tensor(0.0290), tensor(0.0071), tensor(0.0144), tensor(0.0813), tensor(0.0801), tensor(-0.0023), tensor(0.0878), tensor(0.0591), tensor(0.0175), tensor(0.0078), tensor(0.0557), tensor(0.0492), tensor(0.0159), tensor(0.0964), tensor(-0.0054), tensor(0.0700), tensor(0.0291), tensor(0.0598), tensor(0.0201), tensor(0.0731), tensor(0.0932), tensor(0.2299), tensor(0.0733), tensor(0.0354), tensor(-0.0042), tensor(0.0359), tensor(0.0356), tensor(0.0195), tensor(-0.0273), tensor(0.0521), tensor(0.0590), tensor(0.0321), tensor(-0.0037), tensor(0.0282), tensor(-0.0046), tensor(0.0160), tensor(0.1475), tensor(0.0870), tensor(-0.0216), tensor(0.0374), tensor(-0.0104), tensor(0.0894), tensor(0.0015), tensor(0.0186), tensor(-0.0167), tensor(0.0422), tensor(0.1109), tensor(-0.0054), tensor(0.0342), tensor(0.0955), tensor(0.0770), tensor(-0.0289), tensor(0.0159), tensor(0.0197), tensor(0.0286), tensor(0.0321), tensor(0.0333), tensor(0.0102), tensor(0.0073), tensor(0.0125), tensor(-0.0078), tensor(0.0254), tensor(0.0459), tensor(0.0535), tensor(0.0007), tensor(0.0242), tensor(0.0350), tensor(0.0366), tensor(0.0069), tensor(0.0002), tensor(0.0310), tensor(0.0483), tensor(0.0773), tensor(0.0607), tensor(0.0296), tensor(0.0210), tensor(0.0391), tensor(0.0356), tensor(0.0801), tensor(0.0202), tensor(-0.0343), tensor(-0.0184), tensor(0.0053), tensor(0.0465), tensor(0.1346), tensor(0.0207), tensor(0.0730), tensor(0.0401), tensor(0.0526), tensor(0.0237), tensor(0.0090), tensor(0.0131), tensor(0.0303), tensor(0.0274), tensor(0.0205), tensor(-0.0049), tensor(0.0113), tensor(-0.0109), tensor(0.0293), tensor(0.0198), tensor(0.0264), tensor(0.0205), tensor(0.0961), tensor(0.0052), tensor(0.0074), tensor(0.0732), tensor(0.0931), tensor(0.0054), tensor(0.0205), tensor(0.0138), tensor(0.0052), tensor(-0.0143), tensor(0.1204)]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwltUoySfQdJ",
        "outputId": "a4abbcdb-5b71-41bd-b7ee-b29f770db6db"
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "running_fp = 0\n",
        "running_fn = 0\n",
        "running_tp = 0\n",
        "running_tn = 0\n",
        "\n",
        "epoch_loss = 0\n",
        "epoch_acc = 0\n",
        "epoch_accuracy = 0\n",
        "epoch_recall = 0\n",
        "epoch_precision = 0 \n",
        "\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "with torch.no_grad():\n",
        "    for inputs, masks, labels in dataloaders['test']:\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model_ft(inputs.float())\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "        cm = confusion_matrix(labels.data.to('cpu'), preds.to('cpu'), labels=[1,0])\n",
        "\n",
        "        TP = cm[0][0]\n",
        "        TN = cm[1][1]\n",
        "        FN = cm[1][0]\n",
        "        FP = cm[0][1]\n",
        "\n",
        "        running_fp += FP\n",
        "        running_fn += FN\n",
        "        running_tp += TP\n",
        "        running_tn += TN\n",
        "\n",
        "epoch_acc = correct / dataset_sizes['test']\n",
        "\n",
        "\n",
        "epoch_accuracy = 100. * correct/dataset_sizes['test']\n",
        "epoch_recall = 100. * running_tp/(running_tp + running_fn)\n",
        "epoch_precision = 100. * running_tp/(running_tp + running_fp)\n",
        "\n",
        "print(f'Accuracy: {epoch_accuracy} Recall: {epoch_recall} Precision: {epoch_precision} F1: {(2*epoch_precision*epoch_recall)/(epoch_precision + epoch_recall)}')\n",
        "\n",
        "print(f'Accuracy of the network on the {dataloaders[\"sizes\"][\"test\"]} test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 71.54639175257732 Recall: 72.93762575452716 Precision: 83.33333333333333 F1: 77.78969957081546\n",
            "Accuracy of the network on the 1455 test images: 71 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXOzq0BL0qUM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeKb8XVK0qYR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhJU-0O5V1Dn"
      },
      "source": [
        "model_ft_ = models.vgg13(pretrained=True)\n",
        "\n",
        "for param in model_ft_.features.parameters():\n",
        "  param.requires_grad_(False)\n",
        "\n",
        "feats_list = list(model_ft_.features)\n",
        "new_feats_list = []\n",
        "for feat in feats_list:\n",
        "    new_feats_list.append(feat)\n",
        "    if isinstance(feat, nn.Conv2d):\n",
        "        new_feats_list.append(nn.Dropout(p=0.4, inplace=True))\n",
        "\n",
        "# modify convolution layers\n",
        "model_ft_.features = nn.Sequential(*new_feats_list)\n",
        "\n",
        "model_ft_.classifier[-1] = nn.Linear(in_features=4096, out_features=2)\n",
        "\n",
        "model_ft_ = model_ft_.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight.to(device))\n",
        "\n",
        "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_ft = optim.AdamW(model_ft_.parameters(), lr=0.0001)\n",
        "\n",
        "# exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r6Ae7qU3V5VH",
        "outputId": "072fc5d8-21d9-4127-8464-d96fbafc6c94"
      },
      "source": [
        "model_ft_ = train_model(model_ft_, criterion, optimizer_ft, dataloaders, num_epochs=50)\n",
        "# Save model\n",
        "torch.save(model_ft_.state_dict(), 'vgg13_50_AdamW.pt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3205 Size: 5295\n",
            "Running Loss: 3661.0635808110237 Size: 5295\n",
            "train Loss: 0.6914, train Acc: train 60.53, train Recall: 62.37, train Precision: 88.74\n",
            "train Loss: 0.6914 Acc: 0.6053\n",
            "-----------------val-----------------\n",
            "Running Corrects: 570 Size: 780\n",
            "Running Loss: 477.22158122062683 Size: 780\n",
            "val Loss: 0.6118, val Acc: val 73.08, val Recall: 73.08, val Precision: 100.00\n",
            "val Loss: 0.6118 Acc: 0.7308\n",
            "\n",
            "Epoch 1/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3371 Size: 5295\n",
            "Running Loss: 3110.879088252783 Size: 5295\n",
            "train Loss: 0.5875, train Acc: train 63.66, train Recall: 64.08, train Precision: 91.78\n",
            "train Loss: 0.5875 Acc: 0.6366\n",
            "-----------------val-----------------\n",
            "Running Corrects: 522 Size: 780\n",
            "Running Loss: 488.7853729724884 Size: 780\n",
            "val Loss: 0.6266, val Acc: val 66.92, val Recall: 71.31, val Precision: 91.58\n",
            "val Loss: 0.6266 Acc: 0.6692\n",
            "\n",
            "Epoch 2/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3474 Size: 5295\n",
            "Running Loss: 3019.6141951680183 Size: 5295\n",
            "train Loss: 0.5703, train Acc: train 65.61, train Recall: 65.77, train Precision: 90.79\n",
            "train Loss: 0.5703 Acc: 0.6561\n",
            "-----------------val-----------------\n",
            "Running Corrects: 529 Size: 780\n",
            "Running Loss: 497.77198004722595 Size: 780\n",
            "val Loss: 0.6382, val Acc: val 67.82, val Recall: 72.37, val Precision: 90.53\n",
            "val Loss: 0.6382 Acc: 0.6782\n",
            "\n",
            "Epoch 3/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3589 Size: 5295\n",
            "Running Loss: 2820.533200621605 Size: 5295\n",
            "train Loss: 0.5327, train Acc: train 67.78, train Recall: 67.63, train Precision: 90.33\n",
            "train Loss: 0.5327 Acc: 0.6778\n",
            "-----------------val-----------------\n",
            "Running Corrects: 529 Size: 780\n",
            "Running Loss: 497.22478556632996 Size: 780\n",
            "val Loss: 0.6375, val Acc: val 67.82, val Recall: 72.69, val Precision: 89.65\n",
            "val Loss: 0.6375 Acc: 0.6782\n",
            "\n",
            "Epoch 4/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3634 Size: 5295\n",
            "Running Loss: 2810.099238693714 Size: 5295\n",
            "train Loss: 0.5307, train Acc: train 68.63, train Recall: 68.19, train Precision: 90.88\n",
            "train Loss: 0.5307 Acc: 0.6863\n",
            "-----------------val-----------------\n",
            "Running Corrects: 568 Size: 780\n",
            "Running Loss: 520.1681516170502 Size: 780\n",
            "val Loss: 0.6669, val Acc: val 72.82, val Recall: 83.27, val Precision: 78.60\n",
            "val Loss: 0.6669 Acc: 0.7282\n",
            "\n",
            "Epoch 5/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3723 Size: 5295\n",
            "Running Loss: 2718.222079396248 Size: 5295\n",
            "train Loss: 0.5134, train Acc: train 70.31, train Recall: 69.59, train Precision: 91.04\n",
            "train Loss: 0.5134 Acc: 0.7031\n",
            "-----------------val-----------------\n",
            "Running Corrects: 546 Size: 780\n",
            "Running Loss: 476.4210078716278 Size: 780\n",
            "val Loss: 0.6108, val Acc: val 70.00, val Recall: 73.60, val Precision: 91.93\n",
            "val Loss: 0.6108 Acc: 0.7000\n",
            "\n",
            "Epoch 6/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3792 Size: 5295\n",
            "Running Loss: 2597.5059886574745 Size: 5295\n",
            "train Loss: 0.4906, train Acc: train 71.61, train Recall: 71.02, train Precision: 90.20\n",
            "train Loss: 0.4906 Acc: 0.7161\n",
            "-----------------val-----------------\n",
            "Running Corrects: 573 Size: 780\n",
            "Running Loss: 497.19101190567017 Size: 780\n",
            "val Loss: 0.6374, val Acc: val 73.46, val Recall: 78.67, val Precision: 87.37\n",
            "val Loss: 0.6374 Acc: 0.7346\n",
            "\n",
            "Epoch 7/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3867 Size: 5295\n",
            "Running Loss: 2525.7353481650352 Size: 5295\n",
            "train Loss: 0.4770, train Acc: train 73.03, train Recall: 72.30, train Precision: 90.33\n",
            "train Loss: 0.4770 Acc: 0.7303\n",
            "-----------------val-----------------\n",
            "Running Corrects: 589 Size: 780\n",
            "Running Loss: 474.6131317615509 Size: 780\n",
            "val Loss: 0.6085, val Acc: val 75.51, val Recall: 78.41, val Precision: 91.75\n",
            "val Loss: 0.6085 Acc: 0.7551\n",
            "\n",
            "Epoch 8/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3888 Size: 5295\n",
            "Running Loss: 2493.916054815054 Size: 5295\n",
            "train Loss: 0.4710, train Acc: train 73.43, train Recall: 72.49, train Precision: 90.85\n",
            "train Loss: 0.4710 Acc: 0.7343\n",
            "-----------------val-----------------\n",
            "Running Corrects: 552 Size: 780\n",
            "Running Loss: 474.70908784866333 Size: 780\n",
            "val Loss: 0.6086, val Acc: val 70.77, val Recall: 75.68, val Precision: 88.42\n",
            "val Loss: 0.6086 Acc: 0.7077\n",
            "\n",
            "Epoch 9/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3927 Size: 5295\n",
            "Running Loss: 2433.2688717246056 Size: 5295\n",
            "train Loss: 0.4595, train Acc: train 74.16, train Recall: 73.17, train Precision: 90.91\n",
            "train Loss: 0.4595 Acc: 0.7416\n",
            "-----------------val-----------------\n",
            "Running Corrects: 589 Size: 780\n",
            "Running Loss: 473.6199879646301 Size: 780\n",
            "val Loss: 0.6072, val Acc: val 75.51, val Recall: 80.03, val Precision: 88.60\n",
            "val Loss: 0.6072 Acc: 0.7551\n",
            "\n",
            "Epoch 10/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 3975 Size: 5295\n",
            "Running Loss: 2354.4256523251534 Size: 5295\n",
            "train Loss: 0.4447, train Acc: train 75.07, train Recall: 74.05, train Precision: 90.95\n",
            "train Loss: 0.4447 Acc: 0.7507\n",
            "-----------------val-----------------\n",
            "Running Corrects: 565 Size: 780\n",
            "Running Loss: 494.35520815849304 Size: 780\n",
            "val Loss: 0.6338, val Acc: val 72.44, val Recall: 79.44, val Precision: 84.04\n",
            "val Loss: 0.6338 Acc: 0.7244\n",
            "\n",
            "Epoch 11/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4018 Size: 5295\n",
            "Running Loss: 2318.146970361471 Size: 5295\n",
            "train Loss: 0.4378, train Acc: train 75.88, train Recall: 74.80, train Precision: 91.10\n",
            "train Loss: 0.4378 Acc: 0.7588\n",
            "-----------------val-----------------\n",
            "Running Corrects: 585 Size: 780\n",
            "Running Loss: 491.68707609176636 Size: 780\n",
            "val Loss: 0.6304, val Acc: val 75.00, val Recall: 81.51, val Precision: 85.09\n",
            "val Loss: 0.6304 Acc: 0.7500\n",
            "\n",
            "Epoch 12/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4112 Size: 5295\n",
            "Running Loss: 2189.7149659097195 Size: 5295\n",
            "train Loss: 0.4135, train Acc: train 77.66, train Recall: 76.56, train Precision: 91.26\n",
            "train Loss: 0.4135 Acc: 0.7766\n",
            "-----------------val-----------------\n",
            "Running Corrects: 572 Size: 780\n",
            "Running Loss: 488.48261880874634 Size: 780\n",
            "val Loss: 0.6263, val Acc: val 73.33, val Recall: 79.10, val Precision: 86.32\n",
            "val Loss: 0.6263 Acc: 0.7333\n",
            "\n",
            "Epoch 13/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4110 Size: 5295\n",
            "Running Loss: 2219.104747414589 Size: 5295\n",
            "train Loss: 0.4191, train Acc: train 77.62, train Recall: 76.45, train Precision: 91.41\n",
            "train Loss: 0.4191 Acc: 0.7762\n",
            "-----------------val-----------------\n",
            "Running Corrects: 552 Size: 780\n",
            "Running Loss: 514.0004680156708 Size: 780\n",
            "val Loss: 0.6590, val Acc: val 70.77, val Recall: 89.58, val Precision: 67.89\n",
            "val Loss: 0.6590 Acc: 0.7077\n",
            "\n",
            "Epoch 14/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4157 Size: 5295\n",
            "Running Loss: 2167.125297397375 Size: 5295\n",
            "train Loss: 0.4093, train Acc: train 78.51, train Recall: 77.25, train Precision: 91.72\n",
            "train Loss: 0.4093 Acc: 0.7851\n",
            "-----------------val-----------------\n",
            "Running Corrects: 600 Size: 780\n",
            "Running Loss: 483.1277871131897 Size: 780\n",
            "val Loss: 0.6194, val Acc: val 76.92, val Recall: 84.70, val Precision: 83.51\n",
            "val Loss: 0.6194 Acc: 0.7692\n",
            "\n",
            "Epoch 15/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4194 Size: 5295\n",
            "Running Loss: 2069.3496742248535 Size: 5295\n",
            "train Loss: 0.3908, train Acc: train 79.21, train Recall: 78.01, train Precision: 91.72\n",
            "train Loss: 0.3908 Acc: 0.7921\n",
            "-----------------val-----------------\n",
            "Running Corrects: 588 Size: 780\n",
            "Running Loss: 477.3415765762329 Size: 780\n",
            "val Loss: 0.6120, val Acc: val 75.38, val Recall: 82.36, val Precision: 84.39\n",
            "val Loss: 0.6120 Acc: 0.7538\n",
            "\n",
            "Epoch 16/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4212 Size: 5295\n",
            "Running Loss: 2058.928752541542 Size: 5295\n",
            "train Loss: 0.3888, train Acc: train 79.55, train Recall: 78.62, train Precision: 91.22\n",
            "train Loss: 0.3888 Acc: 0.7955\n",
            "-----------------val-----------------\n",
            "Running Corrects: 596 Size: 780\n",
            "Running Loss: 476.11298179626465 Size: 780\n",
            "val Loss: 0.6104, val Acc: val 76.41, val Recall: 82.49, val Precision: 85.96\n",
            "val Loss: 0.6104 Acc: 0.7641\n",
            "\n",
            "Epoch 17/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4216 Size: 5295\n",
            "Running Loss: 2052.4140825867653 Size: 5295\n",
            "train Loss: 0.3876, train Acc: train 79.62, train Recall: 78.48, train Precision: 91.69\n",
            "train Loss: 0.3876 Acc: 0.7962\n",
            "-----------------val-----------------\n",
            "Running Corrects: 576 Size: 780\n",
            "Running Loss: 498.09573793411255 Size: 780\n",
            "val Loss: 0.6386, val Acc: val 73.85, val Recall: 87.65, val Precision: 74.74\n",
            "val Loss: 0.6386 Acc: 0.7385\n",
            "\n",
            "Epoch 18/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4313 Size: 5295\n",
            "Running Loss: 1946.537468612194 Size: 5295\n",
            "train Loss: 0.3676, train Acc: train 81.45, train Recall: 80.25, train Precision: 92.25\n",
            "train Loss: 0.3676 Acc: 0.8145\n",
            "-----------------val-----------------\n",
            "Running Corrects: 561 Size: 780\n",
            "Running Loss: 441.4645628929138 Size: 780\n",
            "val Loss: 0.5660, val Acc: val 71.92, val Recall: 75.25, val Precision: 91.75\n",
            "val Loss: 0.5660 Acc: 0.7192\n",
            "\n",
            "Epoch 19/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4287 Size: 5295\n",
            "Running Loss: 1919.3502394258976 Size: 5295\n",
            "train Loss: 0.3625, train Acc: train 80.96, train Recall: 79.79, train Precision: 92.06\n",
            "train Loss: 0.3625 Acc: 0.8096\n",
            "-----------------val-----------------\n",
            "Running Corrects: 556 Size: 780\n",
            "Running Loss: 482.8571856021881 Size: 780\n",
            "val Loss: 0.6190, val Acc: val 71.28, val Recall: 80.57, val Precision: 80.00\n",
            "val Loss: 0.6190 Acc: 0.7128\n",
            "\n",
            "Epoch 20/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4307 Size: 5295\n",
            "Running Loss: 1876.436776906252 Size: 5295\n",
            "train Loss: 0.3544, train Acc: train 81.34, train Recall: 79.95, train Precision: 92.59\n",
            "train Loss: 0.3544 Acc: 0.8134\n",
            "-----------------val-----------------\n",
            "Running Corrects: 547 Size: 780\n",
            "Running Loss: 501.7171070575714 Size: 780\n",
            "val Loss: 0.6432, val Acc: val 70.13, val Recall: 83.50, val Precision: 73.68\n",
            "val Loss: 0.6432 Acc: 0.7013\n",
            "\n",
            "Epoch 21/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4288 Size: 5295\n",
            "Running Loss: 1908.1582787930965 Size: 5295\n",
            "train Loss: 0.3604, train Acc: train 80.98, train Recall: 79.88, train Precision: 91.94\n",
            "train Loss: 0.3604 Acc: 0.8098\n",
            "-----------------val-----------------\n",
            "Running Corrects: 540 Size: 780\n",
            "Running Loss: 493.0328724384308 Size: 780\n",
            "val Loss: 0.6321, val Acc: val 69.23, val Recall: 81.37, val Precision: 75.09\n",
            "val Loss: 0.6321 Acc: 0.6923\n",
            "\n",
            "Epoch 22/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4341 Size: 5295\n",
            "Running Loss: 1842.7469361126423 Size: 5295\n",
            "train Loss: 0.3480, train Acc: train 81.98, train Recall: 80.71, train Precision: 92.53\n",
            "train Loss: 0.3480 Acc: 0.8198\n",
            "-----------------val-----------------\n",
            "Running Corrects: 566 Size: 780\n",
            "Running Loss: 474.54379773139954 Size: 780\n",
            "val Loss: 0.6084, val Acc: val 72.56, val Recall: 82.96, val Precision: 78.60\n",
            "val Loss: 0.6084 Acc: 0.7256\n",
            "\n",
            "Epoch 23/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4448 Size: 5295\n",
            "Running Loss: 1731.1810578107834 Size: 5295\n",
            "train Loss: 0.3269, train Acc: train 84.00, train Recall: 82.85, train Precision: 92.99\n",
            "train Loss: 0.3269 Acc: 0.8400\n",
            "-----------------val-----------------\n",
            "Running Corrects: 514 Size: 780\n",
            "Running Loss: 516.8379256725311 Size: 780\n",
            "val Loss: 0.6626, val Acc: val 65.90, val Recall: 88.19, val Precision: 61.58\n",
            "val Loss: 0.6626 Acc: 0.6590\n",
            "\n",
            "Epoch 24/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4399 Size: 5295\n",
            "Running Loss: 1718.082556694746 Size: 5295\n",
            "train Loss: 0.3245, train Acc: train 83.08, train Recall: 81.93, train Precision: 92.65\n",
            "train Loss: 0.3245 Acc: 0.8308\n",
            "-----------------val-----------------\n",
            "Running Corrects: 454 Size: 780\n",
            "Running Loss: 544.0045168399811 Size: 780\n",
            "val Loss: 0.6974, val Acc: val 58.21, val Recall: 88.85, val Precision: 48.95\n",
            "val Loss: 0.6974 Acc: 0.5821\n",
            "\n",
            "Epoch 25/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4489 Size: 5295\n",
            "Running Loss: 1618.784154444933 Size: 5295\n",
            "train Loss: 0.3057, train Acc: train 84.78, train Recall: 83.53, train Precision: 93.43\n",
            "train Loss: 0.3057 Acc: 0.8478\n",
            "-----------------val-----------------\n",
            "Running Corrects: 559 Size: 780\n",
            "Running Loss: 473.06406569480896 Size: 780\n",
            "val Loss: 0.6065, val Acc: val 71.67, val Recall: 82.99, val Precision: 77.02\n",
            "val Loss: 0.6065 Acc: 0.7167\n",
            "\n",
            "Epoch 26/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4458 Size: 5295\n",
            "Running Loss: 1664.169045329094 Size: 5295\n",
            "train Loss: 0.3143, train Acc: train 84.19, train Recall: 83.00, train Precision: 93.12\n",
            "train Loss: 0.3143 Acc: 0.8419\n",
            "-----------------val-----------------\n",
            "Running Corrects: 526 Size: 780\n",
            "Running Loss: 500.7986536026001 Size: 780\n",
            "val Loss: 0.6420, val Acc: val 67.44, val Recall: 83.19, val Precision: 69.47\n",
            "val Loss: 0.6420 Acc: 0.6744\n",
            "\n",
            "Epoch 27/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4452 Size: 5295\n",
            "Running Loss: 1655.809500336647 Size: 5295\n",
            "train Loss: 0.3127, train Acc: train 84.08, train Recall: 82.65, train Precision: 93.49\n",
            "train Loss: 0.3127 Acc: 0.8408\n",
            "-----------------val-----------------\n",
            "Running Corrects: 482 Size: 780\n",
            "Running Loss: 517.2296221256256 Size: 780\n",
            "val Loss: 0.6631, val Acc: val 61.79, val Recall: 80.77, val Precision: 62.63\n",
            "val Loss: 0.6631 Acc: 0.6179\n",
            "\n",
            "Epoch 28/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4503 Size: 5295\n",
            "Running Loss: 1609.3442162275314 Size: 5295\n",
            "train Loss: 0.3039, train Acc: train 85.04, train Recall: 83.88, train Precision: 93.40\n",
            "train Loss: 0.3039 Acc: 0.8504\n",
            "-----------------val-----------------\n",
            "Running Corrects: 510 Size: 780\n",
            "Running Loss: 515.2306199073792 Size: 780\n",
            "val Loss: 0.6606, val Acc: val 65.38, val Recall: 83.33, val Precision: 65.79\n",
            "val Loss: 0.6606 Acc: 0.6538\n",
            "\n",
            "Epoch 29/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4526 Size: 5295\n",
            "Running Loss: 1576.94840452075 Size: 5295\n",
            "train Loss: 0.2978, train Acc: train 85.48, train Recall: 84.05, train Precision: 93.98\n",
            "train Loss: 0.2978 Acc: 0.8548\n",
            "-----------------val-----------------\n",
            "Running Corrects: 477 Size: 780\n",
            "Running Loss: 545.2654185295105 Size: 780\n",
            "val Loss: 0.6991, val Acc: val 61.15, val Recall: 87.39, val Precision: 54.74\n",
            "val Loss: 0.6991 Acc: 0.6115\n",
            "\n",
            "Epoch 30/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4528 Size: 5295\n",
            "Running Loss: 1548.6758965402842 Size: 5295\n",
            "train Loss: 0.2925, train Acc: train 85.51, train Recall: 84.58, train Precision: 93.21\n",
            "train Loss: 0.2925 Acc: 0.8551\n",
            "-----------------val-----------------\n",
            "Running Corrects: 488 Size: 780\n",
            "Running Loss: 531.9251081943512 Size: 780\n",
            "val Loss: 0.6820, val Acc: val 62.56, val Recall: 86.97, val Precision: 57.37\n",
            "val Loss: 0.6820 Acc: 0.6256\n",
            "\n",
            "Epoch 31/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4493 Size: 5295\n",
            "Running Loss: 1587.4076446592808 Size: 5295\n",
            "train Loss: 0.2998, train Acc: train 84.85, train Recall: 83.76, train Precision: 93.21\n",
            "train Loss: 0.2998 Acc: 0.8485\n",
            "-----------------val-----------------\n",
            "Running Corrects: 491 Size: 780\n",
            "Running Loss: 522.3040146827698 Size: 780\n",
            "val Loss: 0.6696, val Acc: val 62.95, val Recall: 83.86, val Precision: 61.05\n",
            "val Loss: 0.6696 Acc: 0.6295\n",
            "\n",
            "Epoch 32/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4574 Size: 5295\n",
            "Running Loss: 1448.531387731433 Size: 5295\n",
            "train Loss: 0.2736, train Acc: train 86.38, train Recall: 85.05, train Precision: 94.20\n",
            "train Loss: 0.2736 Acc: 0.8638\n",
            "-----------------val-----------------\n",
            "Running Corrects: 431 Size: 780\n",
            "Running Loss: 603.45370221138 Size: 780\n",
            "val Loss: 0.7737, val Acc: val 55.26, val Recall: 92.02, val Precision: 42.46\n",
            "val Loss: 0.7737 Acc: 0.5526\n",
            "\n",
            "Epoch 33/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4565 Size: 5295\n",
            "Running Loss: 1465.7838686406612 Size: 5295\n",
            "train Loss: 0.2768, train Acc: train 86.21, train Recall: 85.01, train Precision: 93.92\n",
            "train Loss: 0.2768 Acc: 0.8621\n",
            "-----------------val-----------------\n",
            "Running Corrects: 453 Size: 780\n",
            "Running Loss: 572.8023707866669 Size: 780\n",
            "val Loss: 0.7344, val Acc: val 58.08, val Recall: 85.84, val Precision: 51.05\n",
            "val Loss: 0.7344 Acc: 0.5808\n",
            "\n",
            "Epoch 34/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4643 Size: 5295\n",
            "Running Loss: 1370.6030361056328 Size: 5295\n",
            "train Loss: 0.2588, train Acc: train 87.69, train Recall: 86.60, train Precision: 94.39\n",
            "train Loss: 0.2588 Acc: 0.8769\n",
            "-----------------val-----------------\n",
            "Running Corrects: 463 Size: 780\n",
            "Running Loss: 560.6144425868988 Size: 780\n",
            "val Loss: 0.7187, val Acc: val 59.36, val Recall: 85.63, val Precision: 53.33\n",
            "val Loss: 0.7187 Acc: 0.5936\n",
            "\n",
            "Epoch 35/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4620 Size: 5295\n",
            "Running Loss: 1418.9335057735443 Size: 5295\n",
            "train Loss: 0.2680, train Acc: train 87.25, train Recall: 86.22, train Precision: 94.11\n",
            "train Loss: 0.2680 Acc: 0.8725\n",
            "-----------------val-----------------\n",
            "Running Corrects: 494 Size: 780\n",
            "Running Loss: 517.092668056488 Size: 780\n",
            "val Loss: 0.6629, val Acc: val 63.33, val Recall: 83.49, val Precision: 62.11\n",
            "val Loss: 0.6629 Acc: 0.6333\n",
            "\n",
            "Epoch 36/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4629 Size: 5295\n",
            "Running Loss: 1373.3882879018784 Size: 5295\n",
            "train Loss: 0.2594, train Acc: train 87.42, train Recall: 86.36, train Precision: 94.23\n",
            "train Loss: 0.2594 Acc: 0.8742\n",
            "-----------------val-----------------\n",
            "Running Corrects: 456 Size: 780\n",
            "Running Loss: 557.6906456947327 Size: 780\n",
            "val Loss: 0.7150, val Acc: val 58.46, val Recall: 83.06, val Precision: 54.21\n",
            "val Loss: 0.7150 Acc: 0.5846\n",
            "\n",
            "Epoch 37/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4587 Size: 5295\n",
            "Running Loss: 1444.6988487541676 Size: 5295\n",
            "train Loss: 0.2728, train Acc: train 86.63, train Recall: 85.80, train Precision: 93.52\n",
            "train Loss: 0.2728 Acc: 0.8663\n",
            "-----------------val-----------------\n",
            "Running Corrects: 465 Size: 780\n",
            "Running Loss: 571.3698217868805 Size: 780\n",
            "val Loss: 0.7325, val Acc: val 59.62, val Recall: 89.72, val Precision: 50.53\n",
            "val Loss: 0.7325 Acc: 0.5962\n",
            "\n",
            "Epoch 38/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4678 Size: 5295\n",
            "Running Loss: 1282.2449487149715 Size: 5295\n",
            "train Loss: 0.2422, train Acc: train 88.35, train Recall: 87.17, train Precision: 94.82\n",
            "train Loss: 0.2422 Acc: 0.8835\n",
            "-----------------val-----------------\n",
            "Running Corrects: 495 Size: 780\n",
            "Running Loss: 521.4198141098022 Size: 780\n",
            "val Loss: 0.6685, val Acc: val 63.46, val Recall: 82.61, val Precision: 63.33\n",
            "val Loss: 0.6685 Acc: 0.6346\n",
            "\n",
            "Epoch 39/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4646 Size: 5295\n",
            "Running Loss: 1334.874092578888 Size: 5295\n",
            "train Loss: 0.2521, train Acc: train 87.74, train Recall: 86.55, train Precision: 94.57\n",
            "train Loss: 0.2521 Acc: 0.8774\n",
            "-----------------val-----------------\n",
            "Running Corrects: 490 Size: 780\n",
            "Running Loss: 544.6720979213715 Size: 780\n",
            "val Loss: 0.6983, val Acc: val 62.82, val Recall: 90.23, val Precision: 55.09\n",
            "val Loss: 0.6983 Acc: 0.6282\n",
            "\n",
            "Epoch 40/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4676 Size: 5295\n",
            "Running Loss: 1299.8738943338394 Size: 5295\n",
            "train Loss: 0.2455, train Acc: train 88.31, train Recall: 87.51, train Precision: 94.26\n",
            "train Loss: 0.2455 Acc: 0.8831\n",
            "-----------------val-----------------\n",
            "Running Corrects: 503 Size: 780\n",
            "Running Loss: 512.4465188980103 Size: 780\n",
            "val Loss: 0.6570, val Acc: val 64.49, val Recall: 83.68, val Precision: 63.86\n",
            "val Loss: 0.6570 Acc: 0.6449\n",
            "\n",
            "Epoch 41/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4716 Size: 5295\n",
            "Running Loss: 1239.543271124363 Size: 5295\n",
            "train Loss: 0.2341, train Acc: train 89.07, train Recall: 88.06, train Precision: 94.91\n",
            "train Loss: 0.2341 Acc: 0.8907\n",
            "-----------------val-----------------\n",
            "Running Corrects: 482 Size: 780\n",
            "Running Loss: 543.2729587554932 Size: 780\n",
            "val Loss: 0.6965, val Acc: val 61.79, val Recall: 86.17, val Precision: 56.84\n",
            "val Loss: 0.6965 Acc: 0.6179\n",
            "\n",
            "Epoch 42/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4663 Size: 5295\n",
            "Running Loss: 1281.816570788622 Size: 5295\n",
            "train Loss: 0.2421, train Acc: train 88.06, train Recall: 87.03, train Precision: 94.48\n",
            "train Loss: 0.2421 Acc: 0.8806\n",
            "-----------------val-----------------\n",
            "Running Corrects: 433 Size: 780\n",
            "Running Loss: 577.1177661418915 Size: 780\n",
            "val Loss: 0.7399, val Acc: val 55.51, val Recall: 92.72, val Precision: 42.46\n",
            "val Loss: 0.7399 Acc: 0.5551\n",
            "\n",
            "Epoch 43/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4741 Size: 5295\n",
            "Running Loss: 1198.9877219498158 Size: 5295\n",
            "train Loss: 0.2264, train Acc: train 89.54, train Recall: 88.56, train Precision: 95.10\n",
            "train Loss: 0.2264 Acc: 0.8954\n",
            "-----------------val-----------------\n",
            "Running Corrects: 440 Size: 780\n",
            "Running Loss: 575.0617227554321 Size: 780\n",
            "val Loss: 0.7373, val Acc: val 56.41, val Recall: 89.38, val Precision: 45.79\n",
            "val Loss: 0.7373 Acc: 0.5641\n",
            "\n",
            "Epoch 44/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4726 Size: 5295\n",
            "Running Loss: 1210.277134373784 Size: 5295\n",
            "train Loss: 0.2286, train Acc: train 89.25, train Recall: 88.20, train Precision: 95.07\n",
            "train Loss: 0.2286 Acc: 0.8925\n",
            "-----------------val-----------------\n",
            "Running Corrects: 500 Size: 780\n",
            "Running Loss: 515.6292490959167 Size: 780\n",
            "val Loss: 0.6611, val Acc: val 64.10, val Recall: 84.20, val Precision: 62.63\n",
            "val Loss: 0.6611 Acc: 0.6410\n",
            "\n",
            "Epoch 45/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4741 Size: 5295\n",
            "Running Loss: 1162.7163324058056 Size: 5295\n",
            "train Loss: 0.2196, train Acc: train 89.54, train Recall: 88.50, train Precision: 95.19\n",
            "train Loss: 0.2196 Acc: 0.8954\n",
            "-----------------val-----------------\n",
            "Running Corrects: 543 Size: 780\n",
            "Running Loss: 486.00153851509094 Size: 780\n",
            "val Loss: 0.6231, val Acc: val 69.62, val Recall: 84.33, val Precision: 71.75\n",
            "val Loss: 0.6231 Acc: 0.6962\n",
            "\n",
            "Epoch 46/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4745 Size: 5295\n",
            "Running Loss: 1169.3460269868374 Size: 5295\n",
            "train Loss: 0.2208, train Acc: train 89.61, train Recall: 88.60, train Precision: 95.19\n",
            "train Loss: 0.2208 Acc: 0.8961\n",
            "-----------------val-----------------\n",
            "Running Corrects: 527 Size: 780\n",
            "Running Loss: 476.5472319126129 Size: 780\n",
            "val Loss: 0.6110, val Acc: val 67.56, val Recall: 81.64, val Precision: 71.75\n",
            "val Loss: 0.6110 Acc: 0.6756\n",
            "\n",
            "Epoch 47/49\n",
            "----------\n",
            "-----------------train-----------------\n",
            "Running Corrects: 4716 Size: 5295\n",
            "Running Loss: 1193.8797807991505 Size: 5295\n",
            "train Loss: 0.2255, train Acc: train 89.07, train Recall: 88.15, train Precision: 94.79\n",
            "train Loss: 0.2255 Acc: 0.8907\n",
            "-----------------val-----------------\n",
            "Running Corrects: 485 Size: 780\n",
            "Running Loss: 529.771871805191 Size: 780\n",
            "val Loss: 0.6792, val Acc: val 62.18, val Recall: 86.86, val Precision: 56.84\n",
            "val Loss: 0.6792 Acc: 0.6218\n",
            "\n",
            "Epoch 48/49\n",
            "----------\n",
            "-----------------train-----------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7jTdgFa77Si"
      },
      "source": [
        "model_ft_ = models.vgg13(pretrained=True)\n",
        "\n",
        "for param in model_ft_.features.parameters():\n",
        "  param.requires_grad_(False)\n",
        "\n",
        "feats_list = list(model_ft_.features)\n",
        "new_feats_list = []\n",
        "for feat in feats_list:\n",
        "    new_feats_list.append(feat)\n",
        "    if isinstance(feat, nn.Conv2d):\n",
        "        new_feats_list.append(nn.Dropout(p=0.5, inplace=True))\n",
        "\n",
        "# modify convolution layers\n",
        "model_ft_.features = nn.Sequential(*new_feats_list)\n",
        "\n",
        "model_ft_.classifier[-1] = nn.Linear(in_features=4096, out_features=2)\n",
        "\n",
        "model_ft_ = model_ft_.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight.to(device))\n",
        "\n",
        "# optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_ft = optim.AdamW(model_ft_.parameters(), lr=0.0001)\n",
        "\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ugf-Ko-UMBnY"
      },
      "source": [
        "model_ft_ = train_model(model_ft_, criterion, optimizer_ft, dataloaders, exp_lr_scheduler, num_epochs=50)\n",
        "# Save model\n",
        "torch.save(model_ft_.state_dict(), 'vgg13_50_AdamW.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "momQ935EMQ-K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}